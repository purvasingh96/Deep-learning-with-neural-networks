{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Harry_Potter",
      "provenance": [],
      "authorship_tag": "ABX9TyMz9Xj+6ZxeNyDGXUqS+wuU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/purvasingh96/Deep-learning-with-neural-networks/blob/master/Chapter-wise%20code/Code%20-%20PyTorch/3.%20Recurrent%20Neural%20Networks/Harry_Potter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0sgEvJ691SK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t07nFFS4INNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('harry_potter_1.txt', 'r') as f:\n",
        "  text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2kr6nTCIV8R",
        "colab_type": "code",
        "outputId": "477cf16c-7f74-4455-d728-67ad78304e99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1 CHAPTER ONE The Boy Who Lived M r and Mrs Dursley, of number four, Privet Drive, were proud to say'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmAgZsf9IbKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch : ii for ii, ch in int2char.items()}\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7BakAI9JCrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n44aJvR9NEgh",
        "colab_type": "code",
        "outputId": "1dce5c36-0b50-4076-8623-18861b783539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "test = np.array([[3, 4, 6]])\n",
        "one_hot = one_hot_encode(test, 8)\n",
        "print(one_hot)"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFlrTqs4NgAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "  batch_size_total = batch_size*seq_length\n",
        "  n_batches = len(arr)//batch_size_total\n",
        "  arr = arr[:n_batches*batch_size_total]\n",
        "  arr = arr.reshape((batch_size, -1))\n",
        "\n",
        "  for n in range(0, arr.shape[1], seq_length):\n",
        "      # The features\n",
        "      x = arr[:, n:n+seq_length]\n",
        "      # The targets, shifted by one\n",
        "      y = np.zeros_like(x)\n",
        "      try:\n",
        "        y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "      except IndexError:\n",
        "          y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "      yield x, y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I_dAnN6VcDM",
        "colab_type": "code",
        "outputId": "64e7ab15-893f-466e-88af-7ec7ba00a0ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "batches = generate_batches(encoded, 8, 50)\n",
        "x, y = next(batches)\n",
        "\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[29 63 64 60 37  9 53 59 73 63]\n",
            " [42 76 16 26 33 67 12 63 35  6]\n",
            " [65 63 22  6 30 16  4 11  6 26]\n",
            " [12  4 33  1 63 11 12 30 22 11]\n",
            " [63 34 42 40 56  6 12 44 63 68]\n",
            " [30 11 63 76 33 11 12  4 40 56]\n",
            " [40 30 33 67 12 63 43  6 30 33]\n",
            " [28 68 11 63 12 35 30 12 63  7]]\n",
            "\n",
            "y\n",
            " [[63 64 60 37  9 53 59 73 63 25]\n",
            " [76 16 26 33 67 12 63 35  6 16]\n",
            " [63 22  6 30 16  4 11  6 26 63]\n",
            " [ 4 33  1 63 11 12 30 22 11 63]\n",
            " [34 42 40 56  6 12 44 63 68 12]\n",
            " [11 63 76 33 11 12  4 40 56  4]\n",
            " [30 33 67 12 63 43  6 30 33 63]\n",
            " [68 11 63 12 35 30 12 63  7 35]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u4XmyEWVrD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "  def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
        "    super().__init__()\n",
        "    self.drop_prob = drop_prob\n",
        "    self.n_layers = n_layers\n",
        "    self.n_hidden = n_hidden\n",
        "    self.lr = lr\n",
        "\n",
        "    self.chars = tokens\n",
        "    self.int2char = dict(enumerate(self.chars))\n",
        "    self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "    self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
        "\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "    self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "  \n",
        "  def forward(self, x, hidden):\n",
        "    r_output, hidden = self.lstm(x, hidden)\n",
        "    out = self.dropout(r_output)\n",
        "    out = out.contiguous().view(-1, self.n_hidden)\n",
        "    out = self.fc(out)\n",
        "\n",
        "    return out, hidden\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "              weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "    return hidden\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM-iN9vaEXOW",
        "colab_type": "code",
        "outputId": "ae6a47ba-9a70-4a29-8730-86b39e1c9b49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rZprpA8omRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9aOLoYpxZ-q",
        "colab_type": "code",
        "outputId": "b2c26664-10b0-4648-bc4f-6283c6baeb81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "n_hidden = 512\n",
        "n_layers = 3\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)\n",
        "batch_size = 10\n",
        "seq_length = 10\n",
        "n_epochs = 40\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(77, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=77, bias=True)\n",
            ")\n",
            "Epoch: 1/40... Step: 10... Loss: 3.1513... Val Loss: 3.3527\n",
            "Epoch: 1/40... Step: 20... Loss: 3.4311... Val Loss: 3.3249\n",
            "Epoch: 1/40... Step: 30... Loss: 3.5290... Val Loss: 3.2952\n",
            "Epoch: 1/40... Step: 40... Loss: 3.2703... Val Loss: 3.2745\n",
            "Epoch: 1/40... Step: 50... Loss: 3.0551... Val Loss: 3.2843\n",
            "Epoch: 1/40... Step: 60... Loss: 3.4411... Val Loss: 3.2833\n",
            "Epoch: 1/40... Step: 70... Loss: 3.2058... Val Loss: 3.2719\n",
            "Epoch: 1/40... Step: 80... Loss: 3.2626... Val Loss: 3.2796\n",
            "Epoch: 1/40... Step: 90... Loss: 3.2019... Val Loss: 3.2726\n",
            "Epoch: 1/40... Step: 100... Loss: 3.3569... Val Loss: 3.2725\n",
            "Epoch: 1/40... Step: 110... Loss: 3.1359... Val Loss: 3.2685\n",
            "Epoch: 1/40... Step: 120... Loss: 3.2607... Val Loss: 3.2667\n",
            "Epoch: 1/40... Step: 130... Loss: 3.3359... Val Loss: 3.2553\n",
            "Epoch: 1/40... Step: 140... Loss: 3.0402... Val Loss: 3.2804\n",
            "Epoch: 1/40... Step: 150... Loss: 3.4176... Val Loss: 3.2856\n",
            "Epoch: 1/40... Step: 160... Loss: 3.0600... Val Loss: 3.2591\n",
            "Epoch: 1/40... Step: 170... Loss: 3.3333... Val Loss: 4.2973\n",
            "Epoch: 1/40... Step: 180... Loss: 3.0787... Val Loss: 3.2749\n",
            "Epoch: 1/40... Step: 190... Loss: 2.9419... Val Loss: 3.2976\n",
            "Epoch: 1/40... Step: 200... Loss: 3.3305... Val Loss: 3.2917\n",
            "Epoch: 1/40... Step: 210... Loss: 3.1864... Val Loss: 3.3014\n",
            "Epoch: 1/40... Step: 220... Loss: 3.4836... Val Loss: 3.2437\n",
            "Epoch: 1/40... Step: 230... Loss: 3.1042... Val Loss: 3.2154\n",
            "Epoch: 1/40... Step: 240... Loss: 3.2701... Val Loss: 3.1805\n",
            "Epoch: 2/40... Step: 250... Loss: 2.9942... Val Loss: 3.1448\n",
            "Epoch: 2/40... Step: 260... Loss: 3.0104... Val Loss: 3.0802\n",
            "Epoch: 2/40... Step: 270... Loss: 3.2140... Val Loss: 3.0282\n",
            "Epoch: 2/40... Step: 280... Loss: 3.0218... Val Loss: 2.9936\n",
            "Epoch: 2/40... Step: 290... Loss: 3.0488... Val Loss: 2.9800\n",
            "Epoch: 2/40... Step: 300... Loss: 3.1167... Val Loss: 2.9776\n",
            "Epoch: 2/40... Step: 310... Loss: 2.6567... Val Loss: 2.9491\n",
            "Epoch: 2/40... Step: 320... Loss: 2.8674... Val Loss: 3.0129\n",
            "Epoch: 2/40... Step: 330... Loss: 3.0756... Val Loss: 2.9128\n",
            "Epoch: 2/40... Step: 340... Loss: 2.7386... Val Loss: 2.8992\n",
            "Epoch: 2/40... Step: 350... Loss: 3.0145... Val Loss: 2.8775\n",
            "Epoch: 2/40... Step: 360... Loss: 2.8563... Val Loss: 2.8725\n",
            "Epoch: 2/40... Step: 370... Loss: 2.8637... Val Loss: 2.8370\n",
            "Epoch: 2/40... Step: 380... Loss: 2.9196... Val Loss: 2.8280\n",
            "Epoch: 2/40... Step: 390... Loss: 2.8933... Val Loss: 2.8189\n",
            "Epoch: 2/40... Step: 400... Loss: 2.8308... Val Loss: 2.7968\n",
            "Epoch: 2/40... Step: 410... Loss: 2.9728... Val Loss: 2.7999\n",
            "Epoch: 2/40... Step: 420... Loss: 2.9627... Val Loss: 2.7733\n",
            "Epoch: 2/40... Step: 430... Loss: 2.8201... Val Loss: 2.7404\n",
            "Epoch: 2/40... Step: 440... Loss: 2.6195... Val Loss: 2.7271\n",
            "Epoch: 2/40... Step: 450... Loss: 2.6724... Val Loss: 2.6939\n",
            "Epoch: 2/40... Step: 460... Loss: 2.7029... Val Loss: 2.6736\n",
            "Epoch: 2/40... Step: 470... Loss: 2.6262... Val Loss: 2.6740\n",
            "Epoch: 2/40... Step: 480... Loss: 2.6347... Val Loss: 2.6335\n",
            "Epoch: 2/40... Step: 490... Loss: 2.6650... Val Loss: 2.5965\n",
            "Epoch: 3/40... Step: 500... Loss: 2.7991... Val Loss: 2.6978\n",
            "Epoch: 3/40... Step: 510... Loss: 2.7036... Val Loss: 2.6402\n",
            "Epoch: 3/40... Step: 520... Loss: 2.7507... Val Loss: 2.5824\n",
            "Epoch: 3/40... Step: 530... Loss: 2.5124... Val Loss: 2.5716\n",
            "Epoch: 3/40... Step: 540... Loss: 2.4639... Val Loss: 2.5894\n",
            "Epoch: 3/40... Step: 550... Loss: 2.7200... Val Loss: 2.5387\n",
            "Epoch: 3/40... Step: 560... Loss: 2.3965... Val Loss: 2.5143\n",
            "Epoch: 3/40... Step: 570... Loss: 2.6520... Val Loss: 2.5194\n",
            "Epoch: 3/40... Step: 580... Loss: 2.7179... Val Loss: 2.4703\n",
            "Epoch: 3/40... Step: 590... Loss: 2.6167... Val Loss: 2.4739\n",
            "Epoch: 3/40... Step: 600... Loss: 2.5925... Val Loss: 2.4632\n",
            "Epoch: 3/40... Step: 610... Loss: 2.3988... Val Loss: 2.4219\n",
            "Epoch: 3/40... Step: 620... Loss: 2.4942... Val Loss: 2.4065\n",
            "Epoch: 3/40... Step: 630... Loss: 2.3762... Val Loss: 2.4667\n",
            "Epoch: 3/40... Step: 640... Loss: 2.1955... Val Loss: 2.4081\n",
            "Epoch: 3/40... Step: 650... Loss: 2.5113... Val Loss: 2.4641\n",
            "Epoch: 3/40... Step: 660... Loss: 2.4636... Val Loss: 2.4103\n",
            "Epoch: 3/40... Step: 670... Loss: 2.2996... Val Loss: 2.3638\n",
            "Epoch: 3/40... Step: 680... Loss: 2.1886... Val Loss: 2.3434\n",
            "Epoch: 3/40... Step: 690... Loss: 2.5901... Val Loss: 2.3249\n",
            "Epoch: 3/40... Step: 700... Loss: 2.3669... Val Loss: 2.3347\n",
            "Epoch: 3/40... Step: 710... Loss: 2.6392... Val Loss: 2.2939\n",
            "Epoch: 3/40... Step: 720... Loss: 2.4191... Val Loss: 2.2845\n",
            "Epoch: 3/40... Step: 730... Loss: 2.1740... Val Loss: 2.2676\n",
            "Epoch: 4/40... Step: 740... Loss: 2.4044... Val Loss: 2.2462\n",
            "Epoch: 4/40... Step: 750... Loss: 2.5304... Val Loss: 2.2406\n",
            "Epoch: 4/40... Step: 760... Loss: 2.1461... Val Loss: 2.2269\n",
            "Epoch: 4/40... Step: 770... Loss: 2.1853... Val Loss: 2.2007\n",
            "Epoch: 4/40... Step: 780... Loss: 2.1852... Val Loss: 2.2198\n",
            "Epoch: 4/40... Step: 790... Loss: 2.2180... Val Loss: 2.2161\n",
            "Epoch: 4/40... Step: 800... Loss: 2.1286... Val Loss: 2.1787\n",
            "Epoch: 4/40... Step: 810... Loss: 2.2438... Val Loss: 2.1768\n",
            "Epoch: 4/40... Step: 820... Loss: 2.1712... Val Loss: 2.1603\n",
            "Epoch: 4/40... Step: 830... Loss: 2.3592... Val Loss: 2.1576\n",
            "Epoch: 4/40... Step: 840... Loss: 2.1463... Val Loss: 2.1834\n",
            "Epoch: 4/40... Step: 850... Loss: 2.1769... Val Loss: 2.1724\n",
            "Epoch: 4/40... Step: 860... Loss: 2.0700... Val Loss: 2.1536\n",
            "Epoch: 4/40... Step: 870... Loss: 2.0471... Val Loss: 2.1606\n",
            "Epoch: 4/40... Step: 880... Loss: 2.0068... Val Loss: 2.1943\n",
            "Epoch: 4/40... Step: 890... Loss: 2.2322... Val Loss: 2.0971\n",
            "Epoch: 4/40... Step: 900... Loss: 2.5593... Val Loss: 2.0811\n",
            "Epoch: 4/40... Step: 910... Loss: 2.3997... Val Loss: 2.0902\n",
            "Epoch: 4/40... Step: 920... Loss: 1.9638... Val Loss: 2.0560\n",
            "Epoch: 4/40... Step: 930... Loss: 2.2644... Val Loss: 2.0657\n",
            "Epoch: 4/40... Step: 940... Loss: 2.4758... Val Loss: 2.0565\n",
            "Epoch: 4/40... Step: 950... Loss: 2.0751... Val Loss: 2.0462\n",
            "Epoch: 4/40... Step: 960... Loss: 2.5297... Val Loss: 2.0335\n",
            "Epoch: 4/40... Step: 970... Loss: 2.3240... Val Loss: 2.0599\n",
            "Epoch: 4/40... Step: 980... Loss: 2.1294... Val Loss: 2.0173\n",
            "Epoch: 5/40... Step: 990... Loss: 2.2478... Val Loss: 2.0158\n",
            "Epoch: 5/40... Step: 1000... Loss: 2.1199... Val Loss: 2.0178\n",
            "Epoch: 5/40... Step: 1010... Loss: 1.7529... Val Loss: 2.0059\n",
            "Epoch: 5/40... Step: 1020... Loss: 2.3345... Val Loss: 1.9964\n",
            "Epoch: 5/40... Step: 1030... Loss: 2.0843... Val Loss: 1.9853\n",
            "Epoch: 5/40... Step: 1040... Loss: 1.6763... Val Loss: 1.9849\n",
            "Epoch: 5/40... Step: 1050... Loss: 2.1606... Val Loss: 2.0923\n",
            "Epoch: 5/40... Step: 1060... Loss: 2.0179... Val Loss: 2.0824\n",
            "Epoch: 5/40... Step: 1070... Loss: 2.4862... Val Loss: 2.0689\n",
            "Epoch: 5/40... Step: 1080... Loss: 2.1389... Val Loss: 2.0407\n",
            "Epoch: 5/40... Step: 1090... Loss: 2.0080... Val Loss: 1.9956\n",
            "Epoch: 5/40... Step: 1100... Loss: 1.9410... Val Loss: 1.9997\n",
            "Epoch: 5/40... Step: 1110... Loss: 2.0196... Val Loss: 1.9948\n",
            "Epoch: 5/40... Step: 1120... Loss: 1.9288... Val Loss: 1.9595\n",
            "Epoch: 5/40... Step: 1130... Loss: 1.9687... Val Loss: 1.9675\n",
            "Epoch: 5/40... Step: 1140... Loss: 2.0416... Val Loss: 1.9305\n",
            "Epoch: 5/40... Step: 1150... Loss: 1.9965... Val Loss: 1.9347\n",
            "Epoch: 5/40... Step: 1160... Loss: 2.2290... Val Loss: 1.9326\n",
            "Epoch: 5/40... Step: 1170... Loss: 2.0507... Val Loss: 1.9082\n",
            "Epoch: 5/40... Step: 1180... Loss: 1.6749... Val Loss: 1.9086\n",
            "Epoch: 5/40... Step: 1190... Loss: 1.9717... Val Loss: 1.9049\n",
            "Epoch: 5/40... Step: 1200... Loss: 2.0387... Val Loss: 1.8794\n",
            "Epoch: 5/40... Step: 1210... Loss: 2.1962... Val Loss: 1.8863\n",
            "Epoch: 5/40... Step: 1220... Loss: 1.7237... Val Loss: 1.8830\n",
            "Epoch: 5/40... Step: 1230... Loss: 2.2125... Val Loss: 1.8680\n",
            "Epoch: 6/40... Step: 1240... Loss: 2.0025... Val Loss: 1.8860\n",
            "Epoch: 6/40... Step: 1250... Loss: 1.8847... Val Loss: 1.8861\n",
            "Epoch: 6/40... Step: 1260... Loss: 1.9591... Val Loss: 1.8850\n",
            "Epoch: 6/40... Step: 1270... Loss: 2.1264... Val Loss: 1.8825\n",
            "Epoch: 6/40... Step: 1280... Loss: 1.7963... Val Loss: 1.8972\n",
            "Epoch: 6/40... Step: 1290... Loss: 2.0035... Val Loss: 1.8726\n",
            "Epoch: 6/40... Step: 1300... Loss: 1.8563... Val Loss: 1.8693\n",
            "Epoch: 6/40... Step: 1310... Loss: 1.8303... Val Loss: 1.8666\n",
            "Epoch: 6/40... Step: 1320... Loss: 1.7807... Val Loss: 1.8579\n",
            "Epoch: 6/40... Step: 1330... Loss: 1.8307... Val Loss: 1.8767\n",
            "Epoch: 6/40... Step: 1340... Loss: 2.0644... Val Loss: 1.8488\n",
            "Epoch: 6/40... Step: 1350... Loss: 1.6997... Val Loss: 1.8634\n",
            "Epoch: 6/40... Step: 1360... Loss: 2.2012... Val Loss: 1.8653\n",
            "Epoch: 6/40... Step: 1370... Loss: 1.9531... Val Loss: 1.8684\n",
            "Epoch: 6/40... Step: 1380... Loss: 2.2382... Val Loss: 1.8415\n",
            "Epoch: 6/40... Step: 1390... Loss: 1.9559... Val Loss: 1.8307\n",
            "Epoch: 6/40... Step: 1400... Loss: 1.6957... Val Loss: 1.8309\n",
            "Epoch: 6/40... Step: 1410... Loss: 1.5970... Val Loss: 1.8100\n",
            "Epoch: 6/40... Step: 1420... Loss: 2.0373... Val Loss: 1.8077\n",
            "Epoch: 6/40... Step: 1430... Loss: 2.0304... Val Loss: 1.8058\n",
            "Epoch: 6/40... Step: 1440... Loss: 2.3740... Val Loss: 1.8044\n",
            "Epoch: 6/40... Step: 1450... Loss: 1.4335... Val Loss: 1.7784\n",
            "Epoch: 6/40... Step: 1460... Loss: 2.0954... Val Loss: 1.7993\n",
            "Epoch: 6/40... Step: 1470... Loss: 1.6298... Val Loss: 1.7844\n",
            "Epoch: 7/40... Step: 1480... Loss: 1.8409... Val Loss: 1.7791\n",
            "Epoch: 7/40... Step: 1490... Loss: 1.9749... Val Loss: 1.7887\n",
            "Epoch: 7/40... Step: 1500... Loss: 1.9015... Val Loss: 1.8010\n",
            "Epoch: 7/40... Step: 1510... Loss: 1.8920... Val Loss: 1.7813\n",
            "Epoch: 7/40... Step: 1520... Loss: 1.9639... Val Loss: 1.7766\n",
            "Epoch: 7/40... Step: 1530... Loss: 1.5193... Val Loss: 1.7791\n",
            "Epoch: 7/40... Step: 1540... Loss: 1.5041... Val Loss: 1.7638\n",
            "Epoch: 7/40... Step: 1550... Loss: 1.8740... Val Loss: 1.7657\n",
            "Epoch: 7/40... Step: 1560... Loss: 1.6977... Val Loss: 1.7785\n",
            "Epoch: 7/40... Step: 1570... Loss: 2.1736... Val Loss: 1.7742\n",
            "Epoch: 7/40... Step: 1580... Loss: 1.9013... Val Loss: 1.7838\n",
            "Epoch: 7/40... Step: 1590... Loss: 1.6668... Val Loss: 1.7721\n",
            "Epoch: 7/40... Step: 1600... Loss: 1.8590... Val Loss: 1.7779\n",
            "Epoch: 7/40... Step: 1610... Loss: 1.9368... Val Loss: 1.7608\n",
            "Epoch: 7/40... Step: 1620... Loss: 2.0071... Val Loss: 1.7706\n",
            "Epoch: 7/40... Step: 1630... Loss: 1.8781... Val Loss: 1.7648\n",
            "Epoch: 7/40... Step: 1640... Loss: 1.9983... Val Loss: 1.7632\n",
            "Epoch: 7/40... Step: 1650... Loss: 1.8984... Val Loss: 1.7683\n",
            "Epoch: 7/40... Step: 1660... Loss: 1.6009... Val Loss: 1.7370\n",
            "Epoch: 7/40... Step: 1670... Loss: 1.5171... Val Loss: 1.7364\n",
            "Epoch: 7/40... Step: 1680... Loss: 1.9804... Val Loss: 1.7405\n",
            "Epoch: 7/40... Step: 1690... Loss: 1.8967... Val Loss: 1.7214\n",
            "Epoch: 7/40... Step: 1700... Loss: 1.7620... Val Loss: 1.7161\n",
            "Epoch: 7/40... Step: 1710... Loss: 1.9442... Val Loss: 1.7139\n",
            "Epoch: 7/40... Step: 1720... Loss: 1.6102... Val Loss: 1.7097\n",
            "Epoch: 8/40... Step: 1730... Loss: 2.1307... Val Loss: 1.7287\n",
            "Epoch: 8/40... Step: 1740... Loss: 1.4125... Val Loss: 1.7297\n",
            "Epoch: 8/40... Step: 1750... Loss: 1.7897... Val Loss: 1.7243\n",
            "Epoch: 8/40... Step: 1760... Loss: 1.7531... Val Loss: 1.7169\n",
            "Epoch: 8/40... Step: 1770... Loss: 1.6688... Val Loss: 1.7178\n",
            "Epoch: 8/40... Step: 1780... Loss: 1.9397... Val Loss: 1.7243\n",
            "Epoch: 8/40... Step: 1790... Loss: 1.9786... Val Loss: 1.7079\n",
            "Epoch: 8/40... Step: 1800... Loss: 1.4817... Val Loss: 1.7083\n",
            "Epoch: 8/40... Step: 1810... Loss: 1.6242... Val Loss: 1.7089\n",
            "Epoch: 8/40... Step: 1820... Loss: 1.8980... Val Loss: 1.7200\n",
            "Epoch: 8/40... Step: 1830... Loss: 1.7419... Val Loss: 1.7047\n",
            "Epoch: 8/40... Step: 1840... Loss: 1.4493... Val Loss: 1.7174\n",
            "Epoch: 8/40... Step: 1850... Loss: 1.7925... Val Loss: 1.7198\n",
            "Epoch: 8/40... Step: 1860... Loss: 1.8913... Val Loss: 1.7124\n",
            "Epoch: 8/40... Step: 1870... Loss: 1.5048... Val Loss: 1.6970\n",
            "Epoch: 8/40... Step: 1880... Loss: 1.9391... Val Loss: 1.6968\n",
            "Epoch: 8/40... Step: 1890... Loss: 1.7866... Val Loss: 1.6963\n",
            "Epoch: 8/40... Step: 1900... Loss: 1.5800... Val Loss: 1.7098\n",
            "Epoch: 8/40... Step: 1910... Loss: 1.3963... Val Loss: 1.6782\n",
            "Epoch: 8/40... Step: 1920... Loss: 1.6286... Val Loss: 1.6991\n",
            "Epoch: 8/40... Step: 1930... Loss: 1.8563... Val Loss: 1.6684\n",
            "Epoch: 8/40... Step: 1940... Loss: 2.0699... Val Loss: 1.6666\n",
            "Epoch: 8/40... Step: 1950... Loss: 1.9558... Val Loss: 1.6699\n",
            "Epoch: 8/40... Step: 1960... Loss: 1.5652... Val Loss: 1.6658\n",
            "Epoch: 9/40... Step: 1970... Loss: 1.8600... Val Loss: 1.6715\n",
            "Epoch: 9/40... Step: 1980... Loss: 1.9190... Val Loss: 1.6734\n",
            "Epoch: 9/40... Step: 1990... Loss: 1.5942... Val Loss: 1.6786\n",
            "Epoch: 9/40... Step: 2000... Loss: 1.5234... Val Loss: 1.6642\n",
            "Epoch: 9/40... Step: 2010... Loss: 1.5689... Val Loss: 1.6676\n",
            "Epoch: 9/40... Step: 2020... Loss: 1.8210... Val Loss: 1.6724\n",
            "Epoch: 9/40... Step: 2030... Loss: 1.5658... Val Loss: 1.6768\n",
            "Epoch: 9/40... Step: 2040... Loss: 1.7441... Val Loss: 1.6637\n",
            "Epoch: 9/40... Step: 2050... Loss: 1.4677... Val Loss: 1.6622\n",
            "Epoch: 9/40... Step: 2060... Loss: 1.9883... Val Loss: 1.6889\n",
            "Epoch: 9/40... Step: 2070... Loss: 1.5244... Val Loss: 1.6862\n",
            "Epoch: 9/40... Step: 2080... Loss: 1.7410... Val Loss: 1.6640\n",
            "Epoch: 9/40... Step: 2090... Loss: 1.5068... Val Loss: 1.6719\n",
            "Epoch: 9/40... Step: 2100... Loss: 1.5329... Val Loss: 1.6611\n",
            "Epoch: 9/40... Step: 2110... Loss: 1.5143... Val Loss: 1.6846\n",
            "Epoch: 9/40... Step: 2120... Loss: 1.7069... Val Loss: 1.6725\n",
            "Epoch: 9/40... Step: 2130... Loss: 2.0737... Val Loss: 1.6640\n",
            "Epoch: 9/40... Step: 2140... Loss: 1.8194... Val Loss: 1.6595\n",
            "Epoch: 9/40... Step: 2150... Loss: 1.3592... Val Loss: 1.6453\n",
            "Epoch: 9/40... Step: 2160... Loss: 1.7250... Val Loss: 1.6356\n",
            "Epoch: 9/40... Step: 2170... Loss: 2.0199... Val Loss: 1.6486\n",
            "Epoch: 9/40... Step: 2180... Loss: 1.3958... Val Loss: 1.6426\n",
            "Epoch: 9/40... Step: 2190... Loss: 2.1551... Val Loss: 1.6232\n",
            "Epoch: 9/40... Step: 2200... Loss: 1.8787... Val Loss: 1.6284\n",
            "Epoch: 9/40... Step: 2210... Loss: 1.5970... Val Loss: 1.6114\n",
            "Epoch: 10/40... Step: 2220... Loss: 1.7447... Val Loss: 1.6265\n",
            "Epoch: 10/40... Step: 2230... Loss: 1.6838... Val Loss: 1.6351\n",
            "Epoch: 10/40... Step: 2240... Loss: 1.1052... Val Loss: 1.6397\n",
            "Epoch: 10/40... Step: 2250... Loss: 1.8781... Val Loss: 1.6261\n",
            "Epoch: 10/40... Step: 2260... Loss: 1.6152... Val Loss: 1.6238\n",
            "Epoch: 10/40... Step: 2270... Loss: 1.2982... Val Loss: 1.6316\n",
            "Epoch: 10/40... Step: 2280... Loss: 1.7755... Val Loss: 1.6309\n",
            "Epoch: 10/40... Step: 2290... Loss: 1.6403... Val Loss: 1.6243\n",
            "Epoch: 10/40... Step: 2300... Loss: 1.5973... Val Loss: 1.6372\n",
            "Epoch: 10/40... Step: 2310... Loss: 1.6725... Val Loss: 1.6435\n",
            "Epoch: 10/40... Step: 2320... Loss: 1.3238... Val Loss: 1.6523\n",
            "Epoch: 10/40... Step: 2330... Loss: 1.4500... Val Loss: 1.6301\n",
            "Epoch: 10/40... Step: 2340... Loss: 1.6645... Val Loss: 1.6403\n",
            "Epoch: 10/40... Step: 2350... Loss: 1.6180... Val Loss: 1.6271\n",
            "Epoch: 10/40... Step: 2360... Loss: 1.5917... Val Loss: 1.6465\n",
            "Epoch: 10/40... Step: 2370... Loss: 1.6282... Val Loss: 1.6406\n",
            "Epoch: 10/40... Step: 2380... Loss: 1.5900... Val Loss: 1.6298\n",
            "Epoch: 10/40... Step: 2390... Loss: 1.7163... Val Loss: 1.6251\n",
            "Epoch: 10/40... Step: 2400... Loss: 1.6762... Val Loss: 1.6142\n",
            "Epoch: 10/40... Step: 2410... Loss: 1.3690... Val Loss: 1.6110\n",
            "Epoch: 10/40... Step: 2420... Loss: 1.5148... Val Loss: 1.6058\n",
            "Epoch: 10/40... Step: 2430... Loss: 1.6613... Val Loss: 1.6098\n",
            "Epoch: 10/40... Step: 2440... Loss: 1.9545... Val Loss: 1.5989\n",
            "Epoch: 10/40... Step: 2450... Loss: 1.4959... Val Loss: 1.5978\n",
            "Epoch: 10/40... Step: 2460... Loss: 1.7132... Val Loss: 1.5867\n",
            "Epoch: 11/40... Step: 2470... Loss: 1.6223... Val Loss: 1.6086\n",
            "Epoch: 11/40... Step: 2480... Loss: 1.6341... Val Loss: 1.6095\n",
            "Epoch: 11/40... Step: 2490... Loss: 1.4299... Val Loss: 1.6136\n",
            "Epoch: 11/40... Step: 2500... Loss: 1.7180... Val Loss: 1.6181\n",
            "Epoch: 11/40... Step: 2510... Loss: 1.6931... Val Loss: 1.6120\n",
            "Epoch: 11/40... Step: 2520... Loss: 1.5800... Val Loss: 1.6113\n",
            "Epoch: 11/40... Step: 2530... Loss: 1.5318... Val Loss: 1.6013\n",
            "Epoch: 11/40... Step: 2540... Loss: 1.4573... Val Loss: 1.6089\n",
            "Epoch: 11/40... Step: 2550... Loss: 1.4165... Val Loss: 1.6166\n",
            "Epoch: 11/40... Step: 2560... Loss: 1.5005... Val Loss: 1.6249\n",
            "Epoch: 11/40... Step: 2570... Loss: 1.6076... Val Loss: 1.6161\n",
            "Epoch: 11/40... Step: 2580... Loss: 1.2362... Val Loss: 1.6310\n",
            "Epoch: 11/40... Step: 2590... Loss: 1.8783... Val Loss: 1.6264\n",
            "Epoch: 11/40... Step: 2600... Loss: 1.5140... Val Loss: 1.6166\n",
            "Epoch: 11/40... Step: 2610... Loss: 1.8328... Val Loss: 1.6066\n",
            "Epoch: 11/40... Step: 2620... Loss: 1.6230... Val Loss: 1.6082\n",
            "Epoch: 11/40... Step: 2630... Loss: 1.2456... Val Loss: 1.6152\n",
            "Epoch: 11/40... Step: 2640... Loss: 1.3334... Val Loss: 1.6007\n",
            "Epoch: 11/40... Step: 2650... Loss: 1.6858... Val Loss: 1.5903\n",
            "Epoch: 11/40... Step: 2660... Loss: 1.6581... Val Loss: 1.6037\n",
            "Epoch: 11/40... Step: 2670... Loss: 1.9389... Val Loss: 1.5871\n",
            "Epoch: 11/40... Step: 2680... Loss: 0.9705... Val Loss: 1.5848\n",
            "Epoch: 11/40... Step: 2690... Loss: 1.8909... Val Loss: 1.5800\n",
            "Epoch: 11/40... Step: 2700... Loss: 1.2194... Val Loss: 1.5715\n",
            "Epoch: 12/40... Step: 2710... Loss: 1.4794... Val Loss: 1.5818\n",
            "Epoch: 12/40... Step: 2720... Loss: 1.7694... Val Loss: 1.5825\n",
            "Epoch: 12/40... Step: 2730... Loss: 1.3870... Val Loss: 1.5796\n",
            "Epoch: 12/40... Step: 2740... Loss: 1.5447... Val Loss: 1.5958\n",
            "Epoch: 12/40... Step: 2750... Loss: 1.6034... Val Loss: 1.5913\n",
            "Epoch: 12/40... Step: 2760... Loss: 1.1920... Val Loss: 1.5961\n",
            "Epoch: 12/40... Step: 2770... Loss: 1.4549... Val Loss: 1.5942\n",
            "Epoch: 12/40... Step: 2780... Loss: 1.5624... Val Loss: 1.5918\n",
            "Epoch: 12/40... Step: 2790... Loss: 1.6048... Val Loss: 1.6096\n",
            "Epoch: 12/40... Step: 2800... Loss: 1.8687... Val Loss: 1.6006\n",
            "Epoch: 12/40... Step: 2810... Loss: 1.5547... Val Loss: 1.6159\n",
            "Epoch: 12/40... Step: 2820... Loss: 1.4539... Val Loss: 1.6102\n",
            "Epoch: 12/40... Step: 2830... Loss: 1.4667... Val Loss: 1.6101\n",
            "Epoch: 12/40... Step: 2840... Loss: 1.6265... Val Loss: 1.5945\n",
            "Epoch: 12/40... Step: 2850... Loss: 1.5981... Val Loss: 1.5872\n",
            "Epoch: 12/40... Step: 2860... Loss: 1.5847... Val Loss: 1.5775\n",
            "Epoch: 12/40... Step: 2870... Loss: 1.5418... Val Loss: 1.5842\n",
            "Epoch: 12/40... Step: 2880... Loss: 1.3599... Val Loss: 1.5965\n",
            "Epoch: 12/40... Step: 2890... Loss: 1.2454... Val Loss: 1.5911\n",
            "Epoch: 12/40... Step: 2900... Loss: 1.2485... Val Loss: 1.5998\n",
            "Epoch: 12/40... Step: 2910... Loss: 1.5960... Val Loss: 1.5901\n",
            "Epoch: 12/40... Step: 2920... Loss: 1.5120... Val Loss: 1.5692\n",
            "Epoch: 12/40... Step: 2930... Loss: 1.4705... Val Loss: 1.5703\n",
            "Epoch: 12/40... Step: 2940... Loss: 1.6131... Val Loss: 1.5622\n",
            "Epoch: 12/40... Step: 2950... Loss: 1.3994... Val Loss: 1.5621\n",
            "Epoch: 13/40... Step: 2960... Loss: 1.8163... Val Loss: 1.5893\n",
            "Epoch: 13/40... Step: 2970... Loss: 1.1757... Val Loss: 1.5976\n",
            "Epoch: 13/40... Step: 2980... Loss: 1.3635... Val Loss: 1.6000\n",
            "Epoch: 13/40... Step: 2990... Loss: 1.3992... Val Loss: 1.5849\n",
            "Epoch: 13/40... Step: 3000... Loss: 1.4359... Val Loss: 1.5846\n",
            "Epoch: 13/40... Step: 3010... Loss: 1.6941... Val Loss: 1.5808\n",
            "Epoch: 13/40... Step: 3020... Loss: 1.5834... Val Loss: 1.5798\n",
            "Epoch: 13/40... Step: 3030... Loss: 1.0944... Val Loss: 1.5853\n",
            "Epoch: 13/40... Step: 3040... Loss: 1.3868... Val Loss: 1.5952\n",
            "Epoch: 13/40... Step: 3050... Loss: 1.4509... Val Loss: 1.6086\n",
            "Epoch: 13/40... Step: 3060... Loss: 1.4246... Val Loss: 1.5890\n",
            "Epoch: 13/40... Step: 3070... Loss: 1.1650... Val Loss: 1.5845\n",
            "Epoch: 13/40... Step: 3080... Loss: 1.5904... Val Loss: 1.5921\n",
            "Epoch: 13/40... Step: 3090... Loss: 1.6724... Val Loss: 1.5891\n",
            "Epoch: 13/40... Step: 3100... Loss: 1.2854... Val Loss: 1.6111\n",
            "Epoch: 13/40... Step: 3110... Loss: 1.4993... Val Loss: 1.5879\n",
            "Epoch: 13/40... Step: 3120... Loss: 1.5342... Val Loss: 1.5874\n",
            "Epoch: 13/40... Step: 3130... Loss: 1.3395... Val Loss: 1.5776\n",
            "Epoch: 13/40... Step: 3140... Loss: 1.1127... Val Loss: 1.5771\n",
            "Epoch: 13/40... Step: 3150... Loss: 1.2413... Val Loss: 1.5840\n",
            "Epoch: 13/40... Step: 3160... Loss: 1.6030... Val Loss: 1.5656\n",
            "Epoch: 13/40... Step: 3170... Loss: 1.6693... Val Loss: 1.5606\n",
            "Epoch: 13/40... Step: 3180... Loss: 1.5502... Val Loss: 1.5548\n",
            "Epoch: 13/40... Step: 3190... Loss: 1.3959... Val Loss: 1.5573\n",
            "Epoch: 14/40... Step: 3200... Loss: 1.5722... Val Loss: 1.5471\n",
            "Epoch: 14/40... Step: 3210... Loss: 1.4473... Val Loss: 1.5531\n",
            "Epoch: 14/40... Step: 3220... Loss: 1.1783... Val Loss: 1.5641\n",
            "Epoch: 14/40... Step: 3230... Loss: 1.3473... Val Loss: 1.5824\n",
            "Epoch: 14/40... Step: 3240... Loss: 1.3284... Val Loss: 1.5760\n",
            "Epoch: 14/40... Step: 3250... Loss: 1.6051... Val Loss: 1.5814\n",
            "Epoch: 14/40... Step: 3260... Loss: 1.3725... Val Loss: 1.5813\n",
            "Epoch: 14/40... Step: 3270... Loss: 1.4312... Val Loss: 1.5685\n",
            "Epoch: 14/40... Step: 3280... Loss: 1.1854... Val Loss: 1.5774\n",
            "Epoch: 14/40... Step: 3290... Loss: 1.7343... Val Loss: 1.5943\n",
            "Epoch: 14/40... Step: 3300... Loss: 1.3172... Val Loss: 1.6026\n",
            "Epoch: 14/40... Step: 3310... Loss: 1.4100... Val Loss: 1.5902\n",
            "Epoch: 14/40... Step: 3320... Loss: 1.1957... Val Loss: 1.5909\n",
            "Epoch: 14/40... Step: 3330... Loss: 1.3753... Val Loss: 1.5979\n",
            "Epoch: 14/40... Step: 3340... Loss: 1.2888... Val Loss: 1.5936\n",
            "Epoch: 14/40... Step: 3350... Loss: 1.5286... Val Loss: 1.5985\n",
            "Epoch: 14/40... Step: 3360... Loss: 1.6292... Val Loss: 1.5889\n",
            "Epoch: 14/40... Step: 3370... Loss: 1.5376... Val Loss: 1.5973\n",
            "Epoch: 14/40... Step: 3380... Loss: 1.1341... Val Loss: 1.5906\n",
            "Epoch: 14/40... Step: 3390... Loss: 1.4179... Val Loss: 1.5922\n",
            "Epoch: 14/40... Step: 3400... Loss: 1.6017... Val Loss: 1.5976\n",
            "Epoch: 14/40... Step: 3410... Loss: 1.0453... Val Loss: 1.5815\n",
            "Epoch: 14/40... Step: 3420... Loss: 1.7761... Val Loss: 1.5702\n",
            "Epoch: 14/40... Step: 3430... Loss: 1.4402... Val Loss: 1.5508\n",
            "Epoch: 14/40... Step: 3440... Loss: 1.3143... Val Loss: 1.5566\n",
            "Epoch: 15/40... Step: 3450... Loss: 1.5037... Val Loss: 1.5614\n",
            "Epoch: 15/40... Step: 3460... Loss: 1.5302... Val Loss: 1.5726\n",
            "Epoch: 15/40... Step: 3470... Loss: 0.8583... Val Loss: 1.5825\n",
            "Epoch: 15/40... Step: 3480... Loss: 1.6641... Val Loss: 1.5786\n",
            "Epoch: 15/40... Step: 3490... Loss: 1.2560... Val Loss: 1.5730\n",
            "Epoch: 15/40... Step: 3500... Loss: 1.1037... Val Loss: 1.5707\n",
            "Epoch: 15/40... Step: 3510... Loss: 1.3534... Val Loss: 1.5773\n",
            "Epoch: 15/40... Step: 3520... Loss: 1.3931... Val Loss: 1.5802\n",
            "Epoch: 15/40... Step: 3530... Loss: 1.2211... Val Loss: 1.5964\n",
            "Epoch: 15/40... Step: 3540... Loss: 1.3857... Val Loss: 1.6061\n",
            "Epoch: 15/40... Step: 3550... Loss: 0.9208... Val Loss: 1.6028\n",
            "Epoch: 15/40... Step: 3560... Loss: 1.2950... Val Loss: 1.5886\n",
            "Epoch: 15/40... Step: 3570... Loss: 1.2673... Val Loss: 1.5914\n",
            "Epoch: 15/40... Step: 3580... Loss: 1.2909... Val Loss: 1.5899\n",
            "Epoch: 15/40... Step: 3590... Loss: 1.3344... Val Loss: 1.5897\n",
            "Epoch: 15/40... Step: 3600... Loss: 1.4803... Val Loss: 1.6021\n",
            "Epoch: 15/40... Step: 3610... Loss: 1.3536... Val Loss: 1.5835\n",
            "Epoch: 15/40... Step: 3620... Loss: 1.5496... Val Loss: 1.5879\n",
            "Epoch: 15/40... Step: 3630... Loss: 1.3684... Val Loss: 1.5984\n",
            "Epoch: 15/40... Step: 3640... Loss: 1.0785... Val Loss: 1.5978\n",
            "Epoch: 15/40... Step: 3650... Loss: 1.2786... Val Loss: 1.5761\n",
            "Epoch: 15/40... Step: 3660... Loss: 1.3983... Val Loss: 1.5727\n",
            "Epoch: 15/40... Step: 3670... Loss: 1.5903... Val Loss: 1.5619\n",
            "Epoch: 15/40... Step: 3680... Loss: 1.2106... Val Loss: 1.5651\n",
            "Epoch: 15/40... Step: 3690... Loss: 1.2792... Val Loss: 1.5689\n",
            "Epoch: 16/40... Step: 3700... Loss: 1.4780... Val Loss: 1.5735\n",
            "Epoch: 16/40... Step: 3710... Loss: 1.2889... Val Loss: 1.5726\n",
            "Epoch: 16/40... Step: 3720... Loss: 1.1299... Val Loss: 1.5766\n",
            "Epoch: 16/40... Step: 3730... Loss: 1.3913... Val Loss: 1.5792\n",
            "Epoch: 16/40... Step: 3740... Loss: 1.3368... Val Loss: 1.5878\n",
            "Epoch: 16/40... Step: 3750... Loss: 1.3661... Val Loss: 1.5873\n",
            "Epoch: 16/40... Step: 3760... Loss: 1.2524... Val Loss: 1.5697\n",
            "Epoch: 16/40... Step: 3770... Loss: 1.2173... Val Loss: 1.5732\n",
            "Epoch: 16/40... Step: 3780... Loss: 1.1077... Val Loss: 1.5935\n",
            "Epoch: 16/40... Step: 3790... Loss: 1.1597... Val Loss: 1.6105\n",
            "Epoch: 16/40... Step: 3800... Loss: 1.2765... Val Loss: 1.5999\n",
            "Epoch: 16/40... Step: 3810... Loss: 1.0758... Val Loss: 1.5862\n",
            "Epoch: 16/40... Step: 3820... Loss: 1.5094... Val Loss: 1.5938\n",
            "Epoch: 16/40... Step: 3830... Loss: 1.2439... Val Loss: 1.5834\n",
            "Epoch: 16/40... Step: 3840... Loss: 1.4127... Val Loss: 1.5935\n",
            "Epoch: 16/40... Step: 3850... Loss: 1.3908... Val Loss: 1.5970\n",
            "Epoch: 16/40... Step: 3860... Loss: 1.0991... Val Loss: 1.5996\n",
            "Epoch: 16/40... Step: 3870... Loss: 1.2040... Val Loss: 1.5948\n",
            "Epoch: 16/40... Step: 3880... Loss: 1.4173... Val Loss: 1.5925\n",
            "Epoch: 16/40... Step: 3890... Loss: 1.3827... Val Loss: 1.5951\n",
            "Epoch: 16/40... Step: 3900... Loss: 1.5680... Val Loss: 1.5617\n",
            "Epoch: 16/40... Step: 3910... Loss: 0.7295... Val Loss: 1.5535\n",
            "Epoch: 16/40... Step: 3920... Loss: 1.5192... Val Loss: 1.5695\n",
            "Epoch: 16/40... Step: 3930... Loss: 1.0599... Val Loss: 1.5595\n",
            "Epoch: 17/40... Step: 3940... Loss: 1.2133... Val Loss: 1.5531\n",
            "Epoch: 17/40... Step: 3950... Loss: 1.3123... Val Loss: 1.5602\n",
            "Epoch: 17/40... Step: 3960... Loss: 1.1834... Val Loss: 1.5727\n",
            "Epoch: 17/40... Step: 3970... Loss: 1.1898... Val Loss: 1.5817\n",
            "Epoch: 17/40... Step: 3980... Loss: 1.3333... Val Loss: 1.5751\n",
            "Epoch: 17/40... Step: 3990... Loss: 0.9510... Val Loss: 1.5947\n",
            "Epoch: 17/40... Step: 4000... Loss: 1.3052... Val Loss: 1.6078\n",
            "Epoch: 17/40... Step: 4010... Loss: 1.2026... Val Loss: 1.6033\n",
            "Epoch: 17/40... Step: 4020... Loss: 1.2886... Val Loss: 1.5924\n",
            "Epoch: 17/40... Step: 4030... Loss: 1.5083... Val Loss: 1.6104\n",
            "Epoch: 17/40... Step: 4040... Loss: 1.3698... Val Loss: 1.6167\n",
            "Epoch: 17/40... Step: 4050... Loss: 1.1835... Val Loss: 1.6055\n",
            "Epoch: 17/40... Step: 4060... Loss: 1.3983... Val Loss: 1.6109\n",
            "Epoch: 17/40... Step: 4070... Loss: 1.2115... Val Loss: 1.6054\n",
            "Epoch: 17/40... Step: 4080... Loss: 1.3545... Val Loss: 1.6106\n",
            "Epoch: 17/40... Step: 4090... Loss: 1.2316... Val Loss: 1.6184\n",
            "Epoch: 17/40... Step: 4100... Loss: 1.2581... Val Loss: 1.6148\n",
            "Epoch: 17/40... Step: 4110... Loss: 1.2073... Val Loss: 1.6103\n",
            "Epoch: 17/40... Step: 4120... Loss: 1.0138... Val Loss: 1.6163\n",
            "Epoch: 17/40... Step: 4130... Loss: 1.0754... Val Loss: 1.6091\n",
            "Epoch: 17/40... Step: 4140... Loss: 1.3040... Val Loss: 1.6085\n",
            "Epoch: 17/40... Step: 4150... Loss: 1.3338... Val Loss: 1.5904\n",
            "Epoch: 17/40... Step: 4160... Loss: 1.1082... Val Loss: 1.5955\n",
            "Epoch: 17/40... Step: 4170... Loss: 1.4314... Val Loss: 1.5874\n",
            "Epoch: 17/40... Step: 4180... Loss: 1.2361... Val Loss: 1.5831\n",
            "Epoch: 18/40... Step: 4190... Loss: 1.4319... Val Loss: 1.5957\n",
            "Epoch: 18/40... Step: 4200... Loss: 0.8521... Val Loss: 1.5929\n",
            "Epoch: 18/40... Step: 4210... Loss: 1.2061... Val Loss: 1.5921\n",
            "Epoch: 18/40... Step: 4220... Loss: 1.1970... Val Loss: 1.5989\n",
            "Epoch: 18/40... Step: 4230... Loss: 1.2176... Val Loss: 1.5950\n",
            "Epoch: 18/40... Step: 4240... Loss: 1.2607... Val Loss: 1.6111\n",
            "Epoch: 18/40... Step: 4250... Loss: 1.2204... Val Loss: 1.6023\n",
            "Epoch: 18/40... Step: 4260... Loss: 0.9391... Val Loss: 1.5968\n",
            "Epoch: 18/40... Step: 4270... Loss: 0.9711... Val Loss: 1.6001\n",
            "Epoch: 18/40... Step: 4280... Loss: 1.2785... Val Loss: 1.6222\n",
            "Epoch: 18/40... Step: 4290... Loss: 1.1795... Val Loss: 1.6264\n",
            "Epoch: 18/40... Step: 4300... Loss: 0.9750... Val Loss: 1.6119\n",
            "Epoch: 18/40... Step: 4310... Loss: 1.1703... Val Loss: 1.6126\n",
            "Epoch: 18/40... Step: 4320... Loss: 1.3904... Val Loss: 1.6114\n",
            "Epoch: 18/40... Step: 4330... Loss: 1.1544... Val Loss: 1.6155\n",
            "Epoch: 18/40... Step: 4340... Loss: 1.3142... Val Loss: 1.6025\n",
            "Epoch: 18/40... Step: 4350... Loss: 1.3059... Val Loss: 1.5934\n",
            "Epoch: 18/40... Step: 4360... Loss: 1.2527... Val Loss: 1.6054\n",
            "Epoch: 18/40... Step: 4370... Loss: 0.9774... Val Loss: 1.6132\n",
            "Epoch: 18/40... Step: 4380... Loss: 1.1797... Val Loss: 1.6109\n",
            "Epoch: 18/40... Step: 4390... Loss: 1.4281... Val Loss: 1.6023\n",
            "Epoch: 18/40... Step: 4400... Loss: 1.5132... Val Loss: 1.6062\n",
            "Epoch: 18/40... Step: 4410... Loss: 1.2942... Val Loss: 1.6050\n",
            "Epoch: 18/40... Step: 4420... Loss: 1.1868... Val Loss: 1.5905\n",
            "Epoch: 19/40... Step: 4430... Loss: 1.2221... Val Loss: 1.5944\n",
            "Epoch: 19/40... Step: 4440... Loss: 1.2419... Val Loss: 1.5941\n",
            "Epoch: 19/40... Step: 4450... Loss: 1.1326... Val Loss: 1.6025\n",
            "Epoch: 19/40... Step: 4460... Loss: 1.1750... Val Loss: 1.6167\n",
            "Epoch: 19/40... Step: 4470... Loss: 1.1240... Val Loss: 1.6106\n",
            "Epoch: 19/40... Step: 4480... Loss: 1.4265... Val Loss: 1.6080\n",
            "Epoch: 19/40... Step: 4490... Loss: 1.2238... Val Loss: 1.6233\n",
            "Epoch: 19/40... Step: 4500... Loss: 1.1387... Val Loss: 1.6328\n",
            "Epoch: 19/40... Step: 4510... Loss: 0.8954... Val Loss: 1.6346\n",
            "Epoch: 19/40... Step: 4520... Loss: 1.4574... Val Loss: 1.6345\n",
            "Epoch: 19/40... Step: 4530... Loss: 1.0815... Val Loss: 1.6378\n",
            "Epoch: 19/40... Step: 4540... Loss: 1.1919... Val Loss: 1.6236\n",
            "Epoch: 19/40... Step: 4550... Loss: 0.9939... Val Loss: 1.6222\n",
            "Epoch: 19/40... Step: 4560... Loss: 1.1392... Val Loss: 1.6191\n",
            "Epoch: 19/40... Step: 4570... Loss: 1.2017... Val Loss: 1.6256\n",
            "Epoch: 19/40... Step: 4580... Loss: 1.1912... Val Loss: 1.6381\n",
            "Epoch: 19/40... Step: 4590... Loss: 1.2446... Val Loss: 1.6387\n",
            "Epoch: 19/40... Step: 4600... Loss: 1.0631... Val Loss: 1.6182\n",
            "Epoch: 19/40... Step: 4610... Loss: 0.9201... Val Loss: 1.6143\n",
            "Epoch: 19/40... Step: 4620... Loss: 1.2303... Val Loss: 1.6132\n",
            "Epoch: 19/40... Step: 4630... Loss: 1.4864... Val Loss: 1.6177\n",
            "Epoch: 19/40... Step: 4640... Loss: 1.0003... Val Loss: 1.6011\n",
            "Epoch: 19/40... Step: 4650... Loss: 1.3253... Val Loss: 1.5962\n",
            "Epoch: 19/40... Step: 4660... Loss: 1.1885... Val Loss: 1.5941\n",
            "Epoch: 19/40... Step: 4670... Loss: 1.0139... Val Loss: 1.6038\n",
            "Epoch: 20/40... Step: 4680... Loss: 1.3691... Val Loss: 1.6015\n",
            "Epoch: 20/40... Step: 4690... Loss: 1.2357... Val Loss: 1.5923\n",
            "Epoch: 20/40... Step: 4700... Loss: 0.7136... Val Loss: 1.6006\n",
            "Epoch: 20/40... Step: 4710... Loss: 1.3117... Val Loss: 1.6036\n",
            "Epoch: 20/40... Step: 4720... Loss: 1.0713... Val Loss: 1.5991\n",
            "Epoch: 20/40... Step: 4730... Loss: 1.0313... Val Loss: 1.6225\n",
            "Epoch: 20/40... Step: 4740... Loss: 1.1788... Val Loss: 1.6310\n",
            "Epoch: 20/40... Step: 4750... Loss: 1.1714... Val Loss: 1.6312\n",
            "Epoch: 20/40... Step: 4760... Loss: 1.0789... Val Loss: 1.6370\n",
            "Epoch: 20/40... Step: 4770... Loss: 1.1503... Val Loss: 1.6540\n",
            "Epoch: 20/40... Step: 4780... Loss: 0.7928... Val Loss: 1.6435\n",
            "Epoch: 20/40... Step: 4790... Loss: 0.9471... Val Loss: 1.6418\n",
            "Epoch: 20/40... Step: 4800... Loss: 1.0455... Val Loss: 1.6382\n",
            "Epoch: 20/40... Step: 4810... Loss: 1.1437... Val Loss: 1.6406\n",
            "Epoch: 20/40... Step: 4820... Loss: 1.0743... Val Loss: 1.6406\n",
            "Epoch: 20/40... Step: 4830... Loss: 1.1460... Val Loss: 1.6493\n",
            "Epoch: 20/40... Step: 4840... Loss: 1.1962... Val Loss: 1.6555\n",
            "Epoch: 20/40... Step: 4850... Loss: 1.2541... Val Loss: 1.6439\n",
            "Epoch: 20/40... Step: 4860... Loss: 1.0980... Val Loss: 1.6569\n",
            "Epoch: 20/40... Step: 4870... Loss: 0.9778... Val Loss: 1.6399\n",
            "Epoch: 20/40... Step: 4880... Loss: 1.0982... Val Loss: 1.6521\n",
            "Epoch: 20/40... Step: 4890... Loss: 1.2840... Val Loss: 1.6337\n",
            "Epoch: 20/40... Step: 4900... Loss: 1.3613... Val Loss: 1.6168\n",
            "Epoch: 20/40... Step: 4910... Loss: 0.9784... Val Loss: 1.6057\n",
            "Epoch: 20/40... Step: 4920... Loss: 1.0519... Val Loss: 1.5977\n",
            "Epoch: 21/40... Step: 4930... Loss: 1.2523... Val Loss: 1.6050\n",
            "Epoch: 21/40... Step: 4940... Loss: 1.1229... Val Loss: 1.6077\n",
            "Epoch: 21/40... Step: 4950... Loss: 0.9034... Val Loss: 1.6246\n",
            "Epoch: 21/40... Step: 4960... Loss: 1.1686... Val Loss: 1.6203\n",
            "Epoch: 21/40... Step: 4970... Loss: 1.1752... Val Loss: 1.6324\n",
            "Epoch: 21/40... Step: 4980... Loss: 1.0819... Val Loss: 1.6532\n",
            "Epoch: 21/40... Step: 4990... Loss: 1.0475... Val Loss: 1.6571\n",
            "Epoch: 21/40... Step: 5000... Loss: 1.0108... Val Loss: 1.6579\n",
            "Epoch: 21/40... Step: 5010... Loss: 0.9403... Val Loss: 1.6624\n",
            "Epoch: 21/40... Step: 5020... Loss: 0.9648... Val Loss: 1.6671\n",
            "Epoch: 21/40... Step: 5030... Loss: 1.0494... Val Loss: 1.6484\n",
            "Epoch: 21/40... Step: 5040... Loss: 0.9053... Val Loss: 1.6463\n",
            "Epoch: 21/40... Step: 5050... Loss: 1.1539... Val Loss: 1.6470\n",
            "Epoch: 21/40... Step: 5060... Loss: 1.1152... Val Loss: 1.6537\n",
            "Epoch: 21/40... Step: 5070... Loss: 1.1572... Val Loss: 1.6647\n",
            "Epoch: 21/40... Step: 5080... Loss: 1.1463... Val Loss: 1.6740\n",
            "Epoch: 21/40... Step: 5090... Loss: 0.9725... Val Loss: 1.6649\n",
            "Epoch: 21/40... Step: 5100... Loss: 1.0140... Val Loss: 1.6487\n",
            "Epoch: 21/40... Step: 5110... Loss: 1.3825... Val Loss: 1.6498\n",
            "Epoch: 21/40... Step: 5120... Loss: 1.2461... Val Loss: 1.6627\n",
            "Epoch: 21/40... Step: 5130... Loss: 1.2449... Val Loss: 1.6693\n",
            "Epoch: 21/40... Step: 5140... Loss: 0.6483... Val Loss: 1.6589\n",
            "Epoch: 21/40... Step: 5150... Loss: 1.4005... Val Loss: 1.6416\n",
            "Epoch: 21/40... Step: 5160... Loss: 0.7909... Val Loss: 1.6352\n",
            "Epoch: 22/40... Step: 5170... Loss: 1.0872... Val Loss: 1.6340\n",
            "Epoch: 22/40... Step: 5180... Loss: 1.1549... Val Loss: 1.6429\n",
            "Epoch: 22/40... Step: 5190... Loss: 0.8809... Val Loss: 1.6382\n",
            "Epoch: 22/40... Step: 5200... Loss: 1.0317... Val Loss: 1.6459\n",
            "Epoch: 22/40... Step: 5210... Loss: 1.1033... Val Loss: 1.6489\n",
            "Epoch: 22/40... Step: 5220... Loss: 0.8371... Val Loss: 1.6577\n",
            "Epoch: 22/40... Step: 5230... Loss: 0.9239... Val Loss: 1.6689\n",
            "Epoch: 22/40... Step: 5240... Loss: 1.0113... Val Loss: 1.6901\n",
            "Epoch: 22/40... Step: 5250... Loss: 1.1078... Val Loss: 1.7114\n",
            "Epoch: 22/40... Step: 5260... Loss: 1.2211... Val Loss: 1.7178\n",
            "Epoch: 22/40... Step: 5270... Loss: 0.9464... Val Loss: 1.7155\n",
            "Epoch: 22/40... Step: 5280... Loss: 0.9058... Val Loss: 1.6835\n",
            "Epoch: 22/40... Step: 5290... Loss: 0.9468... Val Loss: 1.6821\n",
            "Epoch: 22/40... Step: 5300... Loss: 0.9469... Val Loss: 1.6888\n",
            "Epoch: 22/40... Step: 5310... Loss: 1.1114... Val Loss: 1.6990\n",
            "Epoch: 22/40... Step: 5320... Loss: 1.0287... Val Loss: 1.6950\n",
            "Epoch: 22/40... Step: 5330... Loss: 1.0345... Val Loss: 1.6976\n",
            "Epoch: 22/40... Step: 5340... Loss: 1.0388... Val Loss: 1.6811\n",
            "Epoch: 22/40... Step: 5350... Loss: 0.9866... Val Loss: 1.6884\n",
            "Epoch: 22/40... Step: 5360... Loss: 0.8912... Val Loss: 1.6863\n",
            "Epoch: 22/40... Step: 5370... Loss: 1.1425... Val Loss: 1.6855\n",
            "Epoch: 22/40... Step: 5380... Loss: 0.9668... Val Loss: 1.6783\n",
            "Epoch: 22/40... Step: 5390... Loss: 0.9065... Val Loss: 1.6745\n",
            "Epoch: 22/40... Step: 5400... Loss: 1.1833... Val Loss: 1.6609\n",
            "Epoch: 22/40... Step: 5410... Loss: 0.9504... Val Loss: 1.6565\n",
            "Epoch: 23/40... Step: 5420... Loss: 1.2360... Val Loss: 1.6513\n",
            "Epoch: 23/40... Step: 5430... Loss: 0.8584... Val Loss: 1.6644\n",
            "Epoch: 23/40... Step: 5440... Loss: 0.9469... Val Loss: 1.6829\n",
            "Epoch: 23/40... Step: 5450... Loss: 1.0756... Val Loss: 1.6781\n",
            "Epoch: 23/40... Step: 5460... Loss: 1.0895... Val Loss: 1.6692\n",
            "Epoch: 23/40... Step: 5470... Loss: 1.1171... Val Loss: 1.6870\n",
            "Epoch: 23/40... Step: 5480... Loss: 1.1096... Val Loss: 1.6919\n",
            "Epoch: 23/40... Step: 5490... Loss: 0.8358... Val Loss: 1.6901\n",
            "Epoch: 23/40... Step: 5500... Loss: 0.9480... Val Loss: 1.7079\n",
            "Epoch: 23/40... Step: 5510... Loss: 1.0281... Val Loss: 1.7172\n",
            "Epoch: 23/40... Step: 5520... Loss: 0.9975... Val Loss: 1.7062\n",
            "Epoch: 23/40... Step: 5530... Loss: 0.7000... Val Loss: 1.6958\n",
            "Epoch: 23/40... Step: 5540... Loss: 1.0525... Val Loss: 1.6983\n",
            "Epoch: 23/40... Step: 5550... Loss: 1.2536... Val Loss: 1.6959\n",
            "Epoch: 23/40... Step: 5560... Loss: 0.9153... Val Loss: 1.6993\n",
            "Epoch: 23/40... Step: 5570... Loss: 0.8961... Val Loss: 1.7083\n",
            "Epoch: 23/40... Step: 5580... Loss: 1.0512... Val Loss: 1.6931\n",
            "Epoch: 23/40... Step: 5590... Loss: 0.9883... Val Loss: 1.6909\n",
            "Epoch: 23/40... Step: 5600... Loss: 0.9542... Val Loss: 1.6934\n",
            "Epoch: 23/40... Step: 5610... Loss: 0.7855... Val Loss: 1.7135\n",
            "Epoch: 23/40... Step: 5620... Loss: 1.1657... Val Loss: 1.7058\n",
            "Epoch: 23/40... Step: 5630... Loss: 1.0247... Val Loss: 1.6973\n",
            "Epoch: 23/40... Step: 5640... Loss: 1.1102... Val Loss: 1.6751\n",
            "Epoch: 23/40... Step: 5650... Loss: 1.0601... Val Loss: 1.6621\n",
            "Epoch: 24/40... Step: 5660... Loss: 1.1810... Val Loss: 1.6682\n",
            "Epoch: 24/40... Step: 5670... Loss: 0.9757... Val Loss: 1.6602\n",
            "Epoch: 24/40... Step: 5680... Loss: 0.8544... Val Loss: 1.6594\n",
            "Epoch: 24/40... Step: 5690... Loss: 0.9453... Val Loss: 1.6698\n",
            "Epoch: 24/40... Step: 5700... Loss: 0.9212... Val Loss: 1.6657\n",
            "Epoch: 24/40... Step: 5710... Loss: 1.0983... Val Loss: 1.6599\n",
            "Epoch: 24/40... Step: 5720... Loss: 1.1123... Val Loss: 1.6723\n",
            "Epoch: 24/40... Step: 5730... Loss: 0.8593... Val Loss: 1.6921\n",
            "Epoch: 24/40... Step: 5740... Loss: 0.8202... Val Loss: 1.7130\n",
            "Epoch: 24/40... Step: 5750... Loss: 1.1724... Val Loss: 1.7301\n",
            "Epoch: 24/40... Step: 5760... Loss: 0.9499... Val Loss: 1.7254\n",
            "Epoch: 24/40... Step: 5770... Loss: 0.9747... Val Loss: 1.7127\n",
            "Epoch: 24/40... Step: 5780... Loss: 0.7997... Val Loss: 1.7182\n",
            "Epoch: 24/40... Step: 5790... Loss: 0.8877... Val Loss: 1.7092\n",
            "Epoch: 24/40... Step: 5800... Loss: 0.8698... Val Loss: 1.7167\n",
            "Epoch: 24/40... Step: 5810... Loss: 0.9057... Val Loss: 1.7325\n",
            "Epoch: 24/40... Step: 5820... Loss: 0.9970... Val Loss: 1.7487\n",
            "Epoch: 24/40... Step: 5830... Loss: 0.8921... Val Loss: 1.7355\n",
            "Epoch: 24/40... Step: 5840... Loss: 0.8629... Val Loss: 1.7279\n",
            "Epoch: 24/40... Step: 5850... Loss: 1.0843... Val Loss: 1.7181\n",
            "Epoch: 24/40... Step: 5860... Loss: 1.1559... Val Loss: 1.7330\n",
            "Epoch: 24/40... Step: 5870... Loss: 0.7400... Val Loss: 1.7299\n",
            "Epoch: 24/40... Step: 5880... Loss: 1.3172... Val Loss: 1.7136\n",
            "Epoch: 24/40... Step: 5890... Loss: 0.8649... Val Loss: 1.6996\n",
            "Epoch: 24/40... Step: 5900... Loss: 0.9033... Val Loss: 1.6955\n",
            "Epoch: 25/40... Step: 5910... Loss: 1.0281... Val Loss: 1.7141\n",
            "Epoch: 25/40... Step: 5920... Loss: 0.9815... Val Loss: 1.7266\n",
            "Epoch: 25/40... Step: 5930... Loss: 0.6289... Val Loss: 1.7365\n",
            "Epoch: 25/40... Step: 5940... Loss: 0.9915... Val Loss: 1.7459\n",
            "Epoch: 25/40... Step: 5950... Loss: 0.8565... Val Loss: 1.7269\n",
            "Epoch: 25/40... Step: 5960... Loss: 0.9326... Val Loss: 1.7120\n",
            "Epoch: 25/40... Step: 5970... Loss: 1.0767... Val Loss: 1.7070\n",
            "Epoch: 25/40... Step: 5980... Loss: 1.0417... Val Loss: 1.7099\n",
            "Epoch: 25/40... Step: 5990... Loss: 0.9122... Val Loss: 1.7266\n",
            "Epoch: 25/40... Step: 6000... Loss: 1.1027... Val Loss: 1.7383\n",
            "Epoch: 25/40... Step: 6010... Loss: 0.6913... Val Loss: 1.7244\n",
            "Epoch: 25/40... Step: 6020... Loss: 0.9335... Val Loss: 1.7152\n",
            "Epoch: 25/40... Step: 6030... Loss: 0.8357... Val Loss: 1.7207\n",
            "Epoch: 25/40... Step: 6040... Loss: 0.9175... Val Loss: 1.7458\n",
            "Epoch: 25/40... Step: 6050... Loss: 1.0280... Val Loss: 1.7614\n",
            "Epoch: 25/40... Step: 6060... Loss: 1.0996... Val Loss: 1.7636\n",
            "Epoch: 25/40... Step: 6070... Loss: 1.0874... Val Loss: 1.7548\n",
            "Epoch: 25/40... Step: 6080... Loss: 0.9335... Val Loss: 1.7382\n",
            "Epoch: 25/40... Step: 6090... Loss: 0.8800... Val Loss: 1.7325\n",
            "Epoch: 25/40... Step: 6100... Loss: 0.7937... Val Loss: 1.7394\n",
            "Epoch: 25/40... Step: 6110... Loss: 0.8247... Val Loss: 1.7370\n",
            "Epoch: 25/40... Step: 6120... Loss: 1.1315... Val Loss: 1.7271\n",
            "Epoch: 25/40... Step: 6130... Loss: 1.0415... Val Loss: 1.7097\n",
            "Epoch: 25/40... Step: 6140... Loss: 0.7541... Val Loss: 1.7162\n",
            "Epoch: 25/40... Step: 6150... Loss: 0.9360... Val Loss: 1.7161\n",
            "Epoch: 26/40... Step: 6160... Loss: 1.2082... Val Loss: 1.7064\n",
            "Epoch: 26/40... Step: 6170... Loss: 0.9235... Val Loss: 1.7125\n",
            "Epoch: 26/40... Step: 6180... Loss: 0.7990... Val Loss: 1.7097\n",
            "Epoch: 26/40... Step: 6190... Loss: 0.9968... Val Loss: 1.7086\n",
            "Epoch: 26/40... Step: 6200... Loss: 1.1970... Val Loss: 1.7020\n",
            "Epoch: 26/40... Step: 6210... Loss: 1.1926... Val Loss: 1.6997\n",
            "Epoch: 26/40... Step: 6220... Loss: 1.0823... Val Loss: 1.7151\n",
            "Epoch: 26/40... Step: 6230... Loss: 0.8789... Val Loss: 1.7325\n",
            "Epoch: 26/40... Step: 6240... Loss: 0.7240... Val Loss: 1.7386\n",
            "Epoch: 26/40... Step: 6250... Loss: 0.8572... Val Loss: 1.7538\n",
            "Epoch: 26/40... Step: 6260... Loss: 0.8554... Val Loss: 1.7586\n",
            "Epoch: 26/40... Step: 6270... Loss: 0.8421... Val Loss: 1.7474\n",
            "Epoch: 26/40... Step: 6280... Loss: 0.9815... Val Loss: 1.7371\n",
            "Epoch: 26/40... Step: 6290... Loss: 1.0460... Val Loss: 1.7444\n",
            "Epoch: 26/40... Step: 6300... Loss: 1.0651... Val Loss: 1.7581\n",
            "Epoch: 26/40... Step: 6310... Loss: 1.1628... Val Loss: 1.7684\n",
            "Epoch: 26/40... Step: 6320... Loss: 0.8546... Val Loss: 1.7608\n",
            "Epoch: 26/40... Step: 6330... Loss: 0.8627... Val Loss: 1.7498\n",
            "Epoch: 26/40... Step: 6340... Loss: 1.1129... Val Loss: 1.7369\n",
            "Epoch: 26/40... Step: 6350... Loss: 0.9718... Val Loss: 1.7348\n",
            "Epoch: 26/40... Step: 6360... Loss: 0.9362... Val Loss: 1.7394\n",
            "Epoch: 26/40... Step: 6370... Loss: 0.6228... Val Loss: 1.7443\n",
            "Epoch: 26/40... Step: 6380... Loss: 1.1906... Val Loss: 1.7287\n",
            "Epoch: 26/40... Step: 6390... Loss: 0.7960... Val Loss: 1.7233\n",
            "Epoch: 27/40... Step: 6400... Loss: 0.8553... Val Loss: 1.7290\n",
            "Epoch: 27/40... Step: 6410... Loss: 1.0453... Val Loss: 1.7234\n",
            "Epoch: 27/40... Step: 6420... Loss: 0.7225... Val Loss: 1.7202\n",
            "Epoch: 27/40... Step: 6430... Loss: 0.8688... Val Loss: 1.7212\n",
            "Epoch: 27/40... Step: 6440... Loss: 0.8512... Val Loss: 1.7245\n",
            "Epoch: 27/40... Step: 6450... Loss: 0.7139... Val Loss: 1.7300\n",
            "Epoch: 27/40... Step: 6460... Loss: 0.9304... Val Loss: 1.7316\n",
            "Epoch: 27/40... Step: 6470... Loss: 0.8399... Val Loss: 1.7345\n",
            "Epoch: 27/40... Step: 6480... Loss: 0.8998... Val Loss: 1.7564\n",
            "Epoch: 27/40... Step: 6490... Loss: 1.0290... Val Loss: 1.7781\n",
            "Epoch: 27/40... Step: 6500... Loss: 0.7091... Val Loss: 1.7712\n",
            "Epoch: 27/40... Step: 6510... Loss: 0.7693... Val Loss: 1.7519\n",
            "Epoch: 27/40... Step: 6520... Loss: 0.9479... Val Loss: 1.7444\n",
            "Epoch: 27/40... Step: 6530... Loss: 0.9313... Val Loss: 1.7448\n",
            "Epoch: 27/40... Step: 6540... Loss: 0.9592... Val Loss: 1.7515\n",
            "Epoch: 27/40... Step: 6550... Loss: 0.9324... Val Loss: 1.7684\n",
            "Epoch: 27/40... Step: 6560... Loss: 0.9492... Val Loss: 1.7762\n",
            "Epoch: 27/40... Step: 6570... Loss: 0.8861... Val Loss: 1.7697\n",
            "Epoch: 27/40... Step: 6580... Loss: 0.9452... Val Loss: 1.7731\n",
            "Epoch: 27/40... Step: 6590... Loss: 0.7484... Val Loss: 1.7706\n",
            "Epoch: 27/40... Step: 6600... Loss: 0.9279... Val Loss: 1.7781\n",
            "Epoch: 27/40... Step: 6610... Loss: 0.9813... Val Loss: 1.7852\n",
            "Epoch: 27/40... Step: 6620... Loss: 0.7787... Val Loss: 1.7520\n",
            "Epoch: 27/40... Step: 6630... Loss: 0.8972... Val Loss: 1.7445\n",
            "Epoch: 27/40... Step: 6640... Loss: 0.7608... Val Loss: 1.7573\n",
            "Epoch: 28/40... Step: 6650... Loss: 0.9336... Val Loss: 1.7566\n",
            "Epoch: 28/40... Step: 6660... Loss: 0.7260... Val Loss: 1.7532\n",
            "Epoch: 28/40... Step: 6670... Loss: 0.6798... Val Loss: 1.7496\n",
            "Epoch: 28/40... Step: 6680... Loss: 0.9582... Val Loss: 1.7522\n",
            "Epoch: 28/40... Step: 6690... Loss: 0.8457... Val Loss: 1.7571\n",
            "Epoch: 28/40... Step: 6700... Loss: 0.9995... Val Loss: 1.7706\n",
            "Epoch: 28/40... Step: 6710... Loss: 0.9828... Val Loss: 1.7765\n",
            "Epoch: 28/40... Step: 6720... Loss: 0.7094... Val Loss: 1.7835\n",
            "Epoch: 28/40... Step: 6730... Loss: 0.8025... Val Loss: 1.7998\n",
            "Epoch: 28/40... Step: 6740... Loss: 0.9062... Val Loss: 1.8094\n",
            "Epoch: 28/40... Step: 6750... Loss: 0.7443... Val Loss: 1.8018\n",
            "Epoch: 28/40... Step: 6760... Loss: 0.6462... Val Loss: 1.7982\n",
            "Epoch: 28/40... Step: 6770... Loss: 0.8840... Val Loss: 1.7988\n",
            "Epoch: 28/40... Step: 6780... Loss: 1.1080... Val Loss: 1.7845\n",
            "Epoch: 28/40... Step: 6790... Loss: 0.8967... Val Loss: 1.7714\n",
            "Epoch: 28/40... Step: 6800... Loss: 0.8900... Val Loss: 1.7800\n",
            "Epoch: 28/40... Step: 6810... Loss: 0.9885... Val Loss: 1.7960\n",
            "Epoch: 28/40... Step: 6820... Loss: 0.8816... Val Loss: 1.7940\n",
            "Epoch: 28/40... Step: 6830... Loss: 0.7500... Val Loss: 1.7951\n",
            "Epoch: 28/40... Step: 6840... Loss: 0.7494... Val Loss: 1.7883\n",
            "Epoch: 28/40... Step: 6850... Loss: 1.0360... Val Loss: 1.7802\n",
            "Epoch: 28/40... Step: 6860... Loss: 0.9166... Val Loss: 1.7882\n",
            "Epoch: 28/40... Step: 6870... Loss: 0.9610... Val Loss: 1.7793\n",
            "Epoch: 28/40... Step: 6880... Loss: 0.8396... Val Loss: 1.7615\n",
            "Epoch: 29/40... Step: 6890... Loss: 0.9119... Val Loss: 1.7740\n",
            "Epoch: 29/40... Step: 6900... Loss: 0.8932... Val Loss: 1.7757\n",
            "Epoch: 29/40... Step: 6910... Loss: 0.7895... Val Loss: 1.7698\n",
            "Epoch: 29/40... Step: 6920... Loss: 0.9535... Val Loss: 1.7652\n",
            "Epoch: 29/40... Step: 6930... Loss: 0.8431... Val Loss: 1.7698\n",
            "Epoch: 29/40... Step: 6940... Loss: 0.9361... Val Loss: 1.7610\n",
            "Epoch: 29/40... Step: 6950... Loss: 0.8684... Val Loss: 1.7825\n",
            "Epoch: 29/40... Step: 6960... Loss: 0.8230... Val Loss: 1.7954\n",
            "Epoch: 29/40... Step: 6970... Loss: 0.6849... Val Loss: 1.7875\n",
            "Epoch: 29/40... Step: 6980... Loss: 1.0965... Val Loss: 1.7921\n",
            "Epoch: 29/40... Step: 6990... Loss: 0.8443... Val Loss: 1.8132\n",
            "Epoch: 29/40... Step: 7000... Loss: 0.8803... Val Loss: 1.8068\n",
            "Epoch: 29/40... Step: 7010... Loss: 0.7507... Val Loss: 1.7979\n",
            "Epoch: 29/40... Step: 7020... Loss: 0.8018... Val Loss: 1.7909\n",
            "Epoch: 29/40... Step: 7030... Loss: 0.8749... Val Loss: 1.7958\n",
            "Epoch: 29/40... Step: 7040... Loss: 0.8197... Val Loss: 1.7943\n",
            "Epoch: 29/40... Step: 7050... Loss: 0.8111... Val Loss: 1.8190\n",
            "Epoch: 29/40... Step: 7060... Loss: 0.8664... Val Loss: 1.8211\n",
            "Epoch: 29/40... Step: 7070... Loss: 0.7901... Val Loss: 1.8128\n",
            "Epoch: 29/40... Step: 7080... Loss: 0.8869... Val Loss: 1.8076\n",
            "Epoch: 29/40... Step: 7090... Loss: 1.1417... Val Loss: 1.8106\n",
            "Epoch: 29/40... Step: 7100... Loss: 0.7975... Val Loss: 1.8296\n",
            "Epoch: 29/40... Step: 7110... Loss: 0.9802... Val Loss: 1.8189\n",
            "Epoch: 29/40... Step: 7120... Loss: 0.7495... Val Loss: 1.8134\n",
            "Epoch: 29/40... Step: 7130... Loss: 0.7965... Val Loss: 1.7913\n",
            "Epoch: 30/40... Step: 7140... Loss: 0.9384... Val Loss: 1.7773\n",
            "Epoch: 30/40... Step: 7150... Loss: 0.9375... Val Loss: 1.7922\n",
            "Epoch: 30/40... Step: 7160... Loss: 0.5784... Val Loss: 1.7962\n",
            "Epoch: 30/40... Step: 7170... Loss: 0.8264... Val Loss: 1.7970\n",
            "Epoch: 30/40... Step: 7180... Loss: 0.6990... Val Loss: 1.7897\n",
            "Epoch: 30/40... Step: 7190... Loss: 0.9185... Val Loss: 1.7919\n",
            "Epoch: 30/40... Step: 7200... Loss: 0.8611... Val Loss: 1.8053\n",
            "Epoch: 30/40... Step: 7210... Loss: 1.0015... Val Loss: 1.8176\n",
            "Epoch: 30/40... Step: 7220... Loss: 0.7224... Val Loss: 1.8225\n",
            "Epoch: 30/40... Step: 7230... Loss: 1.0310... Val Loss: 1.8324\n",
            "Epoch: 30/40... Step: 7240... Loss: 0.7265... Val Loss: 1.8474\n",
            "Epoch: 30/40... Step: 7250... Loss: 0.7847... Val Loss: 1.8344\n",
            "Epoch: 30/40... Step: 7260... Loss: 0.8624... Val Loss: 1.8385\n",
            "Epoch: 30/40... Step: 7270... Loss: 0.7128... Val Loss: 1.8424\n",
            "Epoch: 30/40... Step: 7280... Loss: 0.7244... Val Loss: 1.8440\n",
            "Epoch: 30/40... Step: 7290... Loss: 1.0457... Val Loss: 1.8467\n",
            "Epoch: 30/40... Step: 7300... Loss: 0.7511... Val Loss: 1.8522\n",
            "Epoch: 30/40... Step: 7310... Loss: 1.0637... Val Loss: 1.8306\n",
            "Epoch: 30/40... Step: 7320... Loss: 0.8380... Val Loss: 1.8228\n",
            "Epoch: 30/40... Step: 7330... Loss: 0.7510... Val Loss: 1.8188\n",
            "Epoch: 30/40... Step: 7340... Loss: 0.8086... Val Loss: 1.8203\n",
            "Epoch: 30/40... Step: 7350... Loss: 1.0111... Val Loss: 1.8473\n",
            "Epoch: 30/40... Step: 7360... Loss: 0.8492... Val Loss: 1.8442\n",
            "Epoch: 30/40... Step: 7370... Loss: 0.6681... Val Loss: 1.8306\n",
            "Epoch: 30/40... Step: 7380... Loss: 0.7924... Val Loss: 1.8257\n",
            "Epoch: 31/40... Step: 7390... Loss: 0.9172... Val Loss: 1.8143\n",
            "Epoch: 31/40... Step: 7400... Loss: 0.7320... Val Loss: 1.8163\n",
            "Epoch: 31/40... Step: 7410... Loss: 0.7278... Val Loss: 1.8270\n",
            "Epoch: 31/40... Step: 7420... Loss: 0.8957... Val Loss: 1.8202\n",
            "Epoch: 31/40... Step: 7430... Loss: 0.9242... Val Loss: 1.8204\n",
            "Epoch: 31/40... Step: 7440... Loss: 1.0178... Val Loss: 1.8409\n",
            "Epoch: 31/40... Step: 7450... Loss: 0.7868... Val Loss: 1.8536\n",
            "Epoch: 31/40... Step: 7460... Loss: 0.7149... Val Loss: 1.8517\n",
            "Epoch: 31/40... Step: 7470... Loss: 0.5642... Val Loss: 1.8324\n",
            "Epoch: 31/40... Step: 7480... Loss: 0.6369... Val Loss: 1.8309\n",
            "Epoch: 31/40... Step: 7490... Loss: 0.6054... Val Loss: 1.8372\n",
            "Epoch: 31/40... Step: 7500... Loss: 0.6883... Val Loss: 1.8429\n",
            "Epoch: 31/40... Step: 7510... Loss: 0.8866... Val Loss: 1.8466\n",
            "Epoch: 31/40... Step: 7520... Loss: 0.9078... Val Loss: 1.8348\n",
            "Epoch: 31/40... Step: 7530... Loss: 0.9172... Val Loss: 1.8347\n",
            "Epoch: 31/40... Step: 7540... Loss: 1.0051... Val Loss: 1.8280\n",
            "Epoch: 31/40... Step: 7550... Loss: 0.8331... Val Loss: 1.8362\n",
            "Epoch: 31/40... Step: 7560... Loss: 0.6836... Val Loss: 1.8368\n",
            "Epoch: 31/40... Step: 7570... Loss: 0.9792... Val Loss: 1.8390\n",
            "Epoch: 31/40... Step: 7580... Loss: 0.9509... Val Loss: 1.8387\n",
            "Epoch: 31/40... Step: 7590... Loss: 0.9852... Val Loss: 1.8445\n",
            "Epoch: 31/40... Step: 7600... Loss: 0.5760... Val Loss: 1.8494\n",
            "Epoch: 31/40... Step: 7610... Loss: 0.8670... Val Loss: 1.8237\n",
            "Epoch: 31/40... Step: 7620... Loss: 0.5937... Val Loss: 1.8137\n",
            "Epoch: 32/40... Step: 7630... Loss: 0.7806... Val Loss: 1.8266\n",
            "Epoch: 32/40... Step: 7640... Loss: 0.9776... Val Loss: 1.8390\n",
            "Epoch: 32/40... Step: 7650... Loss: 0.7756... Val Loss: 1.8324\n",
            "Epoch: 32/40... Step: 7660... Loss: 0.8169... Val Loss: 1.8384\n",
            "Epoch: 32/40... Step: 7670... Loss: 0.7530... Val Loss: 1.8407\n",
            "Epoch: 32/40... Step: 7680... Loss: 0.5885... Val Loss: 1.8356\n",
            "Epoch: 32/40... Step: 7690... Loss: 0.7825... Val Loss: 1.8301\n",
            "Epoch: 32/40... Step: 7700... Loss: 0.9479... Val Loss: 1.8409\n",
            "Epoch: 32/40... Step: 7710... Loss: 0.8687... Val Loss: 1.8489\n",
            "Epoch: 32/40... Step: 7720... Loss: 0.9384... Val Loss: 1.8695\n",
            "Epoch: 32/40... Step: 7730... Loss: 0.6354... Val Loss: 1.8739\n",
            "Epoch: 32/40... Step: 7740... Loss: 0.8091... Val Loss: 1.8611\n",
            "Epoch: 32/40... Step: 7750... Loss: 0.7285... Val Loss: 1.8528\n",
            "Epoch: 32/40... Step: 7760... Loss: 0.8256... Val Loss: 1.8572\n",
            "Epoch: 32/40... Step: 7770... Loss: 0.8632... Val Loss: 1.8654\n",
            "Epoch: 32/40... Step: 7780... Loss: 0.9470... Val Loss: 1.8578\n",
            "Epoch: 32/40... Step: 7790... Loss: 0.7178... Val Loss: 1.8578\n",
            "Epoch: 32/40... Step: 7800... Loss: 0.7106... Val Loss: 1.8552\n",
            "Epoch: 32/40... Step: 7810... Loss: 0.7189... Val Loss: 1.8541\n",
            "Epoch: 32/40... Step: 7820... Loss: 0.7751... Val Loss: 1.8613\n",
            "Epoch: 32/40... Step: 7830... Loss: 0.8521... Val Loss: 1.8561\n",
            "Epoch: 32/40... Step: 7840... Loss: 0.8868... Val Loss: 1.8552\n",
            "Epoch: 32/40... Step: 7850... Loss: 0.8861... Val Loss: 1.8545\n",
            "Epoch: 32/40... Step: 7860... Loss: 0.9703... Val Loss: 1.8485\n",
            "Epoch: 32/40... Step: 7870... Loss: 0.7636... Val Loss: 1.8317\n",
            "Epoch: 33/40... Step: 7880... Loss: 1.0325... Val Loss: 1.8270\n",
            "Epoch: 33/40... Step: 7890... Loss: 0.6140... Val Loss: 1.8356\n",
            "Epoch: 33/40... Step: 7900... Loss: 0.7633... Val Loss: 1.8493\n",
            "Epoch: 33/40... Step: 7910... Loss: 0.7751... Val Loss: 1.8438\n",
            "Epoch: 33/40... Step: 7920... Loss: 0.7636... Val Loss: 1.8339\n",
            "Epoch: 33/40... Step: 7930... Loss: 0.8049... Val Loss: 1.8441\n",
            "Epoch: 33/40... Step: 7940... Loss: 0.8220... Val Loss: 1.8532\n",
            "Epoch: 33/40... Step: 7950... Loss: 0.6212... Val Loss: 1.8660\n",
            "Epoch: 33/40... Step: 7960... Loss: 0.6607... Val Loss: 1.8847\n",
            "Epoch: 33/40... Step: 7970... Loss: 0.7899... Val Loss: 1.9207\n",
            "Epoch: 33/40... Step: 7980... Loss: 0.5902... Val Loss: 1.9219\n",
            "Epoch: 33/40... Step: 7990... Loss: 0.6645... Val Loss: 1.9158\n",
            "Epoch: 33/40... Step: 8000... Loss: 0.7666... Val Loss: 1.9004\n",
            "Epoch: 33/40... Step: 8010... Loss: 0.8987... Val Loss: 1.8820\n",
            "Epoch: 33/40... Step: 8020... Loss: 0.6748... Val Loss: 1.8885\n",
            "Epoch: 33/40... Step: 8030... Loss: 0.6306... Val Loss: 1.9048\n",
            "Epoch: 33/40... Step: 8040... Loss: 0.9314... Val Loss: 1.9065\n",
            "Epoch: 33/40... Step: 8050... Loss: 0.7724... Val Loss: 1.8872\n",
            "Epoch: 33/40... Step: 8060... Loss: 0.8077... Val Loss: 1.8880\n",
            "Epoch: 33/40... Step: 8070... Loss: 0.5872... Val Loss: 1.8958\n",
            "Epoch: 33/40... Step: 8080... Loss: 0.8257... Val Loss: 1.8833\n",
            "Epoch: 33/40... Step: 8090... Loss: 0.8732... Val Loss: 1.8708\n",
            "Epoch: 33/40... Step: 8100... Loss: 0.8574... Val Loss: 1.8688\n",
            "Epoch: 33/40... Step: 8110... Loss: 0.6474... Val Loss: 1.8663\n",
            "Epoch: 34/40... Step: 8120... Loss: 0.7029... Val Loss: 1.8645\n",
            "Epoch: 34/40... Step: 8130... Loss: 0.7315... Val Loss: 1.8695\n",
            "Epoch: 34/40... Step: 8140... Loss: 0.6184... Val Loss: 1.8942\n",
            "Epoch: 34/40... Step: 8150... Loss: 0.7290... Val Loss: 1.9105\n",
            "Epoch: 34/40... Step: 8160... Loss: 0.7313... Val Loss: 1.9063\n",
            "Epoch: 34/40... Step: 8170... Loss: 0.8938... Val Loss: 1.9021\n",
            "Epoch: 34/40... Step: 8180... Loss: 0.8649... Val Loss: 1.9051\n",
            "Epoch: 34/40... Step: 8190... Loss: 0.6609... Val Loss: 1.8956\n",
            "Epoch: 34/40... Step: 8200... Loss: 0.7099... Val Loss: 1.8863\n",
            "Epoch: 34/40... Step: 8210... Loss: 0.8315... Val Loss: 1.8999\n",
            "Epoch: 34/40... Step: 8220... Loss: 0.7473... Val Loss: 1.9197\n",
            "Epoch: 34/40... Step: 8230... Loss: 0.7577... Val Loss: 1.9216\n",
            "Epoch: 34/40... Step: 8240... Loss: 0.6256... Val Loss: 1.9260\n",
            "Epoch: 34/40... Step: 8250... Loss: 0.6377... Val Loss: 1.9232\n",
            "Epoch: 34/40... Step: 8260... Loss: 0.8102... Val Loss: 1.9279\n",
            "Epoch: 34/40... Step: 8270... Loss: 0.7037... Val Loss: 1.9246\n",
            "Epoch: 34/40... Step: 8280... Loss: 0.8469... Val Loss: 1.9194\n",
            "Epoch: 34/40... Step: 8290... Loss: 0.7398... Val Loss: 1.8989\n",
            "Epoch: 34/40... Step: 8300... Loss: 0.6394... Val Loss: 1.8976\n",
            "Epoch: 34/40... Step: 8310... Loss: 0.8017... Val Loss: 1.9029\n",
            "Epoch: 34/40... Step: 8320... Loss: 0.7836... Val Loss: 1.9027\n",
            "Epoch: 34/40... Step: 8330... Loss: 0.7356... Val Loss: 1.9004\n",
            "Epoch: 34/40... Step: 8340... Loss: 0.9149... Val Loss: 1.8953\n",
            "Epoch: 34/40... Step: 8350... Loss: 0.6708... Val Loss: 1.9043\n",
            "Epoch: 34/40... Step: 8360... Loss: 0.8345... Val Loss: 1.9061\n",
            "Epoch: 35/40... Step: 8370... Loss: 0.7682... Val Loss: 1.9055\n",
            "Epoch: 35/40... Step: 8380... Loss: 1.0311... Val Loss: 1.9010\n",
            "Epoch: 35/40... Step: 8390... Loss: 0.4083... Val Loss: 1.9105\n",
            "Epoch: 35/40... Step: 8400... Loss: 0.7858... Val Loss: 1.9235\n",
            "Epoch: 35/40... Step: 8410... Loss: 0.6767... Val Loss: 1.9212\n",
            "Epoch: 35/40... Step: 8420... Loss: 0.7183... Val Loss: 1.9139\n",
            "Epoch: 35/40... Step: 8430... Loss: 0.6725... Val Loss: 1.9207\n",
            "Epoch: 35/40... Step: 8440... Loss: 0.8465... Val Loss: 1.9285\n",
            "Epoch: 35/40... Step: 8450... Loss: 0.6380... Val Loss: 1.9362\n",
            "Epoch: 35/40... Step: 8460... Loss: 0.9045... Val Loss: 1.9463\n",
            "Epoch: 35/40... Step: 8470... Loss: 0.5559... Val Loss: 1.9614\n",
            "Epoch: 35/40... Step: 8480... Loss: 0.6718... Val Loss: 1.9633\n",
            "Epoch: 35/40... Step: 8490... Loss: 0.7479... Val Loss: 1.9479\n",
            "Epoch: 35/40... Step: 8500... Loss: 0.7537... Val Loss: 1.9396\n",
            "Epoch: 35/40... Step: 8510... Loss: 0.7724... Val Loss: 1.9357\n",
            "Epoch: 35/40... Step: 8520... Loss: 0.8409... Val Loss: 1.9409\n",
            "Epoch: 35/40... Step: 8530... Loss: 0.7527... Val Loss: 1.9397\n",
            "Epoch: 35/40... Step: 8540... Loss: 0.7317... Val Loss: 1.9335\n",
            "Epoch: 35/40... Step: 8550... Loss: 0.6732... Val Loss: 1.9372\n",
            "Epoch: 35/40... Step: 8560... Loss: 0.6369... Val Loss: 1.9408\n",
            "Epoch: 35/40... Step: 8570... Loss: 0.7426... Val Loss: 1.9573\n",
            "Epoch: 35/40... Step: 8580... Loss: 0.7362... Val Loss: 1.9500\n",
            "Epoch: 35/40... Step: 8590... Loss: 0.7570... Val Loss: 1.9421\n",
            "Epoch: 35/40... Step: 8600... Loss: 0.5833... Val Loss: 1.9230\n",
            "Epoch: 35/40... Step: 8610... Loss: 0.7811... Val Loss: 1.9020\n",
            "Epoch: 36/40... Step: 8620... Loss: 0.8548... Val Loss: 1.8957\n",
            "Epoch: 36/40... Step: 8630... Loss: 0.7370... Val Loss: 1.9121\n",
            "Epoch: 36/40... Step: 8640... Loss: 0.6007... Val Loss: 1.9385\n",
            "Epoch: 36/40... Step: 8650... Loss: 0.8010... Val Loss: 1.9554\n",
            "Epoch: 36/40... Step: 8660... Loss: 0.8376... Val Loss: 1.9627\n",
            "Epoch: 36/40... Step: 8670... Loss: 0.8363... Val Loss: 1.9618\n",
            "Epoch: 36/40... Step: 8680... Loss: 0.9528... Val Loss: 1.9559\n",
            "Epoch: 36/40... Step: 8690... Loss: 0.5803... Val Loss: 1.9436\n",
            "Epoch: 36/40... Step: 8700... Loss: 0.5293... Val Loss: 1.9492\n",
            "Epoch: 36/40... Step: 8710... Loss: 0.7192... Val Loss: 1.9706\n",
            "Epoch: 36/40... Step: 8720... Loss: 0.7136... Val Loss: 1.9815\n",
            "Epoch: 36/40... Step: 8730... Loss: 0.5988... Val Loss: 1.9829\n",
            "Epoch: 36/40... Step: 8740... Loss: 0.6823... Val Loss: 1.9950\n",
            "Epoch: 36/40... Step: 8750... Loss: 0.8440... Val Loss: 1.9957\n",
            "Epoch: 36/40... Step: 8760... Loss: 0.6963... Val Loss: 1.9908\n",
            "Epoch: 36/40... Step: 8770... Loss: 0.8412... Val Loss: 1.9810\n",
            "Epoch: 36/40... Step: 8780... Loss: 0.6399... Val Loss: 1.9601\n",
            "Epoch: 36/40... Step: 8790... Loss: 0.6319... Val Loss: 1.9364\n",
            "Epoch: 36/40... Step: 8800... Loss: 0.8657... Val Loss: 1.9420\n",
            "Epoch: 36/40... Step: 8810... Loss: 0.7154... Val Loss: 1.9514\n",
            "Epoch: 36/40... Step: 8820... Loss: 0.8344... Val Loss: 1.9623\n",
            "Epoch: 36/40... Step: 8830... Loss: 0.5029... Val Loss: 1.9662\n",
            "Epoch: 36/40... Step: 8840... Loss: 0.7934... Val Loss: 1.9466\n",
            "Epoch: 36/40... Step: 8850... Loss: 0.5848... Val Loss: 1.9266\n",
            "Epoch: 37/40... Step: 8860... Loss: 0.6873... Val Loss: 1.9361\n",
            "Epoch: 37/40... Step: 8870... Loss: 0.7977... Val Loss: 1.9410\n",
            "Epoch: 37/40... Step: 8880... Loss: 0.7347... Val Loss: 1.9344\n",
            "Epoch: 37/40... Step: 8890... Loss: 0.6898... Val Loss: 1.9345\n",
            "Epoch: 37/40... Step: 8900... Loss: 0.6857... Val Loss: 1.9335\n",
            "Epoch: 37/40... Step: 8910... Loss: 0.6098... Val Loss: 1.9294\n",
            "Epoch: 37/40... Step: 8920... Loss: 0.7132... Val Loss: 1.9442\n",
            "Epoch: 37/40... Step: 8930... Loss: 0.7341... Val Loss: 1.9627\n",
            "Epoch: 37/40... Step: 8940... Loss: 0.7959... Val Loss: 1.9565\n",
            "Epoch: 37/40... Step: 8950... Loss: 0.8324... Val Loss: 1.9686\n",
            "Epoch: 37/40... Step: 8960... Loss: 0.7397... Val Loss: 1.9752\n",
            "Epoch: 37/40... Step: 8970... Loss: 0.7159... Val Loss: 1.9666\n",
            "Epoch: 37/40... Step: 8980... Loss: 0.7476... Val Loss: 1.9842\n",
            "Epoch: 37/40... Step: 8990... Loss: 0.7759... Val Loss: 1.9805\n",
            "Epoch: 37/40... Step: 9000... Loss: 0.8252... Val Loss: 1.9910\n",
            "Epoch: 37/40... Step: 9010... Loss: 0.8782... Val Loss: 1.9949\n",
            "Epoch: 37/40... Step: 9020... Loss: 0.5279... Val Loss: 1.9910\n",
            "Epoch: 37/40... Step: 9030... Loss: 0.6739... Val Loss: 1.9830\n",
            "Epoch: 37/40... Step: 9040... Loss: 0.6193... Val Loss: 1.9759\n",
            "Epoch: 37/40... Step: 9050... Loss: 0.6434... Val Loss: 1.9883\n",
            "Epoch: 37/40... Step: 9060... Loss: 0.8058... Val Loss: 1.9907\n",
            "Epoch: 37/40... Step: 9070... Loss: 0.5583... Val Loss: 1.9795\n",
            "Epoch: 37/40... Step: 9080... Loss: 0.6363... Val Loss: 1.9713\n",
            "Epoch: 37/40... Step: 9090... Loss: 0.7841... Val Loss: 1.9599\n",
            "Epoch: 37/40... Step: 9100... Loss: 0.5994... Val Loss: 1.9561\n",
            "Epoch: 38/40... Step: 9110... Loss: 0.7930... Val Loss: 1.9671\n",
            "Epoch: 38/40... Step: 9120... Loss: 0.5484... Val Loss: 1.9773\n",
            "Epoch: 38/40... Step: 9130... Loss: 0.6779... Val Loss: 1.9665\n",
            "Epoch: 38/40... Step: 9140... Loss: 0.6761... Val Loss: 1.9676\n",
            "Epoch: 38/40... Step: 9150... Loss: 0.6668... Val Loss: 1.9746\n",
            "Epoch: 38/40... Step: 9160... Loss: 0.7236... Val Loss: 1.9713\n",
            "Epoch: 38/40... Step: 9170... Loss: 0.7540... Val Loss: 1.9674\n",
            "Epoch: 38/40... Step: 9180... Loss: 0.5585... Val Loss: 1.9654\n",
            "Epoch: 38/40... Step: 9190... Loss: 0.7377... Val Loss: 1.9653\n",
            "Epoch: 38/40... Step: 9200... Loss: 0.5427... Val Loss: 1.9728\n",
            "Epoch: 38/40... Step: 9210... Loss: 0.7997... Val Loss: 1.9902\n",
            "Epoch: 38/40... Step: 9220... Loss: 0.5743... Val Loss: 1.9970\n",
            "Epoch: 38/40... Step: 9230... Loss: 0.7675... Val Loss: 2.0029\n",
            "Epoch: 38/40... Step: 9240... Loss: 0.6243... Val Loss: 1.9792\n",
            "Epoch: 38/40... Step: 9250... Loss: 0.7078... Val Loss: 1.9730\n",
            "Epoch: 38/40... Step: 9260... Loss: 0.5160... Val Loss: 1.9854\n",
            "Epoch: 38/40... Step: 9270... Loss: 0.7171... Val Loss: 1.9973\n",
            "Epoch: 38/40... Step: 9280... Loss: 0.6672... Val Loss: 1.9843\n",
            "Epoch: 38/40... Step: 9290... Loss: 0.6463... Val Loss: 1.9683\n",
            "Epoch: 38/40... Step: 9300... Loss: 0.6015... Val Loss: 1.9572\n",
            "Epoch: 38/40... Step: 9310... Loss: 0.6421... Val Loss: 1.9644\n",
            "Epoch: 38/40... Step: 9320... Loss: 0.7517... Val Loss: 1.9717\n",
            "Epoch: 38/40... Step: 9330... Loss: 0.6748... Val Loss: 1.9491\n",
            "Epoch: 38/40... Step: 9340... Loss: 0.5952... Val Loss: 1.9276\n",
            "Epoch: 39/40... Step: 9350... Loss: 0.7196... Val Loss: 1.9349\n",
            "Epoch: 39/40... Step: 9360... Loss: 0.7149... Val Loss: 1.9399\n",
            "Epoch: 39/40... Step: 9370... Loss: 0.7181... Val Loss: 1.9558\n",
            "Epoch: 39/40... Step: 9380... Loss: 0.5790... Val Loss: 1.9684\n",
            "Epoch: 39/40... Step: 9390... Loss: 0.6792... Val Loss: 1.9692\n",
            "Epoch: 39/40... Step: 9400... Loss: 0.9047... Val Loss: 1.9898\n",
            "Epoch: 39/40... Step: 9410... Loss: 0.8638... Val Loss: 2.0014\n",
            "Epoch: 39/40... Step: 9420... Loss: 0.5514... Val Loss: 2.0128\n",
            "Epoch: 39/40... Step: 9430... Loss: 0.5798... Val Loss: 2.0128\n",
            "Epoch: 39/40... Step: 9440... Loss: 0.7610... Val Loss: 2.0238\n",
            "Epoch: 39/40... Step: 9450... Loss: 0.8191... Val Loss: 2.0283\n",
            "Epoch: 39/40... Step: 9460... Loss: 0.8153... Val Loss: 2.0271\n",
            "Epoch: 39/40... Step: 9470... Loss: 0.4506... Val Loss: 2.0181\n",
            "Epoch: 39/40... Step: 9480... Loss: 0.7531... Val Loss: 2.0199\n",
            "Epoch: 39/40... Step: 9490... Loss: 0.6387... Val Loss: 2.0301\n",
            "Epoch: 39/40... Step: 9500... Loss: 0.7775... Val Loss: 2.0257\n",
            "Epoch: 39/40... Step: 9510... Loss: 0.6733... Val Loss: 2.0138\n",
            "Epoch: 39/40... Step: 9520... Loss: 0.6847... Val Loss: 1.9928\n",
            "Epoch: 39/40... Step: 9530... Loss: 0.6009... Val Loss: 1.9914\n",
            "Epoch: 39/40... Step: 9540... Loss: 0.6744... Val Loss: 2.0082\n",
            "Epoch: 39/40... Step: 9550... Loss: 0.7329... Val Loss: 2.0091\n",
            "Epoch: 39/40... Step: 9560... Loss: 0.5723... Val Loss: 2.0034\n",
            "Epoch: 39/40... Step: 9570... Loss: 0.6774... Val Loss: 2.0043\n",
            "Epoch: 39/40... Step: 9580... Loss: 0.5950... Val Loss: 2.0053\n",
            "Epoch: 39/40... Step: 9590... Loss: 0.6448... Val Loss: 1.9862\n",
            "Epoch: 40/40... Step: 9600... Loss: 0.7822... Val Loss: 1.9758\n",
            "Epoch: 40/40... Step: 9610... Loss: 0.7922... Val Loss: 1.9838\n",
            "Epoch: 40/40... Step: 9620... Loss: 0.4535... Val Loss: 1.9924\n",
            "Epoch: 40/40... Step: 9630... Loss: 0.7616... Val Loss: 2.0083\n",
            "Epoch: 40/40... Step: 9640... Loss: 0.7311... Val Loss: 2.0092\n",
            "Epoch: 40/40... Step: 9650... Loss: 0.4468... Val Loss: 1.9958\n",
            "Epoch: 40/40... Step: 9660... Loss: 0.5931... Val Loss: 2.0001\n",
            "Epoch: 40/40... Step: 9670... Loss: 0.7672... Val Loss: 2.0182\n",
            "Epoch: 40/40... Step: 9680... Loss: 0.5557... Val Loss: 2.0337\n",
            "Epoch: 40/40... Step: 9690... Loss: 0.8112... Val Loss: 2.0495\n",
            "Epoch: 40/40... Step: 9700... Loss: 0.6417... Val Loss: 2.0557\n",
            "Epoch: 40/40... Step: 9710... Loss: 0.6342... Val Loss: 2.0608\n",
            "Epoch: 40/40... Step: 9720... Loss: 0.5950... Val Loss: 2.0458\n",
            "Epoch: 40/40... Step: 9730... Loss: 0.6469... Val Loss: 2.0320\n",
            "Epoch: 40/40... Step: 9740... Loss: 0.7451... Val Loss: 2.0421\n",
            "Epoch: 40/40... Step: 9750... Loss: 0.7549... Val Loss: 2.0528\n",
            "Epoch: 40/40... Step: 9760... Loss: 0.5661... Val Loss: 2.0516\n",
            "Epoch: 40/40... Step: 9770... Loss: 0.7001... Val Loss: 2.0378\n",
            "Epoch: 40/40... Step: 9780... Loss: 0.6716... Val Loss: 2.0247\n",
            "Epoch: 40/40... Step: 9790... Loss: 0.6527... Val Loss: 2.0133\n",
            "Epoch: 40/40... Step: 9800... Loss: 0.6082... Val Loss: 2.0166\n",
            "Epoch: 40/40... Step: 9810... Loss: 0.7532... Val Loss: 2.0172\n",
            "Epoch: 40/40... Step: 9820... Loss: 0.6601... Val Loss: 2.0048\n",
            "Epoch: 40/40... Step: 9830... Loss: 0.5257... Val Loss: 1.9888\n",
            "Epoch: 40/40... Step: 9840... Loss: 0.6987... Val Loss: 1.9973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdV0TKz2yOXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "  x = np.array([[net.char2int[char]]])\n",
        "  x = one_hot_encode(x, len(net.chars))\n",
        "  inputs = torch.from_numpy(x)\n",
        "  if(train_on_gpu):\n",
        "    inputs = inputs.cuda()\n",
        "  h = tuple([each.data for each in h])\n",
        "  out, h = net(inputs, h)\n",
        "\n",
        "  p = F.softmax(out, dim=1).data\n",
        "  if(train_on_gpu):\n",
        "    p = p.cpu()\n",
        "  \n",
        "  if top_k is None:\n",
        "    top_ch = np.arange(len(net.chars))\n",
        "  else:\n",
        "      p, top_ch = p.topk(top_k)\n",
        "      top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "  # select the likely next character with some element of randomness\n",
        "  p = p.numpy().squeeze()\n",
        "  char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "  # return the encoded value of the predicted char and the hidden state\n",
        "  return net.int2char[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl8Yf1l0ODBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gExSrtfAOW-P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "58a2df6f-f290-4ff6-8321-599f27d9a817"
      },
      "source": [
        "print(sample(net, 1000, prime='Harry', top_k=5))"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Harry Potter, he was still staring to the wall. Its  theme people to tolk and Mrs Dursley gossepibed shanps into a sight of diskbing. The Dursleys shudderly, but they was nothing how is the bashroom, Mr Dursley picked up his brigh twat when he dad seemed Potter who had a small son, head. Ne, you could have been crinking our over the monn. He had seen a lattle swoke and sat aistore and Mrs Dursley stopped Mrs Dursley. Well, He couldnt kill Harry Potter cake the Put-Outer almost twice as quobas arose his freat the weother of showing on the people hes kissed and stared at the window. Shhohid never know  her sister, buckus it was shousing a map. How very walk havion a smopet and patted her he tanted to the spot. He dad noticed something as it, the Potters  Hapry, the thought before he can homing a little come sear as she wis now and Mrs Dursley stupped 28493.indb 13 8493.indb 14 18/07/2014 16:36 8/07/2014 16:36 8/07/2014 16:36 5 THE BOY WHO LIVED to notice something as inetead what me, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF4xycgvOYkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}