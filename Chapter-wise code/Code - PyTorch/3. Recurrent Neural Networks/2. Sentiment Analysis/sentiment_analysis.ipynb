{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_analysis_via_RNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMqG/tsPN4NixGlYSMt091p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/purvasingh96/Deep-learning-with-neural-networks/blob/master/Deep-learning-with-pytorch/3.%20Recurrent%20Neural%20Networks/Sentiment_analysis_via_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luDRhnBK-n-N",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis with an RNN\n",
        "In this notebook I have implemented a RNN that performs sentiment analysis. <br>\n",
        "Reason for using RNN instead of a strictly feedforward network is that we can also include information about *sequence* of words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiWNgvo--vJo",
        "colab_type": "text"
      },
      "source": [
        "### Network Architecture\n",
        "Below would be the architecture diagram for my sentiment analysis model - <br>\n",
        "<img src=\"https://github.com/purvasingh96/Deep-learning-with-neural-networks/blob/master/Deep-learning-with-pytorch/3.%20Recurrent%20Neural%20Networks/images/network_diagram.png?raw=1\"></img>\n",
        "\n",
        "**Notes -**\n",
        "1. Since we are performing sentiment analysis, we need a more efficient representation of words as compared to one_hot_encoded vectors. Hence, using *embeded layer for dimensionality reduction.*\n",
        "2. The new embeddings will be passed to LSTM cells. LSTM cells will add recurrent connections and add ability to *include information about sequence of words.*\n",
        "3. Final LSTM outputs will go to *Sigmoid output layer.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVeGfBEk-2sE",
        "colab_type": "text"
      },
      "source": [
        "### Load in and visualize the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2YX8Yn0V3Rq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "with open('labels.txt', 'r') as f:\n",
        "  labels = f.read()\n",
        "with open('reviews.txt', 'r') as f:\n",
        "  reviews = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vq2yRY6WXATE",
        "colab_type": "code",
        "outputId": "b569f193-3aeb-4621-9ae1-3bc4084c1dc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(reviews[:200])"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86BbuemN-67X",
        "colab_type": "text"
      },
      "source": [
        "## Data pre-processing\n",
        "### Getting rid of punctuations \n",
        "1. Get rid of punctuation marks etc.\n",
        "2. Reviews are delimited by \\n. Use \\n as delimiter to split text into each reviews.\n",
        "3. Combine reviews in step-2 into 1 big string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk-kJCpFXaEw",
        "colab_type": "code",
        "outputId": "27f08b22-684a-4cf9-a1c1-e1afbd27f05c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from string import punctuation\n",
        "'''\n",
        "bromwell high is a cartoon comedy  \\n\n",
        "it ran at the same time as some other programs about school life  such as  teachers  \\n\n",
        " my   years in the teaching profession lead me to believe that bromwell high  \n",
        "'''\n",
        "reviews = reviews.lower()\n",
        "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
        "\n",
        "reviews_split = all_text.split('\\n')\n",
        "all_text = ' '.join(reviews_split)\n",
        "words = all_text.split()\n",
        "print(words[:10])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQDQEsCC--pF",
        "colab_type": "text"
      },
      "source": [
        "### Encoding reviews\n",
        "Create an array that contains integer encoded version of words in reviews. The word appearing the most should have least integer value. Example if *the* appeared the most in reviews, then assign *'the' : 1*  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnBtSYZSX2Hz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "counts = Counter(words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {word:ii for ii, word in enumerate(vocab, 1)}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ0-JUpF_DfT",
        "colab_type": "text"
      },
      "source": [
        "### Encoding labels\n",
        "If review is positive, then corresponding label is 0 else 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcCQQTGUYyNG",
        "colab_type": "code",
        "outputId": "79927d90-00e6-496b-ff05-576d7d6287cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "reviews_int = []\n",
        "'''\n",
        "reviews_split contains multiple reviews \n",
        "reviews_int will be 2-D array\n",
        "'''\n",
        "for review in reviews_split:\n",
        "  reviews_int.append([vocab_to_int[word] for word in review.split()])\n",
        "print(len(vocab_to_int))\n",
        "print(reviews_int[:10])"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "74072\n",
            "[[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23], [63, 4, 3, 125, 36, 47, 7472, 1395, 16, 3, 4181, 505, 45, 17, 3, 622, 134, 12, 6, 3, 1279, 457, 4, 1721, 207, 3, 10624, 7373, 300, 6, 667, 83, 35, 2116, 1086, 2989, 34, 1, 898, 46417, 4, 8, 13, 5096, 464, 8, 2656, 1721, 1, 221, 57, 17, 58, 794, 1297, 832, 228, 8, 43, 98, 123, 1469, 59, 147, 38, 1, 963, 142, 29, 667, 123, 1, 13584, 410, 61, 94, 1774, 306, 755, 5, 3, 819, 10396, 22, 3, 1724, 635, 8, 13, 128, 73, 21, 233, 102, 17, 49, 50, 617, 34, 682, 85, 28785, 28786, 682, 374, 3341, 11398, 2, 16371, 7946, 51, 29, 108, 3324], [22382, 42, 46418, 15, 706, 17139, 3389, 47, 77, 35, 1819, 16, 154, 19, 114, 3, 1305, 5, 336, 147, 22, 1, 857, 12, 70, 281, 1168, 399, 36, 120, 283, 38, 169, 5, 382, 158, 42, 2269, 16, 1, 541, 90, 78, 102, 4, 1, 3244, 15, 43, 3, 407, 1068, 136, 8055, 44, 182, 140, 15, 3043, 1, 320, 22, 4818, 26224, 346, 5, 3090, 2092, 1, 18839, 17939, 42, 8055, 46, 33, 236, 29, 370, 5, 130, 56, 22, 1, 1928, 7, 7, 19, 48, 46, 21, 70, 344, 3, 2099, 5, 408, 22, 1, 1928, 16, 3, 3119, 205, 1, 28787, 21, 281, 68, 38, 3, 339, 1, 700, 715, 3, 3818, 1229, 22, 1, 1491, 3, 1197, 2, 283, 21, 281, 2435, 5, 66, 48, 8, 13, 39, 5, 29, 3244, 12, 6, 21026, 11723, 13, 2015, 7, 7, 3687, 2818, 36, 4147, 36, 374, 15, 11723, 296, 3, 996, 125, 36, 47, 283, 9, 1, 176, 363, 6893, 5, 94, 3, 2099, 17, 3, 4976, 2932, 14557, 19870, 5, 66, 46, 25, 51, 408, 9, 1, 1928, 16, 3236, 490, 205, 1, 28787, 46, 11723, 2845, 25, 51, 80, 48, 25, 483, 17, 3, 682, 1148, 4, 228, 52, 4461, 1, 2099, 13, 22, 118, 11723, 6, 1347, 22, 1, 857, 17, 3, 18840, 22, 27, 3873, 5, 10167, 27, 174, 829, 118, 25, 51, 23, 1456, 123, 1, 6451, 25, 13, 344, 1, 13585, 28788, 34, 3, 32300, 101, 8, 13, 391, 22, 27, 11724, 118, 11723, 874, 81, 103, 577, 3, 240, 34, 1, 393, 4, 4653, 16372, 1816, 3737, 35, 1200, 3103, 36, 188, 4048, 160, 2284, 41, 339, 2, 41, 8809, 6793, 1984, 4313, 2, 28789, 8810, 2457, 36, 26, 453, 338, 5, 1, 1928, 33, 155, 4219, 11723, 215, 23, 25, 13, 24, 338, 5, 4421, 5903, 28790, 39, 25, 281, 120, 54, 111, 996, 118, 8, 13, 534, 42, 2718, 501, 42, 29, 547, 7, 7, 136, 1, 115, 2003, 198, 4653, 2, 11723, 285, 23, 1644, 5, 112, 10, 254, 110, 4354, 5, 29, 30, 4, 3687, 2818, 15686, 107, 118, 2523, 5, 111, 3, 207, 8, 286, 3, 4220, 488, 1060, 5, 27, 2730, 158, 140, 15, 7473, 11399, 184, 4539, 42, 18841, 16, 1, 541, 5, 121, 48, 8, 13, 39, 255, 141, 4504, 160, 2284, 8, 1, 370, 245, 42, 22, 1, 81, 495, 228, 3, 372, 2099, 39, 31, 996, 78, 80, 54, 33, 89, 23, 122, 48, 5, 80, 17, 67, 273, 277, 33, 142, 200, 8, 5, 1, 3244, 303, 4, 757, 8, 39, 17140, 273, 7, 7, 42, 277, 11, 20, 79, 5853, 21, 5, 336, 400], [4505, 505, 15, 3, 3342, 162, 8312, 1652, 6, 4819, 56, 17, 4504, 5616, 140, 11725, 5, 996, 4919, 2933, 4462, 566, 1201, 36, 6, 1518, 96, 3, 744, 4, 26225, 13, 5, 27, 3461, 9, 10625, 4, 8, 111, 3013, 5, 1, 1027, 15, 3, 4390, 82, 22, 2049, 6, 4462, 538, 2764, 7073, 37443, 41, 463, 1, 8312, 46419, 302, 123, 15, 4221, 19, 1667, 922, 1, 1652, 6, 6129, 19871, 34, 1, 980, 1751, 22383, 646, 24104, 27, 106, 11726, 13, 14045, 15097, 17940, 2457, 466, 21027, 36, 3266, 1, 6365, 1020, 45, 17, 2695, 2499, 33, 1305, 5, 2076, 1, 4504, 11727, 1493, 22, 3, 21028, 1652, 3196, 22, 35, 4314, 1067, 19, 136, 228, 27, 4654, 22383, 217, 1906, 35, 3216, 17141, 9, 1, 4148, 1961, 1110, 4, 1, 1652, 5617, 8, 6524, 83, 1, 1958, 118, 8, 8056, 5, 1, 1301, 204, 3985, 9, 1, 641, 4, 1, 32301, 5854, 17, 922, 9, 342, 6200, 1047, 28791, 9, 255, 15687, 119, 1985, 123, 259, 1, 695, 12046, 16, 1, 4820, 13, 15, 33, 12397, 336, 17, 57, 688, 608, 45, 7, 7, 82, 560, 458, 1, 1056, 271, 28792, 4505, 11, 330, 736, 5, 1, 6702, 557, 1661, 689, 4505, 14, 516, 34, 1435, 9121, 136, 281, 173, 39, 8, 13, 8313, 10, 51, 23, 133, 4505, 6, 100, 428, 4, 1527, 349, 8, 6, 440, 256, 24, 2686, 16, 1, 204, 987, 45, 4, 1, 287, 4505, 107, 10, 28, 108, 37, 227, 10, 165, 418, 11, 30, 1, 117, 43, 8, 47, 60, 1621, 112, 4, 1, 287, 17, 3, 324, 1667, 922, 6129, 32302, 93, 1, 6524, 161, 23, 25, 66, 1, 3216, 17141, 6201, 4, 1, 277, 1, 1154, 70, 264, 5, 1638, 1, 201, 4505, 17, 159, 1043, 1661, 494, 4, 1, 791, 1, 32303, 1115, 18842, 6, 118, 8, 2656, 363, 1, 130, 17, 3, 5245, 6287, 4391, 147, 2582, 982, 343, 37444, 54, 1, 922, 1106, 45, 42, 10878, 15, 1, 24105, 42, 46, 100, 4, 1, 3529, 26, 3013, 8, 13, 3, 531, 322, 12, 97, 28, 91, 16, 3, 85, 116, 1661, 494, 19, 76, 6794, 104, 13, 739, 410, 12047, 265, 1298, 3, 146, 574, 4, 2327, 42, 817, 42, 1054, 800, 11, 6, 3, 1026, 1400, 136, 1, 246, 12767, 112, 918, 30, 2144, 16, 1006, 229, 24, 12, 74, 559, 101, 1, 1652, 8056, 40, 13, 24, 15, 74, 8811, 15, 10, 195, 40, 142, 28, 77, 59, 54, 1, 2914, 409, 562, 182, 89, 23, 1236, 56, 12, 74, 17, 3, 170, 648, 4, 653, 5097, 10626, 1518, 44, 19, 40, 13, 43, 141, 1867, 127, 706, 4015, 15, 1, 37445, 12048, 3444, 865, 37446, 6, 144, 19, 64, 211, 3, 368, 4, 137, 1176, 59, 549, 231, 16373, 5, 43, 167, 3738, 9, 1, 958, 7, 7, 1, 339, 366, 2220, 310, 4, 4505, 506, 229, 136, 1, 178, 242, 2033, 747, 35, 1685, 522, 4, 908, 577, 3, 162, 622, 878, 702, 109, 52, 137, 17, 706, 4015, 15, 37446, 2123, 5, 2077, 45, 104, 13, 1184, 2196, 137, 1, 3766, 42, 159, 368, 4, 340, 2306, 577, 1, 32304, 136, 10, 61, 39, 5, 66, 11, 1685, 908, 10, 239, 24, 249, 10, 97, 848, 145, 3, 733, 287, 522, 584, 4, 4505, 15, 853, 1, 20, 47, 1936, 889, 17, 517, 7818, 7374, 1571, 2800, 10, 79, 133, 58, 52, 81, 73, 1, 2819, 1652, 2117, 298, 696, 23, 85, 343, 364, 17, 1, 81, 106, 4505, 2256, 11, 302, 3076, 4, 269, 9, 1, 10627, 1315, 13, 2280, 4, 879, 256, 10, 51, 102, 4, 760, 4, 429, 107, 73, 11, 37, 10, 10879, 12, 13, 3, 116, 2458, 1, 202, 137, 26, 3, 116, 739, 464, 1, 1040, 6, 539, 24, 74, 2285, 42, 1054, 6, 4821, 62, 6, 3, 879, 15, 10, 10879, 11, 97, 28, 77, 3, 183, 50, 20, 46, 91, 2846, 7, 7, 1, 360, 1205, 26, 2626, 46, 163, 2068, 1, 113, 215, 23, 85, 106, 57, 708, 2186, 669, 3949, 47, 301, 234, 8, 14, 3, 1293, 5, 318, 9, 11, 30, 57, 708, 2186, 566, 1201, 268, 153, 11118, 82, 30, 57, 708, 2186, 743, 1968, 268, 1793, 136, 2642, 1331, 743, 6, 344, 116, 5, 80, 40, 26, 934, 4, 81, 1061, 1562, 5, 167, 45, 16, 98, 7, 7, 4505, 6, 1, 90, 1661, 21029, 4, 1, 287, 4505, 107, 37, 227, 10, 418, 1, 989, 485, 8, 59, 46, 33, 70, 3, 223, 694, 1, 360, 1889, 451, 151, 23, 336, 150, 3, 20, 44, 3, 15688, 1652, 43, 1587, 23, 29, 11, 354, 42, 12047, 1457, 34, 1, 22384, 4505], [520, 119, 113, 34, 16372, 1816, 3737, 117, 885, 21030, 721, 10, 28, 124, 108, 2, 115, 137, 9, 1623, 7691, 26, 330, 5, 589, 1, 6130, 22, 386, 6, 3, 349, 15, 50, 15, 231, 9, 7473, 11399, 1, 191, 22, 8966, 6, 82, 880, 101, 111, 3584, 4, 111, 3, 28793, 3445, 45, 27, 1324, 2, 111, 12398, 1, 2360, 4, 28788, 11723, 24106, 32305, 10, 143, 3, 2360, 25, 549, 287, 164, 697, 4016, 19870, 3, 502, 38, 1, 299, 2643, 6525, 121, 6, 759, 127, 98, 15, 3, 1135, 5098, 36, 483, 5, 4182, 1, 6608, 27, 104, 6, 52, 10397, 73, 630, 1, 1519, 134, 2, 1, 134, 118, 1, 3244, 17142, 3, 14046, 2100, 26, 31, 57, 2179, 167, 16, 1, 2951, 134, 2, 1, 106, 192, 15689, 975, 30, 24107, 11, 18, 211, 128, 253, 57, 10, 66, 8, 62, 6, 179, 395], [11, 20, 3637, 141, 10, 422, 23, 272, 60, 4355, 22, 32, 84, 3286, 22, 1, 172, 4, 1, 952, 507, 11, 4977, 5361, 5, 574, 4, 1155, 54, 53, 5304, 1, 261, 17, 41, 952, 125, 59, 1, 711, 137, 379, 626, 15, 111, 1509, 1, 156, 32, 292, 8, 97, 55, 72, 28, 77, 1, 157, 36, 37447, 48, 25, 871, 38, 1, 156, 10, 43, 89, 23, 122, 7, 7, 19, 97, 8, 28, 77, 1, 860, 43, 605, 36, 14, 1, 8812, 9, 115, 17, 25, 460, 52, 15098, 4, 27, 28794, 1937, 2, 3688, 2, 1092, 4, 309, 2, 27, 6131, 6056, 73, 4, 1702, 42, 231, 326, 25, 114, 2312, 71, 25, 14, 9, 115, 17, 1, 2484, 7, 7, 10, 14, 672, 9, 11, 18, 19, 89, 23, 836, 8, 14, 2270, 16, 35, 708, 37, 1857, 16, 610], [11, 6, 692, 1, 90, 2156, 20, 11728, 1, 2818, 5195, 249, 92, 3006, 8, 126, 24, 200, 3, 802, 634, 4, 22382, 1001, 133, 87, 3530, 3343, 509, 3, 802, 634, 4, 16374, 5096, 42, 2552, 509, 3, 802, 634, 4, 8967, 21, 3709, 109, 4, 1, 623, 787, 1010, 19, 131, 11, 20, 6, 55, 3178, 9, 3, 95, 109, 1258, 26, 24, 2, 5, 1572, 12, 123, 9, 3, 63, 44, 49, 4, 1, 90, 11729, 22385, 1039, 4, 875, 6, 365, 1132, 92, 24, 1, 3446, 607, 19, 92, 24, 582, 343, 60, 64, 3267, 6, 12, 2818, 142, 28, 177, 279, 326, 9, 1, 475, 10, 115, 3687, 15, 3, 157, 2, 533, 24, 37, 74, 15, 3, 475], [786, 295, 10, 122, 11, 6, 419, 5, 29, 35, 482, 20, 19, 1281, 33, 142, 28, 2657, 45, 1840, 32, 1, 2778, 37, 78, 97, 2436, 67, 3950, 45, 2, 24, 105, 256, 1, 134, 1571, 2, 12399, 451, 14, 319, 11, 63, 6, 98, 1321, 5, 105, 1, 3767, 4, 3, 472, 1381, 14, 1736, 1, 46420, 648, 70, 98, 194, 87, 194, 51, 21, 105, 106, 78, 43, 1238, 40, 2, 642, 257, 54, 1, 410, 6, 106, 78, 5246, 10, 65, 68, 3, 250, 57, 43, 390, 145, 11, 20, 1, 351, 70, 319, 19, 87, 74, 4, 12, 454, 14558, 3585, 529, 51, 21, 191, 1, 64, 152, 10, 418, 14, 7074, 14559, 2, 41, 737, 2670, 2, 1073, 134, 881, 11, 14, 3, 7474, 4, 4392, 2, 10, 143, 58, 331, 1311, 27, 343, 10, 102, 251, 36, 549, 33, 499, 620, 4, 11, 6, 72, 3148], [11, 6, 24, 1, 779, 3687, 2818, 20, 8, 14, 74, 325, 2730, 73, 90, 4, 27, 99, 2, 165, 68, 3, 112, 12, 14, 28795, 2779, 1816, 3737, 91, 1, 18, 53, 6, 140, 3, 759, 458, 1124, 507, 40, 70, 49, 381, 12, 97, 28, 77, 6526, 45, 3, 223, 52, 2, 49, 137, 12, 97, 238, 28, 77, 584, 5, 94, 1, 654, 5, 80, 37, 19, 31, 9, 31, 11, 6, 288, 1, 1783, 5, 831, 2, 66, 8, 1, 113, 14, 50, 442, 2818, 309, 120, 3, 50, 289, 205, 27, 7576, 1362, 5, 2514, 5, 1, 300, 173, 3737, 14, 1, 117, 270, 9, 1, 18, 19, 37448, 2, 6793, 197, 252, 67, 521, 72], [54, 10, 14, 116, 60, 798, 552, 71, 364, 5, 1, 730, 5, 66, 8057, 8, 14, 30, 4, 109, 99, 10, 293, 17, 60, 798, 19, 11, 14, 1, 64, 30, 69, 2500, 45, 4, 234, 93, 10, 68, 114, 108, 8057, 363, 43, 1009, 2, 10, 97, 28, 1431, 45, 1, 357, 4, 60, 110, 205, 8, 48, 3, 1929, 10880, 2, 2124, 354, 412, 4, 13, 6609, 2, 2974, 5148, 2125, 1366, 6, 30, 4, 60, 502, 876, 19, 8057, 6, 34, 227, 1, 247, 412, 4, 582, 4, 27, 599, 9, 1, 13586, 396, 4, 14047, 16375, 1366, 403, 178, 3, 454, 21031, 8968, 2601, 9, 5, 1, 450, 4, 3, 212, 11730, 34, 1, 1946, 3986, 2145, 34, 4048, 22386, 599, 115, 683, 115, 46421, 824, 1, 20, 4607, 47, 58, 680, 2113, 58, 224, 2, 6, 14048, 9, 9505, 6795, 11, 20, 396, 51, 29, 117, 4741, 15, 12768, 9, 849, 757, 35, 24108, 4149, 4, 410, 5, 14049, 3, 52, 9924, 1111, 4, 1191, 2, 854, 19, 2125, 1366, 6, 58, 14047, 4392, 1, 20, 6, 2124, 539, 2, 739, 19, 705, 12, 10, 328, 68, 58, 2003, 17, 42, 2467, 16, 100, 4, 1, 103, 303, 10, 416, 64, 5974, 16, 11, 5149, 4, 17941, 6527, 28796, 15690, 28797, 9, 3, 11731, 2644, 16, 2016, 8314, 3, 4078, 4, 22387, 37449, 2, 8058, 37450, 28798, 1, 63, 19872, 39, 3, 7375, 9, 1, 654, 295, 2437, 9, 3844, 16376, 2, 10881, 1074, 198, 10882, 295, 6, 407, 2, 2485, 1645, 5, 168, 451, 42, 1879, 42, 824, 2, 8, 43, 266, 22, 2, 22, 5, 1, 210, 118, 21, 43, 181, 5, 3606, 31, 4, 96, 8, 13, 114, 44, 3429, 8, 13, 64, 44, 9696, 13587, 13154, 8, 6, 163, 52, 73, 3, 1946, 448, 614, 5, 35, 1539, 705, 1, 300, 13, 1230, 5, 3739, 2125, 1366, 2438, 5, 94, 103, 37, 9320, 9, 525, 69, 230, 313, 45, 2, 16, 12, 284, 10, 254, 11, 18, 2124, 524, 5050, 2, 17942, 13588, 10, 66, 48, 25, 14, 169, 16, 19, 27, 13155, 22, 8485, 27, 729, 145, 24109, 15691, 2, 8178, 20, 3325, 32306, 8, 491, 1, 210, 4, 7075, 10, 535, 380, 11, 30, 46, 21, 155, 537, 3, 116, 98, 631, 2, 355, 141, 5, 2988, 21, 4, 333, 881, 278, 13, 43, 3874, 11, 20, 114, 563]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZbdOa6Vb_cu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_split = labels.split('\\n')\n",
        "labels_to_int = np.array([1 if label=='positive' else 0 for label in labels_split])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYKwx9WvcWib",
        "colab_type": "code",
        "outputId": "1a9a5a66-1b5d-4143-815a-8bc58833ce48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "zero_length_reviews = Counter([len(x) for x in reviews_int])\n",
        "print(max(zero_length_reviews))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcExka6h_Io_",
        "colab_type": "text"
      },
      "source": [
        "### Removing Outliers\n",
        "This step involves - \n",
        "1. Getting rid of extremely long/short reviews\n",
        "2. Padding/truncating reaining data to maintain constant review length.\n",
        "\n",
        "<img src=\"https://github.com/purvasingh96/Deep-learning-with-neural-networks/blob/master/Deep-learning-with-pytorch/3.%20Recurrent%20Neural%20Networks/images/outliers_padding_ex.png?raw=1\"></img>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_GWE_qzc9nH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_zero_idx = [ii for ii, review in enumerate(reviews_int) if len(review)!=0]\n",
        "reviews_int = [reviews_int[ii] for ii in non_zero_idx]\n",
        "encoded_labels = np.array([labels_to_int[ii] for ii in non_zero_idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBhYeMn6eaxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_features(reviews_int, seq_length):\n",
        "  features = np.zeros((len(reviews_int), seq_length), dtype=int)\n",
        "  for i, row in enumerate(reviews_int):\n",
        "    features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "  \n",
        "  return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eW0S9SBRgmAP",
        "colab_type": "code",
        "outputId": "3c7810a8-38f1-460b-a24a-6ffdf86efc8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "seq_length = 200\n",
        "features = pad_features(reviews_int, seq_length)\n",
        "print(features[:30, :10])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [22382    42 46418    15   706 17139  3389    47    77    35]\n",
            " [ 4505   505    15     3  3342   162  8312  1652     6  4819]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   54    10    14   116    60   798   552    71   364     5]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   330   578    34     3   162   748  2731     9   325]\n",
            " [    9    11 10171  5305  1946   689   444    22   280   673]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   307 10399  2069  1565  6202  6528  3288 17946 10628]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   21   122  2069  1565   515  8181    88     6  1325  1182]\n",
            " [    1    20     6    76    40     6    58    81    95     5]\n",
            " [   54    10    84   329 26230 46427    63    10    14   614]\n",
            " [   11    20     6    30  1436 32317  3769   690 15100     6]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   40    26   109 17952  1422     9     1   327     4   125]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   10   499     1   307 10399    55    74     8    13    30]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pmggZJA_O_F",
        "colab_type": "text"
      },
      "source": [
        "# Training, Testing and Validating "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBsLDsgPgyAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "split_idx = int(len(features)*split_frac)\n",
        "\n",
        "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ0Ec9iJ_TtB",
        "colab_type": "text"
      },
      "source": [
        "# DataLoaders and Batching\n",
        "\n",
        "A neat way to create data-loaders and batch our training, validation and test Tensor datasets is as follows -<br>\n",
        "```python\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "\n",
        "```\n",
        "This is an alternative to creating a generator function for batching our data into full batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQckYBaStLtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "\n",
        "batch_size=50\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJw3bjvBvFCC",
        "colab_type": "code",
        "outputId": "5847168f-e2d4-4f7f-e8cd-fc7e8d5d9f82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# First checking if GPU is available\n",
        "import torch\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G_P94dY_ZFS",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Network with PyTorch\n",
        "\n",
        "Below are the various layers of our RNN that would perform sentiment analysis - \n",
        "\n",
        "1. An embedding layer that converts our word tokens (integers) into embeddings of a specific size.\n",
        "2. An LSTM layer defined by a hidden_state size and number of layers\n",
        "3. A fully-connected output layer that maps the LSTM layer outputs to a desired output_size\n",
        "4. A sigmoid activation layer which turns all outputs into a value 0-1; return only the last sigmoid output as the output of this network.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ7UcaiBx_g4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "  def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "    super(SentimentRNN, self).__init__()\n",
        "\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    self.fc = nn.Linear(hidden_dim, output_size)\n",
        "    self.sig = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    batch_size = x.size(0)\n",
        "    x = x.long()\n",
        "    embeds = self.embedding(x)\n",
        "    lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "    out = self.dropout(lstm_out)\n",
        "    out = self.fc(out)\n",
        "    sig_out = self.sig(out)\n",
        "\n",
        "    sig_out = sig_out.view(batch_size, -1)\n",
        "    sig_out = sig_out[:, -1]\n",
        "\n",
        "    return sig_out, hidden\n",
        "  \n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    if(train_on_gpu):\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(), \n",
        "                weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "    else:\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim), \n",
        "                weight,new(self.n_layers, batch_size, self.hidden_dim))\n",
        "      \n",
        "    return hidden\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVyfFP90_dny",
        "colab_type": "text"
      },
      "source": [
        "# Instantiate the network\n",
        "Here, I will define the model hyper-parameters - \n",
        "1. `vocab_size`: Size of our vocabulary or the range of values for our input, word tokens.\n",
        "2. `output_size`: Size of our desired output; the number of class scores we want to output (pos/neg).\n",
        "3. `embedding_dim`: Number of columns in the embedding lookup table; size of our embeddings.\n",
        "4. `hidden_dim`: Number of units in the hidden layers of our LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
        "5. `n_layers`: Number of LSTM layers in the network. Typically between 1-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6gixAZK2xgl",
        "colab_type": "code",
        "outputId": "0078e293-437d-499b-fd7d-0271a9a1fff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "\n",
        "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentRNN(\n",
            "  (embedding): Embedding(74073, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP0ykQJ98gJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NoV8ArD20wX",
        "colab_type": "code",
        "outputId": "0f3d7f68-c669-4a5a-bca2-697767a935ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "epochs = 4\n",
        "counter = 0 \n",
        "print_every = 100\n",
        "clip = 5\n",
        "if(train_on_gpu):\n",
        "  net.cuda()\n",
        "\n",
        "net.train()\n",
        "for e in range(epochs):\n",
        "  h = net.init_hidden(batch_size)\n",
        "  for inputs, labels in train_loader:\n",
        "    counter += 1\n",
        "    if(train_on_gpu):\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    h = tuple([each.data for each in h])\n",
        "    net.zero_grad()\n",
        "    output, h = net(inputs, h)\n",
        "    loss = criterion(output.squeeze(), labels.float())\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm(net.parameters(), clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    if counter % print_every == 0:\n",
        "      val_h = net.init_hidden(batch_size)\n",
        "      val_losses = []\n",
        "      net.eval()\n",
        "      for inputs, labels in valid_loader:\n",
        "        val_h = tuple([each.data for each in val_h])\n",
        "        if(train_on_gpu):\n",
        "          inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        output, val_h = net(inputs, val_h)\n",
        "        val_loss = criterion(output.squeeze(), labels.float())\n",
        "        val_losses.append(val_loss.item())\n",
        "      net.train()\n",
        "      print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/4... Step: 100... Loss: 0.652403... Val Loss: 0.654631\n",
            "Epoch: 1/4... Step: 200... Loss: 0.675539... Val Loss: 0.696417\n",
            "Epoch: 1/4... Step: 300... Loss: 0.711188... Val Loss: 0.694983\n",
            "Epoch: 1/4... Step: 400... Loss: 0.689480... Val Loss: 0.692555\n",
            "Epoch: 2/4... Step: 500... Loss: 0.693494... Val Loss: 0.688481\n",
            "Epoch: 2/4... Step: 600... Loss: 0.650585... Val Loss: 0.697146\n",
            "Epoch: 2/4... Step: 700... Loss: 0.494658... Val Loss: 0.695837\n",
            "Epoch: 2/4... Step: 800... Loss: 0.663788... Val Loss: 0.710846\n",
            "Epoch: 3/4... Step: 900... Loss: 0.589776... Val Loss: 0.557277\n",
            "Epoch: 3/4... Step: 1000... Loss: 0.548341... Val Loss: 0.523456\n",
            "Epoch: 3/4... Step: 1100... Loss: 0.464825... Val Loss: 0.521520\n",
            "Epoch: 3/4... Step: 1200... Loss: 0.471691... Val Loss: 0.476805\n",
            "Epoch: 4/4... Step: 1300... Loss: 0.318779... Val Loss: 0.522014\n",
            "Epoch: 4/4... Step: 1400... Loss: 0.400367... Val Loss: 0.463238\n",
            "Epoch: 4/4... Step: 1500... Loss: 0.407411... Val Loss: 0.508995\n",
            "Epoch: 4/4... Step: 1600... Loss: 0.268926... Val Loss: 0.441884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlSvtJvm70br",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "22a1cc06-47b6-4451-f0dd-55201e21c28f"
      },
      "source": [
        "test_losses = []\n",
        "num_correct = 0\n",
        "\n",
        "# np.squeeze() removes single dimension enteries from array\n",
        "\n",
        "h = net.init_hidden(50)\n",
        "net.eval()\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "  h = tuple([each.data for each in h])\n",
        "  if(train_on_gpu):\n",
        "    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "  \n",
        "  output, h = net(inputs, h)\n",
        "  test_loss = criterion(output.squeeze(), labels.float())\n",
        "  test_losses.append(test_loss.item())\n",
        "  pred = torch.round(output.squeeze())\n",
        "  correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "  correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "  num_correct += np.sum(correct)\n",
        "\n",
        "# -- stats! -- ##\n",
        "# avg test loss\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.471\n",
            "Test accuracy: 0.796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbNOeEIc5e8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from string import punctuation\n",
        "\n",
        "def tokenize_movie_review(test_review):\n",
        "  test_review = test_review.lower()\n",
        "  test_text = ''.join([c for c in test_review if c not in punctuation])\n",
        "  test_words = test_text.split()\n",
        "  test_ints = []\n",
        "  test_ints.append([vocab_to_int[word] for word in test_words])\n",
        "  return test_ints"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PZZRFH-_7gB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "19a3c1e2-26c6-435d-c41d-ff5f51cdc0a8"
      },
      "source": [
        "test_review_neg = \"It was a very bad movie. Terrible acting.\"\n",
        "tokenized_review = tokenize_movie_review(test_review_neg)\n",
        "print(tokenize_movie_review(test_review_neg))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[8, 14, 3, 55, 76, 18, 388, 113]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TqfrdNJAHvU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e2f63a60-824e-4179-ba5f-8515c5d8d9ea"
      },
      "source": [
        "seq_length = 200\n",
        "features = pad_features(tokenized_review, seq_length)\n",
        "print(features)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   8  14   3  55  76  18\n",
            "  388 113]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFBSHEqBAX4F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f4be4fa-38c7-4bcf-dc10-c13c82e23052"
      },
      "source": [
        "feature_tensor = torch.from_numpy(features)\n",
        "print(feature_tensor.size(0))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8cV0EykAfDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, test_review, seq_length=200):\n",
        "  net.eval()\n",
        "  test_ints = tokenize_movie_review(test_review)\n",
        "  seq_length=seq_length\n",
        "  features = pad_features(test_ints, seq_length)\n",
        "  feature_tensor = torch.from_numpy(features)\n",
        "  batch_size = feature_tensor.size(0)\n",
        "  h = net.init_hidden(batch_size)\n",
        "  if(train_on_gpu):\n",
        "    feature_tensor=feature_tensor.cuda()\n",
        "  output, h = net(feature_tensor, h)\n",
        "  pred = torch.round(output.squeeze())\n",
        "  print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
        "  if(pred.item()==1):\n",
        "    print(\"Positive\")\n",
        "  else:\n",
        "    print(\"Negative\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUwrmYl_BZHN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "238f43e9-bb94-4de2-8f3d-c1aada923026"
      },
      "source": [
        "\n",
        "# positive test review\n",
        "test_review_pos = 'It was a very bad movie. Terrible acting. I will not recommend it at all. Full money waste.'\n",
        "seq_length=200 # good to use the length that was trained on\n",
        "\n",
        "predict(net, test_review_neg, seq_length)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction value, pre-rounding: 0.040038\n",
            "Negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29oK23Q1BqoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}