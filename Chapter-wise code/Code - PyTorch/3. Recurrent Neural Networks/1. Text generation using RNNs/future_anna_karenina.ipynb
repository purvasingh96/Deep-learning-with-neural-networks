{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Char-RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOGANUseQxSjxvHeqXL0DAK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/purvasingh96/Deep-learning-with-neural-networks/blob/master/Deep-learning-with-pytorch/3.%20Recurrent%20Neural%20Networks/Char_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX4-qHyjXdfh",
        "colab_type": "text"
      },
      "source": [
        "# Text generation with an RNN\n",
        "\n",
        "\n",
        "This notebook demonstrates how to generate text using a **charecter-level LSTM with PyTorch** using dataset from the book **Anna Karenina**. Given a sequence of charecter from this book, the model will generate longer sequences of data by calling the model repeatedly.\n",
        "\n",
        "While some of the sentences are grammatical, most do not make sense. The model has not learned the meaning of words, but consider:\n",
        "\n",
        "* The model is character-based. When training started, the model did not know how to spell an English word, or that words were even a unit of text.\n",
        "\n",
        "* The structure of the output resembles a playâ€”blocks of text generally begin with a speaker name, in all capital letters similar to the dataset.\n",
        "\n",
        "* As demonstrated below, the model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure. Below is the **general architecture of the character-wise RNN.**<br>\n",
        "<img src=\"https://github.com/purvasingh96/Deep-learning-with-neural-networks/blob/master/Deep-learning-with-pytorch/3.%20Recurrent%20Neural%20Networks/images/lstm_rnn_architecture.png?raw=1\"></img> \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaIndEAooDrp",
        "colab_type": "text"
      },
      "source": [
        "# Set Up\n",
        "### Import PyTorch and other libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUisr5JLZG18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "224cXT0PoWx5",
        "colab_type": "text"
      },
      "source": [
        "# Download the Anna Karenina data\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAOjATtAoky4",
        "colab_type": "text"
      },
      "source": [
        "### Read the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzQo8BJ_omPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('sample_data/anna.txt', 'r') as f:\n",
        "  text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnN-njxboz-f",
        "colab_type": "text"
      },
      "source": [
        "### First look at the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE8YyQE5o23N",
        "colab_type": "code",
        "outputId": "c5432c68-6d40-4cb1-ca92-18ce476930a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "print(text[:100])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chapter 1\n",
            "\n",
            "\n",
            "Happy families are all alike; every unhappy family is unhappy in its own\n",
            "way.\n",
            "\n",
            "Everythin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4VDdReg0gS5",
        "colab_type": "text"
      },
      "source": [
        "### GPU Usage\n",
        "Enable GPU acceleration to execute this notebook faster. In Colab: *Runtime > Change runtime type > Hardware acclerator > GPU*. If running locally make sure TensorFlow version >= 1.11."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPxKuEzGrK6t",
        "colab_type": "text"
      },
      "source": [
        "# Process the text\n",
        "### Vectorize the text (Tokenization)\n",
        "Before training, we need to map strings to a numerical representation. Create two lookup tables: one mapping characters to numbers, and another for numbers to characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Po_LfvwrVTj",
        "colab_type": "code",
        "outputId": "3b7dd1f6-9d06-4f84-b6d8-902cbe5a9229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch:ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode text\n",
        "encoded = np.array([char2int[ch] for ch in text])\n",
        "encoded"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1, 46, 32, ..., 70, 10, 46])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdomr__F0tiW",
        "colab_type": "text"
      },
      "source": [
        "# Pre-processing the data\n",
        "Our LSTM expects an input that is **one-hot encoded** meaning that each character is converted into an integer (via our created dictionary) and then converted into a column vector where only it's corresponding integer index will have the value of 1 and the rest of the vector will be filled with 0's."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV3jIMqG0234",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "  # initialize encoded array\n",
        "  # arr.shape = (3,8)\n",
        "  # np.arange(3) = [0, 1, 2]\n",
        "  # arr.flatten() = ([[1, 2, 3]]) => ([1, 2, 3])\n",
        "  one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "  one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
        "  one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "  return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiR-a3iH1bIC",
        "colab_type": "code",
        "outputId": "b54872ee-634b-47be-c12a-857af8f66f51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "test_seq = np.array([[3, 4, 5]])\n",
        "one_hot_encode(test_seq, 8)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0.]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPMJtKD34UG9",
        "colab_type": "text"
      },
      "source": [
        "# Making training mini-batches\n",
        "\n",
        "To train on this data, we create mini batches for training. We want our batches to be multiple sequences of some desired number of sequence steps as below-<br>\n",
        "<img src=\"https://github.com/purvasingh96/Deep-learning-with-neural-networks/blob/master/Deep-learning-with-pytorch/3.%20Recurrent%20Neural%20Networks/images/mini_batch_1.png?raw=1\"></img><br><br>\n",
        "In this example, we'll take the encoded characters (passed in as the arr parameter) and split them into multiple sequences, given by batch_size. Each of our sequences will be seq_length long."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeMQ8TXG79db",
        "colab_type": "text"
      },
      "source": [
        "# Creating Batches\n",
        "\n",
        "### 1. Discard text to accomodate completely full mini-batches\n",
        "\n",
        "* batch_size = `N (2)`\n",
        "* seq_length = `M (3)`\n",
        "* no. of charecters in one batch =` N * M (2 * 3 = 6 )`\n",
        "* Total batches `(K)` that can be made out of the given array :\n",
        "\n",
        "`len(arr)/ (no. of charecters per batch) = 12/6 = 2`\n",
        "\n",
        "* Total charecters in array to be kept in-order to accomodate completely full mini-batch - \n",
        "\n",
        "`arr[:N * M * K] = uptil arr[10]` (discarding arr[11]=12)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIONZY45hPeM",
        "colab_type": "text"
      },
      "source": [
        "### 2. Split the array into N batches\n",
        "You can do this by using :<br>`arr.reshape((batch_size, -1))`.<br>\n",
        "After this the size of array should be -<br>\n",
        "`N * (M * K)`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_r0oetGk4bd",
        "colab_type": "text"
      },
      "source": [
        "### 3. Iterate through mini-batches\n",
        "The idea is, each batch is of size `(N * M) window` on `N * (M * K) array`. This window slides over by `seq_length`. We also want both input and target arrays.\n",
        "<br>\n",
        "Target arrays are basically input arrays shifted over by one charecter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnQnpPo7k6bj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "  total_batch_size = batch_size*seq_length\n",
        "  n_batches = len(arr)//total_batch_size\n",
        "  arr = arr[:n_batches*total_batch_size]\n",
        "  arr = arr.reshape((batch_size, -1))\n",
        "  # iterate through array, on seq_length at a time\n",
        "  for n in range(0, arr.shape[1], seq_length):\n",
        "    x = arr[:, n:n+seq_length]\n",
        "    y = np.zeros_like(x)\n",
        "    try:\n",
        "      y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "    except IndexError:\n",
        "      y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "    yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuIRaLeDmpQs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "1cc14c87-6f51-40fa-c26b-7b9b191d03fa"
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[ 1 46 32 26 10 37 47 70 67 59]\n",
            " [ 5 33 58 19 70 14 32 41 37 19]\n",
            " [26  5 58 58 13 18 14 37 65 70]\n",
            " [70 58  5  0 37 70 58  5 47 10]\n",
            " [10 37 58 58 70 36 47  5  0 70]\n",
            " [10 70 18 37 19 70 26 47 32  8]\n",
            " [58 58 13 58 10 32 33 10 19 70]\n",
            " [37 44 19 70 26 47 13 33 41 37]]\n",
            "\n",
            "y\n",
            " [[46 32 26 10 37 47 70 67 59 59]\n",
            " [33 58 19 70 14 32 41 37 19 70]\n",
            " [ 5 58 58 13 18 14 37 65 70 74]\n",
            " [58  5  0 37 70 58  5 47 10 70]\n",
            " [37 58 58 70 36 47  5  0 70 10]\n",
            " [70 18 37 19 70 26 47 32  8 17]\n",
            " [58 13 58 10 32 33 10 19 70 10]\n",
            " [44 19 70 26 47 13 33 41 37 17]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSHBwUXxsbaD",
        "colab_type": "text"
      },
      "source": [
        "# Defining the network with PyTorch\n",
        "Below is sample structure of our LSTM model: <br>\n",
        "<img src=\"https://github.com/purvasingh96/Deep-learning-with-neural-networks/blob/master/Deep-learning-with-pytorch/3.%20Recurrent%20Neural%20Networks/data/15.%20rnn_classifier.png?raw=1\"></img><br>\n",
        "We will use PyTorch to define the model's architecture and define the forward pass method as well.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PC7YF30t59L",
        "colab_type": "text"
      },
      "source": [
        "### Model Structure\n",
        "In `__init__` followinf structure can be defined -<br>\n",
        "* Storing necessasry dictionaries (int2char, char2int)\n",
        "* Defining LSTM layer that takes the following parameters - \n",
        "  * `input size`\n",
        "  * hidden layer size (`n_hidden`)\n",
        "  * number of layers (`n_layers`)\n",
        "  * dropout probability (`drop_prob`)\n",
        "  * Boolean batch first (`batch_first`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osx33r64uw-B",
        "colab_type": "text"
      },
      "source": [
        "### LSTM Inputs/Outputs\n",
        "Basic LSTM can be created as follows - \n",
        "```python\n",
        "self.lstm = nn.LSTM(input_size, n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        " ```\n",
        "An initial hidden state of all zeros needs to be created as well -<br>\n",
        "```python\n",
        "self.init_hidden()\n",
        "``` \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob9_qiovwzxl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47136f6a-96b2-4da0-a454-5d47bf0d56db"
      },
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t6gGbIMxgD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "  def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
        "    super().__init__()\n",
        "    self.drop_prob = drop_prob\n",
        "    self.n_layers = n_layers\n",
        "    self.n_hidden = n_hidden\n",
        "    self.lr = lr\n",
        "\n",
        "    # creating charecter dictionaries\n",
        "    self.chars = tokens\n",
        "    self.int2char = dict(enumerate(self.chars))\n",
        "    self.char2int = {ch:ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "    # defining LSTM model\n",
        "    self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, batch_first=True, dropout=drop_prob)\n",
        "\n",
        "    # defining dropout layer\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "    # defining final fully-connected layer\n",
        "    self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    # lstm will generate new output and new hidden state\n",
        "    r_output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "    # passing x output through dropout layer\n",
        "    out = self.dropout(r_output)\n",
        "\n",
        "    # stacking LSTM using view\n",
        "    # Using contigious to reshape output\n",
        "    out =  out.contiguous().view(-1, self.n_hidden)#.contigious().view(-1, self.n_hidden)\n",
        "\n",
        "    # put out through fully connected layer\n",
        "    out = self.fc(out)\n",
        "\n",
        "    # returning final output and hidden state\n",
        "    return out, hidden\n",
        "  \n",
        "  def init_hidden(self, batch_size):\n",
        "    # creating 2 new tensors\n",
        "    # size = n_layers * batch_size * n_hidden\n",
        "    # initialize to 0 for hidden and cell state of LSTM\n",
        "    weight = next(self.parameters()).data\n",
        "    if(train_on_gpu):\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "    else:\n",
        "      hidden - (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "    return hidden\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhYeUS9W2Iyx",
        "colab_type": "text"
      },
      "source": [
        "# Time to Train\n",
        "\n",
        "Below we will use **Adam optimizer and Cross Entropy**. We calculate loss and perform backpropogation as usual. Few points to note -<br>\n",
        "* Within the batch loop, we detach the hidden state from its history; this time setting it equal to a new tuple variable because an LSTM has a hidden state that is a tuple of the hidden and cell states.\n",
        "* We use clip_grad_norm_ to help prevent exploding gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng9M-eqj2o2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "  net.train()\n",
        "  opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # creating training and validation data\n",
        "  val_idx = int(len(data)*(1-val_frac))\n",
        "  data, val_data = data[:val_idx], data[val_idx:]\n",
        "\n",
        "  if(train_on_gpu):\n",
        "    net.cuda()\n",
        "  counter = 0\n",
        "  n_chars = len(net.chars)\n",
        "  for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    for x, y in get_batches(data, batch_size, seq_length):\n",
        "      counter += 1\n",
        "\n",
        "      # one hot encode our data and make them torch tensors\n",
        "      x = one_hot_encode(x,n_chars)\n",
        "      inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "      if(train_on_gpu):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        \n",
        "      # create new hidden state variable to \n",
        "      # avoid traversing entire history \n",
        "      h = tuple([each.data for each in h])\n",
        "\n",
        "      net.zero_grad()\n",
        "      output, h = net(inputs, h)\n",
        "\n",
        "      loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "      loss.backward()\n",
        "\n",
        "      nn.utils.clip_grad_norm(net.parameters(), clip)\n",
        "      opt.step()\n",
        "\n",
        "      # loss statistics\n",
        "      if counter%print_every == 0:\n",
        "        val_h = net.init_hidden(batch_size)\n",
        "        val_losses = []\n",
        "        net.eval()\n",
        "        for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "          x = one_hot_encode(x, n_chars)\n",
        "          x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "          val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "          inputs, targets = x, y\n",
        "          if(train_on_gpu):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            \n",
        "          output, val_h = net(inputs, val_h)\n",
        "          val_loss = criterion(output, targets.view(batch_size*seq_length).long()) \n",
        "\n",
        "          val_losses.append(val_loss.item())  \n",
        "\n",
        "        net.train()\n",
        "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SHmvxYvGE4s",
        "colab_type": "text"
      },
      "source": [
        "### Instantiating the model\n",
        "Before training the model, we will first create the network with some given hyper-parameters. Then define mini-batches and start training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i95OCSFPJSdB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c932e28d-071a-4a1c-b6a0-aaa507710ad0"
      },
      "source": [
        "n_hidden = 512\n",
        "n_layers = 2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(75, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=75, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RZLVVH3OQ06",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "d4ffe90c-3005-4009-db8c-195c8afe785c"
      },
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20\n",
        "\n",
        "# training model\n",
        "train(net, encoded, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10... Step: 10... Loss: 3.2211... Val Loss: 3.1686\n",
            "Epoch: 1/10... Step: 20... Loss: 3.1917... Val Loss: 3.1080\n",
            "Epoch: 1/10... Step: 30... Loss: 3.1456... Val Loss: 3.0928\n",
            "Epoch: 1/10... Step: 40... Loss: 3.1375... Val Loss: 3.0888\n",
            "Epoch: 2/10... Step: 50... Loss: 3.1353... Val Loss: 3.0865\n",
            "Epoch: 2/10... Step: 60... Loss: 3.1278... Val Loss: 3.0846\n",
            "Epoch: 2/10... Step: 70... Loss: 3.1352... Val Loss: 3.0832\n",
            "Epoch: 2/10... Step: 80... Loss: 3.1139... Val Loss: 3.0743\n",
            "Epoch: 2/10... Step: 90... Loss: 3.0964... Val Loss: 3.0623\n",
            "Epoch: 3/10... Step: 100... Loss: 3.0625... Val Loss: 3.0478\n",
            "Epoch: 3/10... Step: 110... Loss: 3.0535... Val Loss: 2.9801\n",
            "Epoch: 3/10... Step: 120... Loss: 2.9026... Val Loss: 2.8468\n",
            "Epoch: 3/10... Step: 130... Loss: 2.7777... Val Loss: 2.6890\n",
            "Epoch: 4/10... Step: 140... Loss: 2.6579... Val Loss: 2.5888\n",
            "Epoch: 4/10... Step: 150... Loss: 2.5511... Val Loss: 2.4988\n",
            "Epoch: 4/10... Step: 160... Loss: 2.5353... Val Loss: 2.4445\n",
            "Epoch: 4/10... Step: 170... Loss: 2.4614... Val Loss: 2.4035\n",
            "Epoch: 4/10... Step: 180... Loss: 2.4484... Val Loss: 2.3875\n",
            "Epoch: 5/10... Step: 190... Loss: 2.3717... Val Loss: 2.3517\n",
            "Epoch: 5/10... Step: 200... Loss: 2.4018... Val Loss: 2.3197\n",
            "Epoch: 5/10... Step: 210... Loss: 2.3553... Val Loss: 2.2866\n",
            "Epoch: 5/10... Step: 220... Loss: 2.3200... Val Loss: 2.2569\n",
            "Epoch: 6/10... Step: 230... Loss: 2.2951... Val Loss: 2.2278\n",
            "Epoch: 6/10... Step: 240... Loss: 2.2449... Val Loss: 2.1993\n",
            "Epoch: 6/10... Step: 250... Loss: 2.2540... Val Loss: 2.1745\n",
            "Epoch: 6/10... Step: 260... Loss: 2.2054... Val Loss: 2.1474\n",
            "Epoch: 6/10... Step: 270... Loss: 2.2006... Val Loss: 2.1223\n",
            "Epoch: 7/10... Step: 280... Loss: 2.1193... Val Loss: 2.1056\n",
            "Epoch: 7/10... Step: 290... Loss: 2.1540... Val Loss: 2.0882\n",
            "Epoch: 7/10... Step: 300... Loss: 2.1136... Val Loss: 2.0580\n",
            "Epoch: 7/10... Step: 310... Loss: 2.0986... Val Loss: 2.0367\n",
            "Epoch: 8/10... Step: 320... Loss: 2.0735... Val Loss: 2.0192\n",
            "Epoch: 8/10... Step: 330... Loss: 2.0137... Val Loss: 2.0026\n",
            "Epoch: 8/10... Step: 340... Loss: 2.0512... Val Loss: 1.9844\n",
            "Epoch: 8/10... Step: 350... Loss: 2.0144... Val Loss: 1.9642\n",
            "Epoch: 8/10... Step: 360... Loss: 2.0253... Val Loss: 1.9470\n",
            "Epoch: 9/10... Step: 370... Loss: 1.9378... Val Loss: 1.9309\n",
            "Epoch: 9/10... Step: 380... Loss: 1.9804... Val Loss: 1.9147\n",
            "Epoch: 9/10... Step: 390... Loss: 1.9414... Val Loss: 1.8992\n",
            "Epoch: 9/10... Step: 400... Loss: 1.9308... Val Loss: 1.8853\n",
            "Epoch: 10/10... Step: 410... Loss: 1.9128... Val Loss: 1.8671\n",
            "Epoch: 10/10... Step: 420... Loss: 1.8588... Val Loss: 1.8532\n",
            "Epoch: 10/10... Step: 430... Loss: 1.9056... Val Loss: 1.8447\n",
            "Epoch: 10/10... Step: 440... Loss: 1.8708... Val Loss: 1.8413\n",
            "Epoch: 10/10... Step: 450... Loss: 1.9006... Val Loss: 1.8156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE6PWgR1Q8Ke",
        "colab_type": "text"
      },
      "source": [
        "### Getting the best model\n",
        "\n",
        "* If `training_loss << validation loss` - overfitting \n",
        "  * Solution - Regularization/Drop-out\n",
        "* If `training_loss ~ validation_loss` - underfitting\n",
        "  * Solution - Increase network size\n",
        "* Most important parameters - \n",
        "  * n_layers - 2/3\n",
        "  * n_hidden - adjustable according to your data size\n",
        "* Best model is the one with **least validation loss.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfVWnM5YTGML",
        "colab_type": "text"
      },
      "source": [
        "# Checkpoint\n",
        "After training, we'll save the model so we can load it again later if we need too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfTLGBX5TZD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = 'rnn_20_epoch.net'\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars    \n",
        "              }\n",
        "with open(model_name, 'wb') as f:\n",
        "  torch.save(checkpoint, f)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPZ-XSnzaRWB",
        "colab_type": "text"
      },
      "source": [
        "# Making Predictions\n",
        "The output of our RNN is from a fully-connected layer and it outputs a distribution of next-character scores. <br>\n",
        "To actually get the next character, we apply a softmax function, which gives us a probability distribution that we can then sample to predict the next character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOepmSveaese",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "  '''\n",
        "    Given a charecter, predict next charecter.\n",
        "    Returns the predicted charecter and the hidden state\n",
        "  '''\n",
        "\n",
        "  # tensor inputs\n",
        "  x = np.array([[net.char2int[char]]])\n",
        "  x = one_hot_encode(x, len(net.chars))\n",
        "  inputs = torch.from_numpy(x)\n",
        "\n",
        "  if(train_on_gpu):\n",
        "    inputs = inputs.cuda()\n",
        "  \n",
        "  h = tuple([each.data for each in h])\n",
        "  out, h = net(inputs, h)\n",
        "\n",
        "  p = F.softmax(out, dim=1).data\n",
        "  if(train_on_gpu):\n",
        "    p = p.cpu()\n",
        "\n",
        "  if top_k is None:\n",
        "    top_ch = np.arange(len(net.chars))\n",
        "  else:\n",
        "    p, top_ch = p.topk(top_k)\n",
        "    top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "  p = p.numpy().squeeze()\n",
        "  char = np.random.choice(top_ch, p=p/p.sum())\n",
        "\n",
        "  return net.int2char[char], h\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtr8vcyPgigT",
        "colab_type": "text"
      },
      "source": [
        "# Priming and Generaying Text\n",
        "\n",
        "Typically you'll want to prime the network so you can build up a hidden state. Otherwise the network will start out generating characters at random. In general the first bunch of characters will be a little rough since it hasn't built up a long history of characters to predict from.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu05SKQSg1Kg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(net, size, prime='Purva', top_k=None):\n",
        "  if(train_on_gpu):\n",
        "    net.cuda()\n",
        "  else:\n",
        "    net.cpu()\n",
        "  \n",
        "  net.eval()\n",
        "\n",
        "  chars = [ch for ch in prime]\n",
        "  h = net.init_hidden(1)\n",
        "  for ch in prime:\n",
        "    char, h = predict(net, ch, h, top_k=top_k)\n",
        "  \n",
        "  chars.append(char)\n",
        "\n",
        "  for ii in range(size):\n",
        "    char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "    chars.append(char)\n",
        "  \n",
        "  return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QroaxdXxiA7N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "49cbf750-8fa1-49bf-9fae-0ce712664b3a"
      },
      "source": [
        "print(sample(net, 1000, prime='Purva likes recurrent neural networks', top_k=5))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Purva likes recurrent neural networks world, and hid\n",
            "and at her. Se can the\n",
            "cansting, and and sereshing.\" \n",
            "\"Why mean and whing the stond, seid to to here so ment a constare at it she was she chark and he sad hin., sho lading has saing that whet he came of the peised to shan sead.. \n",
            "\"It tearing her asting to he how and a mentice with she seil ald a colled hor talk, and to\n",
            "shing her tome of the some with her then wand to me what werl of hor, and she had, and to don it, she was aly\n",
            "that all her.\n",
            "\n",
            "\"Wis to by the mare is it here ald your the don't, seid tele, and that his sathed as in he had\n",
            "say her sowe thild and ale and\n",
            "a mothed, and than she had strettion as it were, and he had beter the foll of him then\n",
            "stomt of his beconder a same a certing the paring of she was ale hear and\n",
            "the fack the was ofter, as hes how sad than sho her fert ond on that he saed.\n",
            "\n",
            "There a proulless on the\n",
            "mare and and whaten the sompted, and at they sond his ford wand at has shing that he she sadd on the sache with the simficing.\n",
            "\n",
            "\"Whet thal,\n",
            "thin cou\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KGms2naiHId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}