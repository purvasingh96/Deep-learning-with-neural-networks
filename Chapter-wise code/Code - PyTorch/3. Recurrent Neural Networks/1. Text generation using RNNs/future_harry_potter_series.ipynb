{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Harry_Potter",
      "provenance": [],
      "authorship_tag": "ABX9TyMz9Xj+6ZxeNyDGXUqS+wuU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/purvasingh96/Deep-learning-with-neural-networks/blob/master/Chapter-wise%20code/Code%20-%20PyTorch/3.%20Recurrent%20Neural%20Networks/Harry_Potter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0sgEvJ691SK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t07nFFS4INNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('harry_potter_1.txt', 'r') as f:\n",
        "  text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2kr6nTCIV8R",
        "colab_type": "code",
        "outputId": "477cf16c-7f74-4455-d728-67ad78304e99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1 CHAPTER ONE The Boy Who Lived M r and Mrs Dursley, of number four, Privet Drive, were proud to say'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmAgZsf9IbKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch : ii for ii, ch in int2char.items()}\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7BakAI9JCrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n44aJvR9NEgh",
        "colab_type": "code",
        "outputId": "1dce5c36-0b50-4076-8623-18861b783539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "test = np.array([[3, 4, 6]])\n",
        "one_hot = one_hot_encode(test, 8)\n",
        "print(one_hot)"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFlrTqs4NgAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "  batch_size_total = batch_size*seq_length\n",
        "  n_batches = len(arr)//batch_size_total\n",
        "  arr = arr[:n_batches*batch_size_total]\n",
        "  arr = arr.reshape((batch_size, -1))\n",
        "\n",
        "  for n in range(0, arr.shape[1], seq_length):\n",
        "      # The features\n",
        "      x = arr[:, n:n+seq_length]\n",
        "      # The targets, shifted by one\n",
        "      y = np.zeros_like(x)\n",
        "      try:\n",
        "        y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "      except IndexError:\n",
        "          y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "      yield x, y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I_dAnN6VcDM",
        "colab_type": "code",
        "outputId": "64e7ab15-893f-466e-88af-7ec7ba00a0ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "batches = generate_batches(encoded, 8, 50)\n",
        "x, y = next(batches)\n",
        "\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[29 63 64 60 37  9 53 59 73 63]\n",
            " [42 76 16 26 33 67 12 63 35  6]\n",
            " [65 63 22  6 30 16  4 11  6 26]\n",
            " [12  4 33  1 63 11 12 30 22 11]\n",
            " [63 34 42 40 56  6 12 44 63 68]\n",
            " [30 11 63 76 33 11 12  4 40 56]\n",
            " [40 30 33 67 12 63 43  6 30 33]\n",
            " [28 68 11 63 12 35 30 12 63  7]]\n",
            "\n",
            "y\n",
            " [[63 64 60 37  9 53 59 73 63 25]\n",
            " [76 16 26 33 67 12 63 35  6 16]\n",
            " [63 22  6 30 16  4 11  6 26 63]\n",
            " [ 4 33  1 63 11 12 30 22 11 63]\n",
            " [34 42 40 56  6 12 44 63 68 12]\n",
            " [11 63 76 33 11 12  4 40 56  4]\n",
            " [30 33 67 12 63 43  6 30 33 63]\n",
            " [68 11 63 12 35 30 12 63  7 35]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u4XmyEWVrD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "  def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
        "    super().__init__()\n",
        "    self.drop_prob = drop_prob\n",
        "    self.n_layers = n_layers\n",
        "    self.n_hidden = n_hidden\n",
        "    self.lr = lr\n",
        "\n",
        "    self.chars = tokens\n",
        "    self.int2char = dict(enumerate(self.chars))\n",
        "    self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "    self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
        "\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "    self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "  \n",
        "  def forward(self, x, hidden):\n",
        "    r_output, hidden = self.lstm(x, hidden)\n",
        "    out = self.dropout(r_output)\n",
        "    out = out.contiguous().view(-1, self.n_hidden)\n",
        "    out = self.fc(out)\n",
        "\n",
        "    return out, hidden\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "              weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "    return hidden\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM-iN9vaEXOW",
        "colab_type": "code",
        "outputId": "ae6a47ba-9a70-4a29-8730-86b39e1c9b49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rZprpA8omRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9aOLoYpxZ-q",
        "colab_type": "code",
        "outputId": "b2c26664-10b0-4648-bc4f-6283c6baeb81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "n_hidden = 512\n",
        "n_layers = 3\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)\n",
        "batch_size = 10\n",
        "seq_length = 10\n",
        "n_epochs = 40\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(77, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=77, bias=True)\n",
            ")\n",
            "Epoch: 1/40... Step: 10... Loss: 3.1513... Val Loss: 3.3527\n",
            "Epoch: 1/40... Step: 20... Loss: 3.4311... Val Loss: 3.3249\n",
            "Epoch: 1/40... Step: 30... Loss: 3.5290... Val Loss: 3.2952\n",
            "Epoch: 1/40... Step: 40... Loss: 3.2703... Val Loss: 3.2745\n",
            "Epoch: 1/40... Step: 50... Loss: 3.0551... Val Loss: 3.2843\n",
            "Epoch: 1/40... Step: 60... Loss: 3.4411... Val Loss: 3.2833\n",
            "Epoch: 1/40... Step: 70... Loss: 3.2058... Val Loss: 3.2719\n",
            "Epoch: 1/40... Step: 80... Loss: 3.2626... Val Loss: 3.2796\n",
            "Epoch: 1/40... Step: 90... Loss: 3.2019... Val Loss: 3.2726\n",
            "Epoch: 1/40... Step: 100... Loss: 3.3569... Val Loss: 3.2725\n",
            "Epoch: 1/40... Step: 110... Loss: 3.1359... Val Loss: 3.2685\n",
            "Epoch: 1/40... Step: 120... Loss: 3.2607... Val Loss: 3.2667\n",
            "Epoch: 1/40... Step: 130... Loss: 3.3359... Val Loss: 3.2553\n",
            "Epoch: 1/40... Step: 140... Loss: 3.0402... Val Loss: 3.2804\n",
            "Epoch: 1/40... Step: 150... Loss: 3.4176... Val Loss: 3.2856\n",
            "Epoch: 1/40... Step: 160... Loss: 3.0600... Val Loss: 3.2591\n",
            "Epoch: 1/40... Step: 170... Loss: 3.3333... Val Loss: 4.2973\n",
            "Epoch: 1/40... Step: 180... Loss: 3.0787... Val Loss: 3.2749\n",
            "Epoch: 1/40... Step: 190... Loss: 2.9419... Val Loss: 3.2976\n",
            "Epoch: 1/40... Step: 200... Loss: 3.3305... Val Loss: 3.2917\n",
            "Epoch: 1/40... Step: 210... Loss: 3.1864... Val Loss: 3.3014\n",
            "Epoch: 1/40... Step: 220... Loss: 3.4836... Val Loss: 3.2437\n",
            "Epoch: 1/40... Step: 230... Loss: 3.1042... Val Loss: 3.2154\n",
            "Epoch: 1/40... Step: 240... Loss: 3.2701... Val Loss: 3.1805\n",
            "Epoch: 2/40... Step: 250... Loss: 2.9942... Val Loss: 3.1448\n",
            "Epoch: 2/40... Step: 260... Loss: 3.0104... Val Loss: 3.0802\n",
            "Epoch: 2/40... Step: 270... Loss: 3.2140... Val Loss: 3.0282\n",
            "Epoch: 2/40... Step: 280... Loss: 3.0218... Val Loss: 2.9936\n",
            "Epoch: 2/40... Step: 290... Loss: 3.0488... Val Loss: 2.9800\n",
            "Epoch: 2/40... Step: 300... Loss: 3.1167... Val Loss: 2.9776\n",
            "Epoch: 2/40... Step: 310... Loss: 2.6567... Val Loss: 2.9491\n",
            "Epoch: 2/40... Step: 320... Loss: 2.8674... Val Loss: 3.0129\n",
            "Epoch: 2/40... Step: 330... Loss: 3.0756... Val Loss: 2.9128\n",
            "Epoch: 2/40... Step: 340... Loss: 2.7386... Val Loss: 2.8992\n",
            "Epoch: 2/40... Step: 350... Loss: 3.0145... Val Loss: 2.8775\n",
            "Epoch: 2/40... Step: 360... Loss: 2.8563... Val Loss: 2.8725\n",
            "Epoch: 2/40... Step: 370... Loss: 2.8637... Val Loss: 2.8370\n",
            "Epoch: 2/40... Step: 380... Loss: 2.9196... Val Loss: 2.8280\n",
            "Epoch: 2/40... Step: 390... Loss: 2.8933... Val Loss: 2.8189\n",
            "Epoch: 2/40... Step: 400... Loss: 2.8308... Val Loss: 2.7968\n",
            "Epoch: 2/40... Step: 410... Loss: 2.9728... Val Loss: 2.7999\n",
            "Epoch: 2/40... Step: 420... Loss: 2.9627... Val Loss: 2.7733\n",
            "Epoch: 2/40... Step: 430... Loss: 2.8201... Val Loss: 2.7404\n",
            "Epoch: 2/40... Step: 440... Loss: 2.6195... Val Loss: 2.7271\n",
            "Epoch: 2/40... Step: 450... Loss: 2.6724... Val Loss: 2.6939\n",
            "Epoch: 2/40... Step: 460... Loss: 2.7029... Val Loss: 2.6736\n",
            "Epoch: 2/40... Step: 470... Loss: 2.6262... Val Loss: 2.6740\n",
            "Epoch: 2/40... Step: 480... Loss: 2.6347... Val Loss: 2.6335\n",
            "Epoch: 2/40... Step: 490... Loss: 2.6650... Val Loss: 2.5965\n",
            "Epoch: 3/40... Step: 500... Loss: 2.7991... Val Loss: 2.6978\n",
            "Epoch: 3/40... Step: 510... Loss: 2.7036... Val Loss: 2.6402\n",
            "Epoch: 3/40... Step: 520... Loss: 2.7507... Val Loss: 2.5824\n",
            "Epoch: 3/40... Step: 530... Loss: 2.5124... Val Loss: 2.5716\n",
            "Epoch: 3/40... Step: 540... Loss: 2.4639... Val Loss: 2.5894\n",
            "Epoch: 3/40... Step: 550... Loss: 2.7200... Val Loss: 2.5387\n",
            "Epoch: 3/40... Step: 560... Loss: 2.3965... Val Loss: 2.5143\n",
            "Epoch: 3/40... Step: 570... Loss: 2.6520... Val Loss: 2.5194\n",
            "Epoch: 3/40... Step: 580... Loss: 2.7179... Val Loss: 2.4703\n",
            "Epoch: 3/40... Step: 590... Loss: 2.6167... Val Loss: 2.4739\n",
            "Epoch: 3/40... Step: 600... Loss: 2.5925... Val Loss: 2.4632\n",
            "Epoch: 3/40... Step: 610... Loss: 2.3988... Val Loss: 2.4219\n",
            "Epoch: 3/40... Step: 620... Loss: 2.4942... Val Loss: 2.4065\n",
            "Epoch: 3/40... Step: 630... Loss: 2.3762... Val Loss: 2.4667\n",
            "Epoch: 3/40... Step: 640... Loss: 2.1955... Val Loss: 2.4081\n",
            "Epoch: 3/40... Step: 650... Loss: 2.5113... Val Loss: 2.4641\n",
            "Epoch: 3/40... Step: 660... Loss: 2.4636... Val Loss: 2.4103\n",
            "Epoch: 3/40... Step: 670... Loss: 2.2996... Val Loss: 2.3638\n",
            "Epoch: 3/40... Step: 680... Loss: 2.1886... Val Loss: 2.3434\n",
            "Epoch: 3/40... Step: 690... Loss: 2.5901... Val Loss: 2.3249\n",
            "Epoch: 3/40... Step: 700... Loss: 2.3669... Val Loss: 2.3347\n",
            "Epoch: 3/40... Step: 710... Loss: 2.6392... Val Loss: 2.2939\n",
            "Epoch: 3/40... Step: 720... Loss: 2.4191... Val Loss: 2.2845\n",
            "Epoch: 3/40... Step: 730... Loss: 2.1740... Val Loss: 2.2676\n",
            "Epoch: 4/40... Step: 740... Loss: 2.4044... Val Loss: 2.2462\n",
            "Epoch: 4/40... Step: 750... Loss: 2.5304... Val Loss: 2.2406\n",
            "Epoch: 4/40... Step: 760... Loss: 2.1461... Val Loss: 2.2269\n",
            "Epoch: 4/40... Step: 770... Loss: 2.1853... Val Loss: 2.2007\n",
            "Epoch: 4/40... Step: 780... Loss: 2.1852... Val Loss: 2.2198\n",
            "Epoch: 4/40... Step: 790... Loss: 2.2180... Val Loss: 2.2161\n",
            "Epoch: 4/40... Step: 800... Loss: 2.1286... Val Loss: 2.1787\n",
            "Epoch: 4/40... Step: 810... Loss: 2.2438... Val Loss: 2.1768\n",
            "Epoch: 4/40... Step: 820... Loss: 2.1712... Val Loss: 2.1603\n",
            "Epoch: 4/40... Step: 830... Loss: 2.3592... Val Loss: 2.1576\n",
            "Epoch: 4/40... Step: 840... Loss: 2.1463... Val Loss: 2.1834\n",
            "Epoch: 4/40... Step: 850... Loss: 2.1769... Val Loss: 2.1724\n",
            "Epoch: 4/40... Step: 860... Loss: 2.0700... Val Loss: 2.1536\n",
            "Epoch: 4/40... Step: 870... Loss: 2.0471... Val Loss: 2.1606\n",
            "Epoch: 4/40... Step: 880... Loss: 2.0068... Val Loss: 2.1943\n",
            "Epoch: 4/40... Step: 890... Loss: 2.2322... Val Loss: 2.0971\n",
            "Epoch: 4/40... Step: 900... Loss: 2.5593... Val Loss: 2.0811\n",
            "Epoch: 4/40... Step: 910... Loss: 2.3997... Val Loss: 2.0902\n",
            "Epoch: 4/40... Step: 920... Loss: 1.9638... Val Loss: 2.0560\n",
            "Epoch: 4/40... Step: 930... Loss: 2.2644... Val Loss: 2.0657\n",
            "Epoch: 4/40... Step: 940... Loss: 2.4758... Val Loss: 2.0565\n",
            "Epoch: 4/40... Step: 950... Loss: 2.0751... Val Loss: 2.0462\n",
            "Epoch: 4/40... Step: 960... Loss: 2.5297... Val Loss: 2.0335\n",
            "Epoch: 4/40... Step: 970... Loss: 2.3240... Val Loss: 2.0599\n",
            "Epoch: 4/40... Step: 980... Loss: 2.1294... Val Loss: 2.0173\n",
            "Epoch: 5/40... Step: 990... Loss: 2.2478... Val Loss: 2.0158\n",
            "Epoch: 5/40... Step: 1000... Loss: 2.1199... Val Loss: 2.0178\n",
            "Epoch: 5/40... Step: 1010... Loss: 1.7529... Val Loss: 2.0059\n",
            "Epoch: 5/40... Step: 1020... Loss: 2.3345... Val Loss: 1.9964\n",
            "Epoch: 5/40... Step: 1030... Loss: 2.0843... Val Loss: 1.9853\n",
            "Epoch: 5/40... Step: 1040... Loss: 1.6763... Val Loss: 1.9849\n",
            "Epoch: 5/40... Step: 1050... Loss: 2.1606... Val Loss: 2.0923\n",
            "Epoch: 5/40... Step: 1060... Loss: 2.0179... Val Loss: 2.0824\n",
            "Epoch: 5/40... Step: 1070... Loss: 2.4862... Val Loss: 2.0689\n",
            "Epoch: 5/40... Step: 1080... Loss: 2.1389... Val Loss: 2.0407\n",
            "Epoch: 5/40... Step: 1090... Loss: 2.0080... Val Loss: 1.9956\n",
            "Epoch: 5/40... Step: 1100... Loss: 1.9410... Val Loss: 1.9997\n",
            "Epoch: 5/40... Step: 1110... Loss: 2.0196... Val Loss: 1.9948\n",
            "Epoch: 5/40... Step: 1120... Loss: 1.9288... Val Loss: 1.9595\n",
            "Epoch: 5/40... Step: 1130... Loss: 1.9687... Val Loss: 1.9675\n",
            "Epoch: 5/40... Step: 1140... Loss: 2.0416... Val Loss: 1.9305\n",
            "Epoch: 5/40... Step: 1150... Loss: 1.9965... Val Loss: 1.9347\n",
            "Epoch: 5/40... Step: 1160... Loss: 2.2290... Val Loss: 1.9326\n",
            "Epoch: 5/40... Step: 1170... Loss: 2.0507... Val Loss: 1.9082\n",
            "Epoch: 5/40... Step: 1180... Loss: 1.6749... Val Loss: 1.9086\n",
            "Epoch: 5/40... Step: 1190... Loss: 1.9717... Val Loss: 1.9049\n",
            "Epoch: 5/40... Step: 1200... Loss: 2.0387... Val Loss: 1.8794\n",
            "Epoch: 5/40... Step: 1210... Loss: 2.1962... Val Loss: 1.8863\n",
            "Epoch: 5/40... Step: 1220... Loss: 1.7237... Val Loss: 1.8830\n",
            "Epoch: 5/40... Step: 1230... Loss: 2.2125... Val Loss: 1.8680\n",
            "Epoch: 6/40... Step: 1240... Loss: 2.0025... Val Loss: 1.8860\n",
            "Epoch: 6/40... Step: 1250... Loss: 1.8847... Val Loss: 1.8861\n",
            "Epoch: 6/40... Step: 1260... Loss: 1.9591... Val Loss: 1.8850\n",
            "Epoch: 6/40... Step: 1270... Loss: 2.1264... Val Loss: 1.8825\n",
            "Epoch: 6/40... Step: 1280... Loss: 1.7963... Val Loss: 1.8972\n",
            "Epoch: 6/40... Step: 1290... Loss: 2.0035... Val Loss: 1.8726\n",
            "Epoch: 6/40... Step: 1300... Loss: 1.8563... Val Loss: 1.8693\n",
            "Epoch: 6/40... Step: 1310... Loss: 1.8303... Val Loss: 1.8666\n",
            "Epoch: 6/40... Step: 1320... Loss: 1.7807... Val Loss: 1.8579\n",
            "Epoch: 6/40... Step: 1330... Loss: 1.8307... Val Loss: 1.8767\n",
            "Epoch: 6/40... Step: 1340... Loss: 2.0644... Val Loss: 1.8488\n",
            "Epoch: 6/40... Step: 1350... Loss: 1.6997... Val Loss: 1.8634\n",
            "Epoch: 6/40... Step: 1360... Loss: 2.2012... Val Loss: 1.8653\n",
            "Epoch: 6/40... Step: 1370... Loss: 1.9531... Val Loss: 1.8684\n",
            "Epoch: 6/40... Step: 1380... Loss: 2.2382... Val Loss: 1.8415\n",
            "Epoch: 6/40... Step: 1390... Loss: 1.9559... Val Loss: 1.8307\n",
            "Epoch: 6/40... Step: 1400... Loss: 1.6957... Val Loss: 1.8309\n",
            "Epoch: 6/40... Step: 1410... Loss: 1.5970... Val Loss: 1.8100\n",
            "Epoch: 6/40... Step: 1420... Loss: 2.0373... Val Loss: 1.8077\n",
            "Epoch: 6/40... Step: 1430... Loss: 2.0304... Val Loss: 1.8058\n",
            "Epoch: 6/40... Step: 1440... Loss: 2.3740... Val Loss: 1.8044\n",
            "Epoch: 6/40... Step: 1450... Loss: 1.4335... Val Loss: 1.7784\n",
            "Epoch: 6/40... Step: 1460... Loss: 2.0954... Val Loss: 1.7993\n",
            "Epoch: 6/40... Step: 1470... Loss: 1.6298... Val Loss: 1.7844\n",
            "Epoch: 7/40... Step: 1480... Loss: 1.8409... Val Loss: 1.7791\n",
            "Epoch: 7/40... Step: 1490... Loss: 1.9749... Val Loss: 1.7887\n",
            "Epoch: 7/40... Step: 1500... Loss: 1.9015... Val Loss: 1.8010\n",
            "Epoch: 7/40... Step: 1510... Loss: 1.8920... Val Loss: 1.7813\n",
            "Epoch: 7/40... Step: 1520... Loss: 1.9639... Val Loss: 1.7766\n",
            "Epoch: 7/40... Step: 1530... Loss: 1.5193... Val Loss: 1.7791\n",
            "Epoch: 7/40... Step: 1540... Loss: 1.5041... Val Loss: 1.7638\n",
            "Epoch: 7/40... Step: 1550... Loss: 1.8740... Val Loss: 1.7657\n",
            "Epoch: 7/40... Step: 1560... Loss: 1.6977... Val Loss: 1.7785\n",
            "Epoch: 7/40... Step: 1570... Loss: 2.1736... Val Loss: 1.7742\n",
            "Epoch: 7/40... Step: 1580... Loss: 1.9013... Val Loss: 1.7838\n",
            "Epoch: 7/40... Step: 1590... Loss: 1.6668... Val Loss: 1.7721\n",
            "Epoch: 7/40... Step: 1600... Loss: 1.8590... Val Loss: 1.7779\n",
            "Epoch: 7/40... Step: 1610... Loss: 1.9368... Val Loss: 1.7608\n",
            "Epoch: 7/40... Step: 1620... Loss: 2.0071... Val Loss: 1.7706\n",
            "Epoch: 7/40... Step: 1630... Loss: 1.8781... Val Loss: 1.7648\n",
            "Epoch: 7/40... Step: 1640... Loss: 1.9983... Val Loss: 1.7632\n",
            "Epoch: 7/40... Step: 1650... Loss: 1.8984... Val Loss: 1.7683\n",
            "Epoch: 7/40... Step: 1660... Loss: 1.6009... Val Loss: 1.7370\n",
            "Epoch: 7/40... Step: 1670... Loss: 1.5171... Val Loss: 1.7364\n",
            "Epoch: 7/40... Step: 1680... Loss: 1.9804... Val Loss: 1.7405\n",
            "Epoch: 7/40... Step: 1690... Loss: 1.8967... Val Loss: 1.7214\n",
            "Epoch: 7/40... Step: 1700... Loss: 1.7620... Val Loss: 1.7161\n",
            "Epoch: 7/40... Step: 1710... Loss: 1.9442... Val Loss: 1.7139\n",
            "Epoch: 7/40... Step: 1720... Loss: 1.6102... Val Loss: 1.7097\n",
            "Epoch: 8/40... Step: 1730... Loss: 2.1307... Val Loss: 1.7287\n",
            "Epoch: 8/40... Step: 1740... Loss: 1.4125... Val Loss: 1.7297\n",
            "Epoch: 8/40... Step: 1750... Loss: 1.7897... Val Loss: 1.7243\n",
            "Epoch: 8/40... Step: 1760... Loss: 1.7531... Val Loss: 1.7169\n",
            "Epoch: 8/40... Step: 1770... Loss: 1.6688... Val Loss: 1.7178\n",
            "Epoch: 8/40... Step: 1780... Loss: 1.9397... Val Loss: 1.7243\n",
            "Epoch: 8/40... Step: 1790... Loss: 1.9786... Val Loss: 1.7079\n",
            "Epoch: 8/40... Step: 1800... Loss: 1.4817... Val Loss: 1.7083\n",
            "Epoch: 8/40... Step: 1810... Loss: 1.6242... Val Loss: 1.7089\n",
            "Epoch: 8/40... Step: 1820... Loss: 1.8980... Val Loss: 1.7200\n",
            "Epoch: 8/40... Step: 1830... Loss: 1.7419... Val Loss: 1.7047\n",
            "Epoch: 8/40... Step: 1840... Loss: 1.4493... Val Loss: 1.7174\n",
            "Epoch: 8/40... Step: 1850... Loss: 1.7925... Val Loss: 1.7198\n",
            "Epoch: 8/40... Step: 1860... Loss: 1.8913... Val Loss: 1.7124\n",
            "Epoch: 8/40... Step: 1870... Loss: 1.5048... Val Loss: 1.6970\n",
            "Epoch: 8/40... Step: 1880... Loss: 1.9391... Val Loss: 1.6968\n",
            "Epoch: 8/40... Step: 1890... Loss: 1.7866... Val Loss: 1.6963\n",
            "Epoch: 8/40... Step: 1900... Loss: 1.5800... Val Loss: 1.7098\n",
            "Epoch: 8/40... Step: 1910... Loss: 1.3963... Val Loss: 1.6782\n",
            "Epoch: 8/40... Step: 1920... Loss: 1.6286... Val Loss: 1.6991\n",
            "Epoch: 8/40... Step: 1930... Loss: 1.8563... Val Loss: 1.6684\n",
            "Epoch: 8/40... Step: 1940... Loss: 2.0699... Val Loss: 1.6666\n",
            "Epoch: 8/40... Step: 1950... Loss: 1.9558... Val Loss: 1.6699\n",
            "Epoch: 8/40... Step: 1960... Loss: 1.5652... Val Loss: 1.6658\n",
            "Epoch: 9/40... Step: 1970... Loss: 1.8600... Val Loss: 1.6715\n",
            "Epoch: 9/40... Step: 1980... Loss: 1.9190... Val Loss: 1.6734\n",
            "Epoch: 9/40... Step: 1990... Loss: 1.5942... Val Loss: 1.6786\n",
            "Epoch: 9/40... Step: 2000... Loss: 1.5234... Val Loss: 1.6642\n",
            "Epoch: 9/40... Step: 2010... Loss: 1.5689... Val Loss: 1.6676\n",
            "Epoch: 9/40... Step: 2020... Loss: 1.8210... Val Loss: 1.6724\n",
            "Epoch: 9/40... Step: 2030... Loss: 1.5658... Val Loss: 1.6768\n",
            "Epoch: 9/40... Step: 2040... Loss: 1.7441... Val Loss: 1.6637\n",
            "Epoch: 9/40... Step: 2050... Loss: 1.4677... Val Loss: 1.6622\n",
            "Epoch: 9/40... Step: 2060... Loss: 1.9883... Val Loss: 1.6889\n",
            "Epoch: 9/40... Step: 2070... Loss: 1.5244... Val Loss: 1.6862\n",
            "Epoch: 9/40... Step: 2080... Loss: 1.7410... Val Loss: 1.6640\n",
            "Epoch: 9/40... Step: 2090... Loss: 1.5068... Val Loss: 1.6719\n",
            "Epoch: 9/40... Step: 2100... Loss: 1.5329... Val Loss: 1.6611\n",
            "Epoch: 9/40... Step: 2110... Loss: 1.5143... Val Loss: 1.6846\n",
            "Epoch: 9/40... Step: 2120... Loss: 1.7069... Val Loss: 1.6725\n",
            "Epoch: 9/40... Step: 2130... Loss: 2.0737... Val Loss: 1.6640\n",
            "Epoch: 9/40... Step: 2140... Loss: 1.8194... Val Loss: 1.6595\n",
            "Epoch: 9/40... Step: 2150... Loss: 1.3592... Val Loss: 1.6453\n",
            "Epoch: 9/40... Step: 2160... Loss: 1.7250... Val Loss: 1.6356\n",
            "Epoch: 9/40... Step: 2170... Loss: 2.0199... Val Loss: 1.6486\n",
            "Epoch: 9/40... Step: 2180... Loss: 1.3958... Val Loss: 1.6426\n",
            "Epoch: 9/40... Step: 2190... Loss: 2.1551... Val Loss: 1.6232\n",
            "Epoch: 9/40... Step: 2200... Loss: 1.8787... Val Loss: 1.6284\n",
            "Epoch: 9/40... Step: 2210... Loss: 1.5970... Val Loss: 1.6114\n",
            "Epoch: 10/40... Step: 2220... Loss: 1.7447... Val Loss: 1.6265\n",
            "Epoch: 10/40... Step: 2230... Loss: 1.6838... Val Loss: 1.6351\n",
            "Epoch: 10/40... Step: 2240... Loss: 1.1052... Val Loss: 1.6397\n",
            "Epoch: 10/40... Step: 2250... Loss: 1.8781... Val Loss: 1.6261\n",
            "Epoch: 10/40... Step: 2260... Loss: 1.6152... Val Loss: 1.6238\n",
            "Epoch: 10/40... Step: 2270... Loss: 1.2982... Val Loss: 1.6316\n",
            "Epoch: 10/40... Step: 2280... Loss: 1.7755... Val Loss: 1.6309\n",
            "Epoch: 10/40... Step: 2290... Loss: 1.6403... Val Loss: 1.6243\n",
            "Epoch: 10/40... Step: 2300... Loss: 1.5973... Val Loss: 1.6372\n",
            "Epoch: 10/40... Step: 2310... Loss: 1.6725... Val Loss: 1.6435\n",
            "Epoch: 10/40... Step: 2320... Loss: 1.3238... Val Loss: 1.6523\n",
            "Epoch: 10/40... Step: 2330... Loss: 1.4500... Val Loss: 1.6301\n",
            "Epoch: 10/40... Step: 2340... Loss: 1.6645... Val Loss: 1.6403\n",
            "Epoch: 10/40... Step: 2350... Loss: 1.6180... Val Loss: 1.6271\n",
            "Epoch: 10/40... Step: 2360... Loss: 1.5917... Val Loss: 1.6465\n",
            "Epoch: 10/40... Step: 2370... Loss: 1.6282... Val Loss: 1.6406\n",
            "Epoch: 10/40... Step: 2380... Loss: 1.5900... Val Loss: 1.6298\n",
            "Epoch: 10/40... Step: 2390... Loss: 1.7163... Val Loss: 1.6251\n",
            "Epoch: 10/40... Step: 2400... Loss: 1.6762... Val Loss: 1.6142\n",
            "Epoch: 10/40... Step: 2410... Loss: 1.3690... Val Loss: 1.6110\n",
            "Epoch: 10/40... Step: 2420... Loss: 1.5148... Val Loss: 1.6058\n",
            "Epoch: 10/40... Step: 2430... Loss: 1.6613... Val Loss: 1.6098\n",
            "Epoch: 10/40... Step: 2440... Loss: 1.9545... Val Loss: 1.5989\n",
            "Epoch: 10/40... Step: 2450... Loss: 1.4959... Val Loss: 1.5978\n",
            "Epoch: 10/40... Step: 2460... Loss: 1.7132... Val Loss: 1.5867\n",
            "Epoch: 11/40... Step: 2470... Loss: 1.6223... Val Loss: 1.6086\n",
            "Epoch: 11/40... Step: 2480... Loss: 1.6341... Val Loss: 1.6095\n",
            "Epoch: 11/40... Step: 2490... Loss: 1.4299... Val Loss: 1.6136\n",
            "Epoch: 11/40... Step: 2500... Loss: 1.7180... Val Loss: 1.6181\n",
            "Epoch: 11/40... Step: 2510... Loss: 1.6931... Val Loss: 1.6120\n",
            "Epoch: 11/40... Step: 2520... Loss: 1.5800... Val Loss: 1.6113\n",
            "Epoch: 11/40... Step: 2530... Loss: 1.5318... Val Loss: 1.6013\n",
            "Epoch: 11/40... Step: 2540... Loss: 1.4573... Val Loss: 1.6089\n",
            "Epoch: 11/40... Step: 2550... Loss: 1.4165... Val Loss: 1.6166\n",
            "Epoch: 11/40... Step: 2560... Loss: 1.5005... Val Loss: 1.6249\n",
            "Epoch: 11/40... Step: 2570... Loss: 1.6076... Val Loss: 1.6161\n",
            "Epoch: 11/40... Step: 2580... Loss: 1.2362... Val Loss: 1.6310\n",
            "Epoch: 11/40... Step: 2590... Loss: 1.8783... Val Loss: 1.6264\n",
            "Epoch: 11/40... Step: 2600... Loss: 1.5140... Val Loss: 1.6166\n",
            "Epoch: 11/40... Step: 2610... Loss: 1.8328... Val Loss: 1.6066\n",
            "Epoch: 11/40... Step: 2620... Loss: 1.6230... Val Loss: 1.6082\n",
            "Epoch: 11/40... Step: 2630... Loss: 1.2456... Val Loss: 1.6152\n",
            "Epoch: 11/40... Step: 2640... Loss: 1.3334... Val Loss: 1.6007\n",
            "Epoch: 11/40... Step: 2650... Loss: 1.6858... Val Loss: 1.5903\n",
            "Epoch: 11/40... Step: 2660... Loss: 1.6581... Val Loss: 1.6037\n",
            "Epoch: 11/40... Step: 2670... Loss: 1.9389... Val Loss: 1.5871\n",
            "Epoch: 11/40... Step: 2680... Loss: 0.9705... Val Loss: 1.5848\n",
            "Epoch: 11/40... Step: 2690... Loss: 1.8909... Val Loss: 1.5800\n",
            "Epoch: 11/40... Step: 2700... Loss: 1.2194... Val Loss: 1.5715\n",
            "Epoch: 12/40... Step: 2710... Loss: 1.4794... Val Loss: 1.5818\n",
            "Epoch: 12/40... Step: 2720... Loss: 1.7694... Val Loss: 1.5825\n",
            "Epoch: 12/40... Step: 2730... Loss: 1.3870... Val Loss: 1.5796\n",
            "Epoch: 12/40... Step: 2740... Loss: 1.5447... Val Loss: 1.5958\n",
            "Epoch: 12/40... Step: 2750... Loss: 1.6034... Val Loss: 1.5913\n",
            "Epoch: 12/40... Step: 2760... Loss: 1.1920... Val Loss: 1.5961\n",
            "Epoch: 12/40... Step: 2770... Loss: 1.4549... Val Loss: 1.5942\n",
            "Epoch: 12/40... Step: 2780... Loss: 1.5624... Val Loss: 1.5918\n",
            "Epoch: 12/40... Step: 2790... Loss: 1.6048... Val Loss: 1.6096\n",
            "Epoch: 12/40... Step: 2800... Loss: 1.8687... Val Loss: 1.6006\n",
            "Epoch: 12/40... Step: 2810... Loss: 1.5547... Val Loss: 1.6159\n",
            "Epoch: 12/40... Step: 2820... Loss: 1.4539... Val Loss: 1.6102\n",
            "Epoch: 12/40... Step: 2830... Loss: 1.4667... Val Loss: 1.6101\n",
            "Epoch: 12/40... Step: 2840... Loss: 1.6265... Val Loss: 1.5945\n",
            "Epoch: 12/40... Step: 2850... Loss: 1.5981... Val Loss: 1.5872\n",
            "Epoch: 12/40... Step: 2860... Loss: 1.5847... Val Loss: 1.5775\n",
            "Epoch: 12/40... Step: 2870... Loss: 1.5418... Val Loss: 1.5842\n",
            "Epoch: 12/40... Step: 2880... Loss: 1.3599... Val Loss: 1.5965\n",
            "Epoch: 12/40... Step: 2890... Loss: 1.2454... Val Loss: 1.5911\n",
            "Epoch: 12/40... Step: 2900... Loss: 1.2485... Val Loss: 1.5998\n",
            "Epoch: 12/40... Step: 2910... Loss: 1.5960... Val Loss: 1.5901\n",
            "Epoch: 12/40... Step: 2920... Loss: 1.5120... Val Loss: 1.5692\n",
            "Epoch: 12/40... Step: 2930... Loss: 1.4705... Val Loss: 1.5703\n",
            "Epoch: 12/40... Step: 2940... Loss: 1.6131... Val Loss: 1.5622\n",
            "Epoch: 12/40... Step: 2950... Loss: 1.3994... Val Loss: 1.5621\n",
            "Epoch: 13/40... Step: 2960... Loss: 1.8163... Val Loss: 1.5893\n",
            "Epoch: 13/40... Step: 2970... Loss: 1.1757... Val Loss: 1.5976\n",
            "Epoch: 13/40... Step: 2980... Loss: 1.3635... Val Loss: 1.6000\n",
            "Epoch: 13/40... Step: 2990... Loss: 1.3992... Val Loss: 1.5849\n",
            "Epoch: 13/40... Step: 3000... Loss: 1.4359... Val Loss: 1.5846\n",
            "Epoch: 13/40... Step: 3010... Loss: 1.6941... Val Loss: 1.5808\n",
            "Epoch: 13/40... Step: 3020... Loss: 1.5834... Val Loss: 1.5798\n",
            "Epoch: 13/40... Step: 3030... Loss: 1.0944... Val Loss: 1.5853\n",
            "Epoch: 13/40... Step: 3040... Loss: 1.3868... Val Loss: 1.5952\n",
            "Epoch: 13/40... Step: 3050... Loss: 1.4509... Val Loss: 1.6086\n",
            "Epoch: 13/40... Step: 3060... Loss: 1.4246... Val Loss: 1.5890\n",
            "Epoch: 13/40... Step: 3070... Loss: 1.1650... Val Loss: 1.5845\n",
            "Epoch: 13/40... Step: 3080... Loss: 1.5904... Val Loss: 1.5921\n",
            "Epoch: 13/40... Step: 3090... Loss: 1.6724... Val Loss: 1.5891\n",
            "Epoch: 13/40... Step: 3100... Loss: 1.2854... Val Loss: 1.6111\n",
            "Epoch: 13/40... Step: 3110... Loss: 1.4993... Val Loss: 1.5879\n",
            "Epoch: 13/40... Step: 3120... Loss: 1.5342... Val Loss: 1.5874\n",
            "Epoch: 13/40... Step: 3130... Loss: 1.3395... Val Loss: 1.5776\n",
            "Epoch: 13/40... Step: 3140... Loss: 1.1127... Val Loss: 1.5771\n",
            "Epoch: 13/40... Step: 3150... Loss: 1.2413... Val Loss: 1.5840\n",
            "Epoch: 13/40... Step: 3160... Loss: 1.6030... Val Loss: 1.5656\n",
            "Epoch: 13/40... Step: 3170... Loss: 1.6693... Val Loss: 1.5606\n",
            "Epoch: 13/40... Step: 3180... Loss: 1.5502... Val Loss: 1.5548\n",
            "Epoch: 13/40... Step: 3190... Loss: 1.3959... Val Loss: 1.5573\n",
            "Epoch: 14/40... Step: 3200... Loss: 1.5722... Val Loss: 1.5471\n",
            "Epoch: 14/40... Step: 3210... Loss: 1.4473... Val Loss: 1.5531\n",
            "Epoch: 14/40... Step: 3220... Loss: 1.1783... Val Loss: 1.5641\n",
            "Epoch: 14/40... Step: 3230... Loss: 1.3473... Val Loss: 1.5824\n",
            "Epoch: 14/40... Step: 3240... Loss: 1.3284... Val Loss: 1.5760\n",
            "Epoch: 14/40... Step: 3250... Loss: 1.6051... Val Loss: 1.5814\n",
            "Epoch: 14/40... Step: 3260... Loss: 1.3725... Val Loss: 1.5813\n",
            "Epoch: 14/40... Step: 3270... Loss: 1.4312... Val Loss: 1.5685\n",
            "Epoch: 14/40... Step: 3280... Loss: 1.1854... Val Loss: 1.5774\n",
            "Epoch: 14/40... Step: 3290... Loss: 1.7343... Val Loss: 1.5943\n",
            "Epoch: 14/40... Step: 3300... Loss: 1.3172... Val Loss: 1.6026\n",
            "Epoch: 14/40... Step: 3310... Loss: 1.4100... Val Loss: 1.5902\n",
            "Epoch: 14/40... Step: 3320... Loss: 1.1957... Val Loss: 1.5909\n",
            "Epoch: 14/40... Step: 3330... Loss: 1.3753... Val Loss: 1.5979\n",
            "Epoch: 14/40... Step: 3340... Loss: 1.2888... Val Loss: 1.5936\n",
            "Epoch: 14/40... Step: 3350... Loss: 1.5286... Val Loss: 1.5985\n",
            "Epoch: 14/40... Step: 3360... Loss: 1.6292... Val Loss: 1.5889\n",
            "Epoch: 14/40... Step: 3370... Loss: 1.5376... Val Loss: 1.5973\n",
            "Epoch: 14/40... Step: 3380... Loss: 1.1341... Val Loss: 1.5906\n",
            "Epoch: 14/40... Step: 3390... Loss: 1.4179... Val Loss: 1.5922\n",
            "Epoch: 14/40... Step: 3400... Loss: 1.6017... Val Loss: 1.5976\n",
            "Epoch: 14/40... Step: 3410... Loss: 1.0453... Val Loss: 1.5815\n",
            "Epoch: 14/40... Step: 3420... Loss: 1.7761... Val Loss: 1.5702\n",
            "Epoch: 14/40... Step: 3430... Loss: 1.4402... Val Loss: 1.5508\n",
            "Epoch: 14/40... Step: 3440... Loss: 1.3143... Val Loss: 1.5566\n",
            "Epoch: 15/40... Step: 3450... Loss: 1.5037... Val Loss: 1.5614\n",
            "Epoch: 15/40... Step: 3460... Loss: 1.5302... Val Loss: 1.5726\n",
            "Epoch: 15/40... Step: 3470... Loss: 0.8583... Val Loss: 1.5825\n",
            "Epoch: 15/40... Step: 3480... Loss: 1.6641... Val Loss: 1.5786\n",
            "Epoch: 15/40... Step: 3490... Loss: 1.2560... Val Loss: 1.5730\n",
            "Epoch: 15/40... Step: 3500... Loss: 1.1037... Val Loss: 1.5707\n",
            "Epoch: 15/40... Step: 3510... Loss: 1.3534... Val Loss: 1.5773\n",
            "Epoch: 15/40... Step: 3520... Loss: 1.3931... Val Loss: 1.5802\n",
            "Epoch: 15/40... Step: 3530... Loss: 1.2211... Val Loss: 1.5964\n",
            "Epoch: 15/40... Step: 3540... Loss: 1.3857... Val Loss: 1.6061\n",
            "Epoch: 15/40... Step: 3550... Loss: 0.9208... Val Loss: 1.6028\n",
            "Epoch: 15/40... Step: 3560... Loss: 1.2950... Val Loss: 1.5886\n",
            "Epoch: 15/40... Step: 3570... Loss: 1.2673... Val Loss: 1.5914\n",
            "Epoch: 15/40... Step: 3580... Loss: 1.2909... Val Loss: 1.5899\n",
            "Epoch: 15/40... Step: 3590... Loss: 1.3344... Val Loss: 1.5897\n",
            "Epoch: 15/40... Step: 3600... Loss: 1.4803... Val Loss: 1.6021\n",
            "Epoch: 15/40... Step: 3610... Loss: 1.3536... Val Loss: 1.5835\n",
            "Epoch: 15/40... Step: 3620... Loss: 1.5496... Val Loss: 1.5879\n",
            "Epoch: 15/40... Step: 3630... Loss: 1.3684... Val Loss: 1.5984\n",
            "Epoch: 15/40... Step: 3640... Loss: 1.0785... Val Loss: 1.5978\n",
            "Epoch: 15/40... Step: 3650... Loss: 1.2786... Val Loss: 1.5761\n",
            "Epoch: 15/40... Step: 3660... Loss: 1.3983... Val Loss: 1.5727\n",
            "Epoch: 15/40... Step: 3670... Loss: 1.5903... Val Loss: 1.5619\n",
            "Epoch: 15/40... Step: 3680... Loss: 1.2106... Val Loss: 1.5651\n",
            "Epoch: 15/40... Step: 3690... Loss: 1.2792... Val Loss: 1.5689\n",
            "Epoch: 16/40... Step: 3700... Loss: 1.4780... Val Loss: 1.5735\n",
            "Epoch: 16/40... Step: 3710... Loss: 1.2889... Val Loss: 1.5726\n",
            "Epoch: 16/40... Step: 3720... Loss: 1.1299... Val Loss: 1.5766\n",
            "Epoch: 16/40... Step: 3730... Loss: 1.3913... Val Loss: 1.5792\n",
            "Epoch: 16/40... Step: 3740... Loss: 1.3368... Val Loss: 1.5878\n",
            "Epoch: 16/40... Step: 3750... Loss: 1.3661... Val Loss: 1.5873\n",
            "Epoch: 16/40... Step: 3760... Loss: 1.2524... Val Loss: 1.5697\n",
            "Epoch: 16/40... Step: 3770... Loss: 1.2173... Val Loss: 1.5732\n",
            "Epoch: 16/40... Step: 3780... Loss: 1.1077... Val Loss: 1.5935\n",
            "Epoch: 16/40... Step: 3790... Loss: 1.1597... Val Loss: 1.6105\n",
            "Epoch: 16/40... Step: 3800... Loss: 1.2765... Val Loss: 1.5999\n",
            "Epoch: 16/40... Step: 3810... Loss: 1.0758... Val Loss: 1.5862\n",
            "Epoch: 16/40... Step: 3820... Loss: 1.5094... Val Loss: 1.5938\n",
            "Epoch: 16/40... Step: 3830... Loss: 1.2439... Val Loss: 1.5834\n",
            "Epoch: 16/40... Step: 3840... Loss: 1.4127... Val Loss: 1.5935\n",
            "Epoch: 16/40... Step: 3850... Loss: 1.3908... Val Loss: 1.5970\n",
            "Epoch: 16/40... Step: 3860... Loss: 1.0991... Val Loss: 1.5996\n",
            "Epoch: 16/40... Step: 3870... Loss: 1.2040... Val Loss: 1.5948\n",
            "Epoch: 16/40... Step: 3880... Loss: 1.4173... Val Loss: 1.5925\n",
            "Epoch: 16/40... Step: 3890... Loss: 1.3827... Val Loss: 1.5951\n",
            "Epoch: 16/40... Step: 3900... Loss: 1.5680... Val Loss: 1.5617\n",
            "Epoch: 16/40... Step: 3910... Loss: 0.7295... Val Loss: 1.5535\n",
            "Epoch: 16/40... Step: 3920... Loss: 1.5192... Val Loss: 1.5695\n",
            "Epoch: 16/40... Step: 3930... Loss: 1.0599... Val Loss: 1.5595\n",
            "Epoch: 17/40... Step: 3940... Loss: 1.2133... Val Loss: 1.5531\n",
            "Epoch: 17/40... Step: 3950... Loss: 1.3123... Val Loss: 1.5602\n",
            "Epoch: 17/40... Step: 3960... Loss: 1.1834... Val Loss: 1.5727\n",
            "Epoch: 17/40... Step: 3970... Loss: 1.1898... Val Loss: 1.5817\n",
            "Epoch: 17/40... Step: 3980... Loss: 1.3333... Val Loss: 1.5751\n",
            "Epoch: 17/40... Step: 3990... Loss: 0.9510... Val Loss: 1.5947\n",
            "Epoch: 17/40... Step: 4000... Loss: 1.3052... Val Loss: 1.6078\n",
            "Epoch: 17/40... Step: 4010... Loss: 1.2026... Val Loss: 1.6033\n",
            "Epoch: 17/40... Step: 4020... Loss: 1.2886... Val Loss: 1.5924\n",
            "Epoch: 17/40... Step: 4030... Loss: 1.5083... Val Loss: 1.6104\n",
            "Epoch: 17/40... Step: 4040... Loss: 1.3698... Val Loss: 1.6167\n",
            "Epoch: 17/40... Step: 4050... Loss: 1.1835... Val Loss: 1.6055\n",
            "Epoch: 17/40... Step: 4060... Loss: 1.3983... Val Loss: 1.6109\n",
            "Epoch: 17/40... Step: 4070... Loss: 1.2115... Val Loss: 1.6054\n",
            "Epoch: 17/40... Step: 4080... Loss: 1.3545... Val Loss: 1.6106\n",
            "Epoch: 17/40... Step: 4090... Loss: 1.2316... Val Loss: 1.6184\n",
            "Epoch: 17/40... Step: 4100... Loss: 1.2581... Val Loss: 1.6148\n",
            "Epoch: 17/40... Step: 4110... Loss: 1.2073... Val Loss: 1.6103\n",
            "Epoch: 17/40... Step: 4120... Loss: 1.0138... Val Loss: 1.6163\n",
            "Epoch: 17/40... Step: 4130... Loss: 1.0754... Val Loss: 1.6091\n",
            "Epoch: 17/40... Step: 4140... Loss: 1.3040... Val Loss: 1.6085\n",
            "Epoch: 17/40... Step: 4150... Loss: 1.3338... Val Loss: 1.5904\n",
            "Epoch: 17/40... Step: 4160... Loss: 1.1082... Val Loss: 1.5955\n",
            "Epoch: 17/40... Step: 4170... Loss: 1.4314... Val Loss: 1.5874\n",
            "Epoch: 17/40... Step: 4180... Loss: 1.2361... Val Loss: 1.5831\n",
            "Epoch: 18/40... Step: 4190... Loss: 1.4319... Val Loss: 1.5957\n",
            "Epoch: 18/40... Step: 4200... Loss: 0.8521... Val Loss: 1.5929\n",
            "Epoch: 18/40... Step: 4210... Loss: 1.2061... Val Loss: 1.5921\n",
            "Epoch: 18/40... Step: 4220... Loss: 1.1970... Val Loss: 1.5989\n",
            "Epoch: 18/40... Step: 4230... Loss: 1.2176... Val Loss: 1.5950\n",
            "Epoch: 18/40... Step: 4240... Loss: 1.2607... Val Loss: 1.6111\n",
            "Epoch: 18/40... Step: 4250... Loss: 1.2204... Val Loss: 1.6023\n",
            "Epoch: 18/40... Step: 4260... Loss: 0.9391... Val Loss: 1.5968\n",
            "Epoch: 18/40... Step: 4270... Loss: 0.9711... Val Loss: 1.6001\n",
            "Epoch: 18/40... Step: 4280... Loss: 1.2785... Val Loss: 1.6222\n",
            "Epoch: 18/40... Step: 4290... Loss: 1.1795... Val Loss: 1.6264\n",
            "Epoch: 18/40... Step: 4300... Loss: 0.9750... Val Loss: 1.6119\n",
            "Epoch: 18/40... Step: 4310... Loss: 1.1703... Val Loss: 1.6126\n",
            "Epoch: 18/40... Step: 4320... Loss: 1.3904... Val Loss: 1.6114\n",
            "Epoch: 18/40... Step: 4330... Loss: 1.1544... Val Loss: 1.6155\n",
            "Epoch: 18/40... Step: 4340... Loss: 1.3142... Val Loss: 1.6025\n",
            "Epoch: 18/40... Step: 4350... Loss: 1.3059... Val Loss: 1.5934\n",
            "Epoch: 18/40... Step: 4360... Loss: 1.2527... Val Loss: 1.6054\n",
            "Epoch: 18/40... Step: 4370... Loss: 0.9774... Val Loss: 1.6132\n",
            "Epoch: 18/40... Step: 4380... Loss: 1.1797... Val Loss: 1.6109\n",
            "Epoch: 18/40... Step: 4390... Loss: 1.4281... Val Loss: 1.6023\n",
            "Epoch: 18/40... Step: 4400... Loss: 1.5132... Val Loss: 1.6062\n",
            "Epoch: 18/40... Step: 4410... Loss: 1.2942... Val Loss: 1.6050\n",
            "Epoch: 18/40... Step: 4420... Loss: 1.1868... Val Loss: 1.5905\n",
            "Epoch: 19/40... Step: 4430... Loss: 1.2221... Val Loss: 1.5944\n",
            "Epoch: 19/40... Step: 4440... Loss: 1.2419... Val Loss: 1.5941\n",
            "Epoch: 19/40... Step: 4450... Loss: 1.1326... Val Loss: 1.6025\n",
            "Epoch: 19/40... Step: 4460... Loss: 1.1750... Val Loss: 1.6167\n",
            "Epoch: 19/40... Step: 4470... Loss: 1.1240... Val Loss: 1.6106\n",
            "Epoch: 19/40... Step: 4480... Loss: 1.4265... Val Loss: 1.6080\n",
            "Epoch: 19/40... Step: 4490... Loss: 1.2238... Val Loss: 1.6233\n",
            "Epoch: 19/40... Step: 4500... Loss: 1.1387... Val Loss: 1.6328\n",
            "Epoch: 19/40... Step: 4510... Loss: 0.8954... Val Loss: 1.6346\n",
            "Epoch: 19/40... Step: 4520... Loss: 1.4574... Val Loss: 1.6345\n",
            "Epoch: 19/40... Step: 4530... Loss: 1.0815... Val Loss: 1.6378\n",
            "Epoch: 19/40... Step: 4540... Loss: 1.1919... Val Loss: 1.6236\n",
            "Epoch: 19/40... Step: 4550... Loss: 0.9939... Val Loss: 1.6222\n",
            "Epoch: 19/40... Step: 4560... Loss: 1.1392... Val Loss: 1.6191\n",
            "Epoch: 19/40... Step: 4570... Loss: 1.2017... Val Loss: 1.6256\n",
            "Epoch: 19/40... Step: 4580... Loss: 1.1912... Val Loss: 1.6381\n",
            "Epoch: 19/40... Step: 4590... Loss: 1.2446... Val Loss: 1.6387\n",
            "Epoch: 19/40... Step: 4600... Loss: 1.0631... Val Loss: 1.6182\n",
            "Epoch: 19/40... Step: 4610... Loss: 0.9201... Val Loss: 1.6143\n",
            "Epoch: 19/40... Step: 4620... Loss: 1.2303... Val Loss: 1.6132\n",
            "Epoch: 19/40... Step: 4630... Loss: 1.4864... Val Loss: 1.6177\n",
            "Epoch: 19/40... Step: 4640... Loss: 1.0003... Val Loss: 1.6011\n",
            "Epoch: 19/40... Step: 4650... Loss: 1.3253... Val Loss: 1.5962\n",
            "Epoch: 19/40... Step: 4660... Loss: 1.1885... Val Loss: 1.5941\n",
            "Epoch: 19/40... Step: 4670... Loss: 1.0139... Val Loss: 1.6038\n",
            "Epoch: 20/40... Step: 4680... Loss: 1.3691... Val Loss: 1.6015\n",
            "Epoch: 20/40... Step: 4690... Loss: 1.2357... Val Loss: 1.5923\n",
            "Epoch: 20/40... Step: 4700... Loss: 0.7136... Val Loss: 1.6006\n",
            "Epoch: 20/40... Step: 4710... Loss: 1.3117... Val Loss: 1.6036\n",
            "Epoch: 20/40... Step: 4720... Loss: 1.0713... Val Loss: 1.5991\n",
            "Epoch: 20/40... Step: 4730... Loss: 1.0313... Val Loss: 1.6225\n",
            "Epoch: 20/40... Step: 4740... Loss: 1.1788... Val Loss: 1.6310\n",
            "Epoch: 20/40... Step: 4750... Loss: 1.1714... Val Loss: 1.6312\n",
            "Epoch: 20/40... Step: 4760... Loss: 1.0789... Val Loss: 1.6370\n",
            "Epoch: 20/40... Step: 4770... Loss: 1.1503... Val Loss: 1.6540\n",
            "Epoch: 20/40... Step: 4780... Loss: 0.7928... Val Loss: 1.6435\n",
            "Epoch: 20/40... Step: 4790... Loss: 0.9471... Val Loss: 1.6418\n",
            "Epoch: 20/40... Step: 4800... Loss: 1.0455... Val Loss: 1.6382\n",
            "Epoch: 20/40... Step: 4810... Loss: 1.1437... Val Loss: 1.6406\n",
            "Epoch: 20/40... Step: 4820... Loss: 1.0743... Val Loss: 1.6406\n",
            "Epoch: 20/40... Step: 4830... Loss: 1.1460... Val Loss: 1.6493\n",
            "Epoch: 20/40... Step: 4840... Loss: 1.1962... Val Loss: 1.6555\n",
            "Epoch: 20/40... Step: 4850... Loss: 1.2541... Val Loss: 1.6439\n",
            "Epoch: 20/40... Step: 4860... Loss: 1.0980... Val Loss: 1.6569\n",
            "Epoch: 20/40... Step: 4870... Loss: 0.9778... Val Loss: 1.6399\n",
            "Epoch: 20/40... Step: 4880... Loss: 1.0982... Val Loss: 1.6521\n",
            "Epoch: 20/40... Step: 4890... Loss: 1.2840... Val Loss: 1.6337\n",
            "Epoch: 20/40... Step: 4900... Loss: 1.3613... Val Loss: 1.6168\n",
            "Epoch: 20/40... Step: 4910... Loss: 0.9784... Val Loss: 1.6057\n",
            "Epoch: 20/40... Step: 4920... Loss: 1.0519... Val Loss: 1.5977\n",
            "Epoch: 21/40... Step: 4930... Loss: 1.2523... Val Loss: 1.6050\n",
            "Epoch: 21/40... Step: 4940... Loss: 1.1229... Val Loss: 1.6077\n",
            "Epoch: 21/40... Step: 4950... Loss: 0.9034... Val Loss: 1.6246\n",
            "Epoch: 21/40... Step: 4960... Loss: 1.1686... Val Loss: 1.6203\n",
            "Epoch: 21/40... Step: 4970... Loss: 1.1752... Val Loss: 1.6324\n",
            "Epoch: 21/40... Step: 4980... Loss: 1.0819... Val Loss: 1.6532\n",
            "Epoch: 21/40... Step: 4990... Loss: 1.0475... Val Loss: 1.6571\n",
            "Epoch: 21/40... Step: 5000... Loss: 1.0108... Val Loss: 1.6579\n",
            "Epoch: 21/40... Step: 5010... Loss: 0.9403... Val Loss: 1.6624\n",
            "Epoch: 21/40... Step: 5020... Loss: 0.9648... Val Loss: 1.6671\n",
            "Epoch: 21/40... Step: 5030... Loss: 1.0494... Val Loss: 1.6484\n",
            "Epoch: 21/40... Step: 5040... Loss: 0.9053... Val Loss: 1.6463\n",
            "Epoch: 21/40... Step: 5050... Loss: 1.1539... Val Loss: 1.6470\n",
            "Epoch: 21/40... Step: 5060... Loss: 1.1152... Val Loss: 1.6537\n",
            "Epoch: 21/40... Step: 5070... Loss: 1.1572... Val Loss: 1.6647\n",
            "Epoch: 21/40... Step: 5080... Loss: 1.1463... Val Loss: 1.6740\n",
            "Epoch: 21/40... Step: 5090... Loss: 0.9725... Val Loss: 1.6649\n",
            "Epoch: 21/40... Step: 5100... Loss: 1.0140... Val Loss: 1.6487\n",
            "Epoch: 21/40... Step: 5110... Loss: 1.3825... Val Loss: 1.6498\n",
            "Epoch: 21/40... Step: 5120... Loss: 1.2461... Val Loss: 1.6627\n",
            "Epoch: 21/40... Step: 5130... Loss: 1.2449... Val Loss: 1.6693\n",
            "Epoch: 21/40... Step: 5140... Loss: 0.6483... Val Loss: 1.6589\n",
            "Epoch: 21/40... Step: 5150... Loss: 1.4005... Val Loss: 1.6416\n",
            "Epoch: 21/40... Step: 5160... Loss: 0.7909... Val Loss: 1.6352\n",
            "Epoch: 22/40... Step: 5170... Loss: 1.0872... Val Loss: 1.6340\n",
            "Epoch: 22/40... Step: 5180... Loss: 1.1549... Val Loss: 1.6429\n",
            "Epoch: 22/40... Step: 5190... Loss: 0.8809... Val Loss: 1.6382\n",
            "Epoch: 22/40... Step: 5200... Loss: 1.0317... Val Loss: 1.6459\n",
            "Epoch: 22/40... Step: 5210... Loss: 1.1033... Val Loss: 1.6489\n",
            "Epoch: 22/40... Step: 5220... Loss: 0.8371... Val Loss: 1.6577\n",
            "Epoch: 22/40... Step: 5230... Loss: 0.9239... Val Loss: 1.6689\n",
            "Epoch: 22/40... Step: 5240... Loss: 1.0113... Val Loss: 1.6901\n",
            "Epoch: 22/40... Step: 5250... Loss: 1.1078... Val Loss: 1.7114\n",
            "Epoch: 22/40... Step: 5260... Loss: 1.2211... Val Loss: 1.7178\n",
            "Epoch: 22/40... Step: 5270... Loss: 0.9464... Val Loss: 1.7155\n",
            "Epoch: 22/40... Step: 5280... Loss: 0.9058... Val Loss: 1.6835\n",
            "Epoch: 22/40... Step: 5290... Loss: 0.9468... Val Loss: 1.6821\n",
            "Epoch: 22/40... Step: 5300... Loss: 0.9469... Val Loss: 1.6888\n",
            "Epoch: 22/40... Step: 5310... Loss: 1.1114... Val Loss: 1.6990\n",
            "Epoch: 22/40... Step: 5320... Loss: 1.0287... Val Loss: 1.6950\n",
            "Epoch: 22/40... Step: 5330... Loss: 1.0345... Val Loss: 1.6976\n",
            "Epoch: 22/40... Step: 5340... Loss: 1.0388... Val Loss: 1.6811\n",
            "Epoch: 22/40... Step: 5350... Loss: 0.9866... Val Loss: 1.6884\n",
            "Epoch: 22/40... Step: 5360... Loss: 0.8912... Val Loss: 1.6863\n",
            "Epoch: 22/40... Step: 5370... Loss: 1.1425... Val Loss: 1.6855\n",
            "Epoch: 22/40... Step: 5380... Loss: 0.9668... Val Loss: 1.6783\n",
            "Epoch: 22/40... Step: 5390... Loss: 0.9065... Val Loss: 1.6745\n",
            "Epoch: 22/40... Step: 5400... Loss: 1.1833... Val Loss: 1.6609\n",
            "Epoch: 22/40... Step: 5410... Loss: 0.9504... Val Loss: 1.6565\n",
            "Epoch: 23/40... Step: 5420... Loss: 1.2360... Val Loss: 1.6513\n",
            "Epoch: 23/40... Step: 5430... Loss: 0.8584... Val Loss: 1.6644\n",
            "Epoch: 23/40... Step: 5440... Loss: 0.9469... Val Loss: 1.6829\n",
            "Epoch: 23/40... Step: 5450... Loss: 1.0756... Val Loss: 1.6781\n",
            "Epoch: 23/40... Step: 5460... Loss: 1.0895... Val Loss: 1.6692\n",
            "Epoch: 23/40... Step: 5470... Loss: 1.1171... Val Loss: 1.6870\n",
            "Epoch: 23/40... Step: 5480... Loss: 1.1096... Val Loss: 1.6919\n",
            "Epoch: 23/40... Step: 5490... Loss: 0.8358... Val Loss: 1.6901\n",
            "Epoch: 23/40... Step: 5500... Loss: 0.9480... Val Loss: 1.7079\n",
            "Epoch: 23/40... Step: 5510... Loss: 1.0281... Val Loss: 1.7172\n",
            "Epoch: 23/40... Step: 5520... Loss: 0.9975... Val Loss: 1.7062\n",
            "Epoch: 23/40... Step: 5530... Loss: 0.7000... Val Loss: 1.6958\n",
            "Epoch: 23/40... Step: 5540... Loss: 1.0525... Val Loss: 1.6983\n",
            "Epoch: 23/40... Step: 5550... Loss: 1.2536... Val Loss: 1.6959\n",
            "Epoch: 23/40... Step: 5560... Loss: 0.9153... Val Loss: 1.6993\n",
            "Epoch: 23/40... Step: 5570... Loss: 0.8961... Val Loss: 1.7083\n",
            "Epoch: 23/40... Step: 5580... Loss: 1.0512... Val Loss: 1.6931\n",
            "Epoch: 23/40... Step: 5590... Loss: 0.9883... Val Loss: 1.6909\n",
            "Epoch: 23/40... Step: 5600... Loss: 0.9542... Val Loss: 1.6934\n",
            "Epoch: 23/40... Step: 5610... Loss: 0.7855... Val Loss: 1.7135\n",
            "Epoch: 23/40... Step: 5620... Loss: 1.1657... Val Loss: 1.7058\n",
            "Epoch: 23/40... Step: 5630... Loss: 1.0247... Val Loss: 1.6973\n",
            "Epoch: 23/40... Step: 5640... Loss: 1.1102... Val Loss: 1.6751\n",
            "Epoch: 23/40... Step: 5650... Loss: 1.0601... Val Loss: 1.6621\n",
            "Epoch: 24/40... Step: 5660... Loss: 1.1810... Val Loss: 1.6682\n",
            "Epoch: 24/40... Step: 5670... Loss: 0.9757... Val Loss: 1.6602\n",
            "Epoch: 24/40... Step: 5680... Loss: 0.8544... Val Loss: 1.6594\n",
            "Epoch: 24/40... Step: 5690... Loss: 0.9453... Val Loss: 1.6698\n",
            "Epoch: 24/40... Step: 5700... Loss: 0.9212... Val Loss: 1.6657\n",
            "Epoch: 24/40... Step: 5710... Loss: 1.0983... Val Loss: 1.6599\n",
            "Epoch: 24/40... Step: 5720... Loss: 1.1123... Val Loss: 1.6723\n",
            "Epoch: 24/40... Step: 5730... Loss: 0.8593... Val Loss: 1.6921\n",
            "Epoch: 24/40... Step: 5740... Loss: 0.8202... Val Loss: 1.7130\n",
            "Epoch: 24/40... Step: 5750... Loss: 1.1724... Val Loss: 1.7301\n",
            "Epoch: 24/40... Step: 5760... Loss: 0.9499... Val Loss: 1.7254\n",
            "Epoch: 24/40... Step: 5770... Loss: 0.9747... Val Loss: 1.7127\n",
            "Epoch: 24/40... Step: 5780... Loss: 0.7997... Val Loss: 1.7182\n",
            "Epoch: 24/40... Step: 5790... Loss: 0.8877... Val Loss: 1.7092\n",
            "Epoch: 24/40... Step: 5800... Loss: 0.8698... Val Loss: 1.7167\n",
            "Epoch: 24/40... Step: 5810... Loss: 0.9057... Val Loss: 1.7325\n",
            "Epoch: 24/40... Step: 5820... Loss: 0.9970... Val Loss: 1.7487\n",
            "Epoch: 24/40... Step: 5830... Loss: 0.8921... Val Loss: 1.7355\n",
            "Epoch: 24/40... Step: 5840... Loss: 0.8629... Val Loss: 1.7279\n",
            "Epoch: 24/40... Step: 5850... Loss: 1.0843... Val Loss: 1.7181\n",
            "Epoch: 24/40... Step: 5860... Loss: 1.1559... Val Loss: 1.7330\n",
            "Epoch: 24/40... Step: 5870... Loss: 0.7400... Val Loss: 1.7299\n",
            "Epoch: 24/40... Step: 5880... Loss: 1.3172... Val Loss: 1.7136\n",
            "Epoch: 24/40... Step: 5890... Loss: 0.8649... Val Loss: 1.6996\n",
            "Epoch: 24/40... Step: 5900... Loss: 0.9033... Val Loss: 1.6955\n",
            "Epoch: 25/40... Step: 5910... Loss: 1.0281... Val Loss: 1.7141\n",
            "Epoch: 25/40... Step: 5920... Loss: 0.9815... Val Loss: 1.7266\n",
            "Epoch: 25/40... Step: 5930... Loss: 0.6289... Val Loss: 1.7365\n",
            "Epoch: 25/40... Step: 5940... Loss: 0.9915... Val Loss: 1.7459\n",
            "Epoch: 25/40... Step: 5950... Loss: 0.8565... Val Loss: 1.7269\n",
            "Epoch: 25/40... Step: 5960... Loss: 0.9326... Val Loss: 1.7120\n",
            "Epoch: 25/40... Step: 5970... Loss: 1.0767... Val Loss: 1.7070\n",
            "Epoch: 25/40... Step: 5980... Loss: 1.0417... Val Loss: 1.7099\n",
            "Epoch: 25/40... Step: 5990... Loss: 0.9122... Val Loss: 1.7266\n",
            "Epoch: 25/40... Step: 6000... Loss: 1.1027... Val Loss: 1.7383\n",
            "Epoch: 25/40... Step: 6010... Loss: 0.6913... Val Loss: 1.7244\n",
            "Epoch: 25/40... Step: 6020... Loss: 0.9335... Val Loss: 1.7152\n",
            "Epoch: 25/40... Step: 6030... Loss: 0.8357... Val Loss: 1.7207\n",
            "Epoch: 25/40... Step: 6040... Loss: 0.9175... Val Loss: 1.7458\n",
            "Epoch: 25/40... Step: 6050... Loss: 1.0280... Val Loss: 1.7614\n",
            "Epoch: 25/40... Step: 6060... Loss: 1.0996... Val Loss: 1.7636\n",
            "Epoch: 25/40... Step: 6070... Loss: 1.0874... Val Loss: 1.7548\n",
            "Epoch: 25/40... Step: 6080... Loss: 0.9335... Val Loss: 1.7382\n",
            "Epoch: 25/40... Step: 6090... Loss: 0.8800... Val Loss: 1.7325\n",
            "Epoch: 25/40... Step: 6100... Loss: 0.7937... Val Loss: 1.7394\n",
            "Epoch: 25/40... Step: 6110... Loss: 0.8247... Val Loss: 1.7370\n",
            "Epoch: 25/40... Step: 6120... Loss: 1.1315... Val Loss: 1.7271\n",
            "Epoch: 25/40... Step: 6130... Loss: 1.0415... Val Loss: 1.7097\n",
            "Epoch: 25/40... Step: 6140... Loss: 0.7541... Val Loss: 1.7162\n",
            "Epoch: 25/40... Step: 6150... Loss: 0.9360... Val Loss: 1.7161\n",
            "Epoch: 26/40... Step: 6160... Loss: 1.2082... Val Loss: 1.7064\n",
            "Epoch: 26/40... Step: 6170... Loss: 0.9235... Val Loss: 1.7125\n",
            "Epoch: 26/40... Step: 6180... Loss: 0.7990... Val Loss: 1.7097\n",
            "Epoch: 26/40... Step: 6190... Loss: 0.9968... Val Loss: 1.7086\n",
            "Epoch: 26/40... Step: 6200... Loss: 1.1970... Val Loss: 1.7020\n",
            "Epoch: 26/40... Step: 6210... Loss: 1.1926... Val Loss: 1.6997\n",
            "Epoch: 26/40... Step: 6220... Loss: 1.0823... Val Loss: 1.7151\n",
            "Epoch: 26/40... Step: 6230... Loss: 0.8789... Val Loss: 1.7325\n",
            "Epoch: 26/40... Step: 6240... Loss: 0.7240... Val Loss: 1.7386\n",
            "Epoch: 26/40... Step: 6250... Loss: 0.8572... Val Loss: 1.7538\n",
            "Epoch: 26/40... Step: 6260... Loss: 0.8554... Val Loss: 1.7586\n",
            "Epoch: 26/40... Step: 6270... Loss: 0.8421... Val Loss: 1.7474\n",
            "Epoch: 26/40... Step: 6280... Loss: 0.9815... Val Loss: 1.7371\n",
            "Epoch: 26/40... Step: 6290... Loss: 1.0460... Val Loss: 1.7444\n",
            "Epoch: 26/40... Step: 6300... Loss: 1.0651... Val Loss: 1.7581\n",
            "Epoch: 26/40... Step: 6310... Loss: 1.1628... Val Loss: 1.7684\n",
            "Epoch: 26/40... Step: 6320... Loss: 0.8546... Val Loss: 1.7608\n",
            "Epoch: 26/40... Step: 6330... Loss: 0.8627... Val Loss: 1.7498\n",
            "Epoch: 26/40... Step: 6340... Loss: 1.1129... Val Loss: 1.7369\n",
            "Epoch: 26/40... Step: 6350... Loss: 0.9718... Val Loss: 1.7348\n",
            "Epoch: 26/40... Step: 6360... Loss: 0.9362... Val Loss: 1.7394\n",
            "Epoch: 26/40... Step: 6370... Loss: 0.6228... Val Loss: 1.7443\n",
            "Epoch: 26/40... Step: 6380... Loss: 1.1906... Val Loss: 1.7287\n",
            "Epoch: 26/40... Step: 6390... Loss: 0.7960... Val Loss: 1.7233\n",
            "Epoch: 27/40... Step: 6400... Loss: 0.8553... Val Loss: 1.7290\n",
            "Epoch: 27/40... Step: 6410... Loss: 1.0453... Val Loss: 1.7234\n",
            "Epoch: 27/40... Step: 6420... Loss: 0.7225... Val Loss: 1.7202\n",
            "Epoch: 27/40... Step: 6430... Loss: 0.8688... Val Loss: 1.7212\n",
            "Epoch: 27/40... Step: 6440... Loss: 0.8512... Val Loss: 1.7245\n",
            "Epoch: 27/40... Step: 6450... Loss: 0.7139... Val Loss: 1.7300\n",
            "Epoch: 27/40... Step: 6460... Loss: 0.9304... Val Loss: 1.7316\n",
            "Epoch: 27/40... Step: 6470... Loss: 0.8399... Val Loss: 1.7345\n",
            "Epoch: 27/40... Step: 6480... Loss: 0.8998... Val Loss: 1.7564\n",
            "Epoch: 27/40... Step: 6490... Loss: 1.0290... Val Loss: 1.7781\n",
            "Epoch: 27/40... Step: 6500... Loss: 0.7091... Val Loss: 1.7712\n",
            "Epoch: 27/40... Step: 6510... Loss: 0.7693... Val Loss: 1.7519\n",
            "Epoch: 27/40... Step: 6520... Loss: 0.9479... Val Loss: 1.7444\n",
            "Epoch: 27/40... Step: 6530... Loss: 0.9313... Val Loss: 1.7448\n",
            "Epoch: 27/40... Step: 6540... Loss: 0.9592... Val Loss: 1.7515\n",
            "Epoch: 27/40... Step: 6550... Loss: 0.9324... Val Loss: 1.7684\n",
            "Epoch: 27/40... Step: 6560... Loss: 0.9492... Val Loss: 1.7762\n",
            "Epoch: 27/40... Step: 6570... Loss: 0.8861... Val Loss: 1.7697\n",
            "Epoch: 27/40... Step: 6580... Loss: 0.9452... Val Loss: 1.7731\n",
            "Epoch: 27/40... Step: 6590... Loss: 0.7484... Val Loss: 1.7706\n",
            "Epoch: 27/40... Step: 6600... Loss: 0.9279... Val Loss: 1.7781\n",
            "Epoch: 27/40... Step: 6610... Loss: 0.9813... Val Loss: 1.7852\n",
            "Epoch: 27/40... Step: 6620... Loss: 0.7787... Val Loss: 1.7520\n",
            "Epoch: 27/40... Step: 6630... Loss: 0.8972... Val Loss: 1.7445\n",
            "Epoch: 27/40... Step: 6640... Loss: 0.7608... Val Loss: 1.7573\n",
            "Epoch: 28/40... Step: 6650... Loss: 0.9336... Val Loss: 1.7566\n",
            "Epoch: 28/40... Step: 6660... Loss: 0.7260... Val Loss: 1.7532\n",
            "Epoch: 28/40... Step: 6670... Loss: 0.6798... Val Loss: 1.7496\n",
            "Epoch: 28/40... Step: 6680... Loss: 0.9582... Val Loss: 1.7522\n",
            "Epoch: 28/40... Step: 6690... Loss: 0.8457... Val Loss: 1.7571\n",
            "Epoch: 28/40... Step: 6700... Loss: 0.9995... Val Loss: 1.7706\n",
            "Epoch: 28/40... Step: 6710... Loss: 0.9828... Val Loss: 1.7765\n",
            "Epoch: 28/40... Step: 6720... Loss: 0.7094... Val Loss: 1.7835\n",
            "Epoch: 28/40... Step: 6730... Loss: 0.8025... Val Loss: 1.7998\n",
            "Epoch: 28/40... Step: 6740... Loss: 0.9062... Val Loss: 1.8094\n",
            "Epoch: 28/40... Step: 6750... Loss: 0.7443... Val Loss: 1.8018\n",
            "Epoch: 28/40... Step: 6760... Loss: 0.6462... Val Loss: 1.7982\n",
            "Epoch: 28/40... Step: 6770... Loss: 0.8840... Val Loss: 1.7988\n",
            "Epoch: 28/40... Step: 6780... Loss: 1.1080... Val Loss: 1.7845\n",
            "Epoch: 28/40... Step: 6790... Loss: 0.8967... Val Loss: 1.7714\n",
            "Epoch: 28/40... Step: 6800... Loss: 0.8900... Val Loss: 1.7800\n",
            "Epoch: 28/40... Step: 6810... Loss: 0.9885... Val Loss: 1.7960\n",
            "Epoch: 28/40... Step: 6820... Loss: 0.8816... Val Loss: 1.7940\n",
            "Epoch: 28/40... Step: 6830... Loss: 0.7500... Val Loss: 1.7951\n",
            "Epoch: 28/40... Step: 6840... Loss: 0.7494... Val Loss: 1.7883\n",
            "Epoch: 28/40... Step: 6850... Loss: 1.0360... Val Loss: 1.7802\n",
            "Epoch: 28/40... Step: 6860... Loss: 0.9166... Val Loss: 1.7882\n",
            "Epoch: 28/40... Step: 6870... Loss: 0.9610... Val Loss: 1.7793\n",
            "Epoch: 28/40... Step: 6880... Loss: 0.8396... Val Loss: 1.7615\n",
            "Epoch: 29/40... Step: 6890... Loss: 0.9119... Val Loss: 1.7740\n",
            "Epoch: 29/40... Step: 6900... Loss: 0.8932... Val Loss: 1.7757\n",
            "Epoch: 29/40... Step: 6910... Loss: 0.7895... Val Loss: 1.7698\n",
            "Epoch: 29/40... Step: 6920... Loss: 0.9535... Val Loss: 1.7652\n",
            "Epoch: 29/40... Step: 6930... Loss: 0.8431... Val Loss: 1.7698\n",
            "Epoch: 29/40... Step: 6940... Loss: 0.9361... Val Loss: 1.7610\n",
            "Epoch: 29/40... Step: 6950... Loss: 0.8684... Val Loss: 1.7825\n",
            "Epoch: 29/40... Step: 6960... Loss: 0.8230... Val Loss: 1.7954\n",
            "Epoch: 29/40... Step: 6970... Loss: 0.6849... Val Loss: 1.7875\n",
            "Epoch: 29/40... Step: 6980... Loss: 1.0965... Val Loss: 1.7921\n",
            "Epoch: 29/40... Step: 6990... Loss: 0.8443... Val Loss: 1.8132\n",
            "Epoch: 29/40... Step: 7000... Loss: 0.8803... Val Loss: 1.8068\n",
            "Epoch: 29/40... Step: 7010... Loss: 0.7507... Val Loss: 1.7979\n",
            "Epoch: 29/40... Step: 7020... Loss: 0.8018... Val Loss: 1.7909\n",
            "Epoch: 29/40... Step: 7030... Loss: 0.8749... Val Loss: 1.7958\n",
            "Epoch: 29/40... Step: 7040... Loss: 0.8197... Val Loss: 1.7943\n",
            "Epoch: 29/40... Step: 7050... Loss: 0.8111... Val Loss: 1.8190\n",
            "Epoch: 29/40... Step: 7060... Loss: 0.8664... Val Loss: 1.8211\n",
            "Epoch: 29/40... Step: 7070... Loss: 0.7901... Val Loss: 1.8128\n",
            "Epoch: 29/40... Step: 7080... Loss: 0.8869... Val Loss: 1.8076\n",
            "Epoch: 29/40... Step: 7090... Loss: 1.1417... Val Loss: 1.8106\n",
            "Epoch: 29/40... Step: 7100... Loss: 0.7975... Val Loss: 1.8296\n",
            "Epoch: 29/40... Step: 7110... Loss: 0.9802... Val Loss: 1.8189\n",
            "Epoch: 29/40... Step: 7120... Loss: 0.7495... Val Loss: 1.8134\n",
            "Epoch: 29/40... Step: 7130... Loss: 0.7965... Val Loss: 1.7913\n",
            "Epoch: 30/40... Step: 7140... Loss: 0.9384... Val Loss: 1.7773\n",
            "Epoch: 30/40... Step: 7150... Loss: 0.9375... Val Loss: 1.7922\n",
            "Epoch: 30/40... Step: 7160... Loss: 0.5784... Val Loss: 1.7962\n",
            "Epoch: 30/40... Step: 7170... Loss: 0.8264... Val Loss: 1.7970\n",
            "Epoch: 30/40... Step: 7180... Loss: 0.6990... Val Loss: 1.7897\n",
            "Epoch: 30/40... Step: 7190... Loss: 0.9185... Val Loss: 1.7919\n",
            "Epoch: 30/40... Step: 7200... Loss: 0.8611... Val Loss: 1.8053\n",
            "Epoch: 30/40... Step: 7210... Loss: 1.0015... Val Loss: 1.8176\n",
            "Epoch: 30/40... Step: 7220... Loss: 0.7224... Val Loss: 1.8225\n",
            "Epoch: 30/40... Step: 7230... Loss: 1.0310... Val Loss: 1.8324\n",
            "Epoch: 30/40... Step: 7240... Loss: 0.7265... Val Loss: 1.8474\n",
            "Epoch: 30/40... Step: 7250... Loss: 0.7847... Val Loss: 1.8344\n",
            "Epoch: 30/40... Step: 7260... Loss: 0.8624... Val Loss: 1.8385\n",
            "Epoch: 30/40... Step: 7270... Loss: 0.7128... Val Loss: 1.8424\n",
            "Epoch: 30/40... Step: 7280... Loss: 0.7244... Val Loss: 1.8440\n",
            "Epoch: 30/40... Step: 7290... Loss: 1.0457... Val Loss: 1.8467\n",
            "Epoch: 30/40... Step: 7300... Loss: 0.7511... Val Loss: 1.8522\n",
            "Epoch: 30/40... Step: 7310... Loss: 1.0637... Val Loss: 1.8306\n",
            "Epoch: 30/40... Step: 7320... Loss: 0.8380... Val Loss: 1.8228\n",
            "Epoch: 30/40... Step: 7330... Loss: 0.7510... Val Loss: 1.8188\n",
            "Epoch: 30/40... Step: 7340... Loss: 0.8086... Val Loss: 1.8203\n",
            "Epoch: 30/40... Step: 7350... Loss: 1.0111... Val Loss: 1.8473\n",
            "Epoch: 30/40... Step: 7360... Loss: 0.8492... Val Loss: 1.8442\n",
            "Epoch: 30/40... Step: 7370... Loss: 0.6681... Val Loss: 1.8306\n",
            "Epoch: 30/40... Step: 7380... Loss: 0.7924... Val Loss: 1.8257\n",
            "Epoch: 31/40... Step: 7390... Loss: 0.9172... Val Loss: 1.8143\n",
            "Epoch: 31/40... Step: 7400... Loss: 0.7320... Val Loss: 1.8163\n",
            "Epoch: 31/40... Step: 7410... Loss: 0.7278... Val Loss: 1.8270\n",
            "Epoch: 31/40... Step: 7420... Loss: 0.8957... Val Loss: 1.8202\n",
            "Epoch: 31/40... Step: 7430... Loss: 0.9242... Val Loss: 1.8204\n",
            "Epoch: 31/40... Step: 7440... Loss: 1.0178... Val Loss: 1.8409\n",
            "Epoch: 31/40... Step: 7450... Loss: 0.7868... Val Loss: 1.8536\n",
            "Epoch: 31/40... Step: 7460... Loss: 0.7149... Val Loss: 1.8517\n",
            "Epoch: 31/40... Step: 7470... Loss: 0.5642... Val Loss: 1.8324\n",
            "Epoch: 31/40... Step: 7480... Loss: 0.6369... Val Loss: 1.8309\n",
            "Epoch: 31/40... Step: 7490... Loss: 0.6054... Val Loss: 1.8372\n",
            "Epoch: 31/40... Step: 7500... Loss: 0.6883... Val Loss: 1.8429\n",
            "Epoch: 31/40... Step: 7510... Loss: 0.8866... Val Loss: 1.8466\n",
            "Epoch: 31/40... Step: 7520... Loss: 0.9078... Val Loss: 1.8348\n",
            "Epoch: 31/40... Step: 7530... Loss: 0.9172... Val Loss: 1.8347\n",
            "Epoch: 31/40... Step: 7540... Loss: 1.0051... Val Loss: 1.8280\n",
            "Epoch: 31/40... Step: 7550... Loss: 0.8331... Val Loss: 1.8362\n",
            "Epoch: 31/40... Step: 7560... Loss: 0.6836... Val Loss: 1.8368\n",
            "Epoch: 31/40... Step: 7570... Loss: 0.9792... Val Loss: 1.8390\n",
            "Epoch: 31/40... Step: 7580... Loss: 0.9509... Val Loss: 1.8387\n",
            "Epoch: 31/40... Step: 7590... Loss: 0.9852... Val Loss: 1.8445\n",
            "Epoch: 31/40... Step: 7600... Loss: 0.5760... Val Loss: 1.8494\n",
            "Epoch: 31/40... Step: 7610... Loss: 0.8670... Val Loss: 1.8237\n",
            "Epoch: 31/40... Step: 7620... Loss: 0.5937... Val Loss: 1.8137\n",
            "Epoch: 32/40... Step: 7630... Loss: 0.7806... Val Loss: 1.8266\n",
            "Epoch: 32/40... Step: 7640... Loss: 0.9776... Val Loss: 1.8390\n",
            "Epoch: 32/40... Step: 7650... Loss: 0.7756... Val Loss: 1.8324\n",
            "Epoch: 32/40... Step: 7660... Loss: 0.8169... Val Loss: 1.8384\n",
            "Epoch: 32/40... Step: 7670... Loss: 0.7530... Val Loss: 1.8407\n",
            "Epoch: 32/40... Step: 7680... Loss: 0.5885... Val Loss: 1.8356\n",
            "Epoch: 32/40... Step: 7690... Loss: 0.7825... Val Loss: 1.8301\n",
            "Epoch: 32/40... Step: 7700... Loss: 0.9479... Val Loss: 1.8409\n",
            "Epoch: 32/40... Step: 7710... Loss: 0.8687... Val Loss: 1.8489\n",
            "Epoch: 32/40... Step: 7720... Loss: 0.9384... Val Loss: 1.8695\n",
            "Epoch: 32/40... Step: 7730... Loss: 0.6354... Val Loss: 1.8739\n",
            "Epoch: 32/40... Step: 7740... Loss: 0.8091... Val Loss: 1.8611\n",
            "Epoch: 32/40... Step: 7750... Loss: 0.7285... Val Loss: 1.8528\n",
            "Epoch: 32/40... Step: 7760... Loss: 0.8256... Val Loss: 1.8572\n",
            "Epoch: 32/40... Step: 7770... Loss: 0.8632... Val Loss: 1.8654\n",
            "Epoch: 32/40... Step: 7780... Loss: 0.9470... Val Loss: 1.8578\n",
            "Epoch: 32/40... Step: 7790... Loss: 0.7178... Val Loss: 1.8578\n",
            "Epoch: 32/40... Step: 7800... Loss: 0.7106... Val Loss: 1.8552\n",
            "Epoch: 32/40... Step: 7810... Loss: 0.7189... Val Loss: 1.8541\n",
            "Epoch: 32/40... Step: 7820... Loss: 0.7751... Val Loss: 1.8613\n",
            "Epoch: 32/40... Step: 7830... Loss: 0.8521... Val Loss: 1.8561\n",
            "Epoch: 32/40... Step: 7840... Loss: 0.8868... Val Loss: 1.8552\n",
            "Epoch: 32/40... Step: 7850... Loss: 0.8861... Val Loss: 1.8545\n",
            "Epoch: 32/40... Step: 7860... Loss: 0.9703... Val Loss: 1.8485\n",
            "Epoch: 32/40... Step: 7870... Loss: 0.7636... Val Loss: 1.8317\n",
            "Epoch: 33/40... Step: 7880... Loss: 1.0325... Val Loss: 1.8270\n",
            "Epoch: 33/40... Step: 7890... Loss: 0.6140... Val Loss: 1.8356\n",
            "Epoch: 33/40... Step: 7900... Loss: 0.7633... Val Loss: 1.8493\n",
            "Epoch: 33/40... Step: 7910... Loss: 0.7751... Val Loss: 1.8438\n",
            "Epoch: 33/40... Step: 7920... Loss: 0.7636... Val Loss: 1.8339\n",
            "Epoch: 33/40... Step: 7930... Loss: 0.8049... Val Loss: 1.8441\n",
            "Epoch: 33/40... Step: 7940... Loss: 0.8220... Val Loss: 1.8532\n",
            "Epoch: 33/40... Step: 7950... Loss: 0.6212... Val Loss: 1.8660\n",
            "Epoch: 33/40... Step: 7960... Loss: 0.6607... Val Loss: 1.8847\n",
            "Epoch: 33/40... Step: 7970... Loss: 0.7899... Val Loss: 1.9207\n",
            "Epoch: 33/40... Step: 7980... Loss: 0.5902... Val Loss: 1.9219\n",
            "Epoch: 33/40... Step: 7990... Loss: 0.6645... Val Loss: 1.9158\n",
            "Epoch: 33/40... Step: 8000... Loss: 0.7666... Val Loss: 1.9004\n",
            "Epoch: 33/40... Step: 8010... Loss: 0.8987... Val Loss: 1.8820\n",
            "Epoch: 33/40... Step: 8020... Loss: 0.6748... Val Loss: 1.8885\n",
            "Epoch: 33/40... Step: 8030... Loss: 0.6306... Val Loss: 1.9048\n",
            "Epoch: 33/40... Step: 8040... Loss: 0.9314... Val Loss: 1.9065\n",
            "Epoch: 33/40... Step: 8050... Loss: 0.7724... Val Loss: 1.8872\n",
            "Epoch: 33/40... Step: 8060... Loss: 0.8077... Val Loss: 1.8880\n",
            "Epoch: 33/40... Step: 8070... Loss: 0.5872... Val Loss: 1.8958\n",
            "Epoch: 33/40... Step: 8080... Loss: 0.8257... Val Loss: 1.8833\n",
            "Epoch: 33/40... Step: 8090... Loss: 0.8732... Val Loss: 1.8708\n",
            "Epoch: 33/40... Step: 8100... Loss: 0.8574... Val Loss: 1.8688\n",
            "Epoch: 33/40... Step: 8110... Loss: 0.6474... Val Loss: 1.8663\n",
            "Epoch: 34/40... Step: 8120... Loss: 0.7029... Val Loss: 1.8645\n",
            "Epoch: 34/40... Step: 8130... Loss: 0.7315... Val Loss: 1.8695\n",
            "Epoch: 34/40... Step: 8140... Loss: 0.6184... Val Loss: 1.8942\n",
            "Epoch: 34/40... Step: 8150... Loss: 0.7290... Val Loss: 1.9105\n",
            "Epoch: 34/40... Step: 8160... Loss: 0.7313... Val Loss: 1.9063\n",
            "Epoch: 34/40... Step: 8170... Loss: 0.8938... Val Loss: 1.9021\n",
            "Epoch: 34/40... Step: 8180... Loss: 0.8649... Val Loss: 1.9051\n",
            "Epoch: 34/40... Step: 8190... Loss: 0.6609... Val Loss: 1.8956\n",
            "Epoch: 34/40... Step: 8200... Loss: 0.7099... Val Loss: 1.8863\n",
            "Epoch: 34/40... Step: 8210... Loss: 0.8315... Val Loss: 1.8999\n",
            "Epoch: 34/40... Step: 8220... Loss: 0.7473... Val Loss: 1.9197\n",
            "Epoch: 34/40... Step: 8230... Loss: 0.7577... Val Loss: 1.9216\n",
            "Epoch: 34/40... Step: 8240... Loss: 0.6256... Val Loss: 1.9260\n",
            "Epoch: 34/40... Step: 8250... Loss: 0.6377... Val Loss: 1.9232\n",
            "Epoch: 34/40... Step: 8260... Loss: 0.8102... Val Loss: 1.9279\n",
            "Epoch: 34/40... Step: 8270... Loss: 0.7037... Val Loss: 1.9246\n",
            "Epoch: 34/40... Step: 8280... Loss: 0.8469... Val Loss: 1.9194\n",
            "Epoch: 34/40... Step: 8290... Loss: 0.7398... Val Loss: 1.8989\n",
            "Epoch: 34/40... Step: 8300... Loss: 0.6394... Val Loss: 1.8976\n",
            "Epoch: 34/40... Step: 8310... Loss: 0.8017... Val Loss: 1.9029\n",
            "Epoch: 34/40... Step: 8320... Loss: 0.7836... Val Loss: 1.9027\n",
            "Epoch: 34/40... Step: 8330... Loss: 0.7356... Val Loss: 1.9004\n",
            "Epoch: 34/40... Step: 8340... Loss: 0.9149... Val Loss: 1.8953\n",
            "Epoch: 34/40... Step: 8350... Loss: 0.6708... Val Loss: 1.9043\n",
            "Epoch: 34/40... Step: 8360... Loss: 0.8345... Val Loss: 1.9061\n",
            "Epoch: 35/40... Step: 8370... Loss: 0.7682... Val Loss: 1.9055\n",
            "Epoch: 35/40... Step: 8380... Loss: 1.0311... Val Loss: 1.9010\n",
            "Epoch: 35/40... Step: 8390... Loss: 0.4083... Val Loss: 1.9105\n",
            "Epoch: 35/40... Step: 8400... Loss: 0.7858... Val Loss: 1.9235\n",
            "Epoch: 35/40... Step: 8410... Loss: 0.6767... Val Loss: 1.9212\n",
            "Epoch: 35/40... Step: 8420... Loss: 0.7183... Val Loss: 1.9139\n",
            "Epoch: 35/40... Step: 8430... Loss: 0.6725... Val Loss: 1.9207\n",
            "Epoch: 35/40... Step: 8440... Loss: 0.8465... Val Loss: 1.9285\n",
            "Epoch: 35/40... Step: 8450... Loss: 0.6380... Val Loss: 1.9362\n",
            "Epoch: 35/40... Step: 8460... Loss: 0.9045... Val Loss: 1.9463\n",
            "Epoch: 35/40... Step: 8470... Loss: 0.5559... Val Loss: 1.9614\n",
            "Epoch: 35/40... Step: 8480... Loss: 0.6718... Val Loss: 1.9633\n",
            "Epoch: 35/40... Step: 8490... Loss: 0.7479... Val Loss: 1.9479\n",
            "Epoch: 35/40... Step: 8500... Loss: 0.7537... Val Loss: 1.9396\n",
            "Epoch: 35/40... Step: 8510... Loss: 0.7724... Val Loss: 1.9357\n",
            "Epoch: 35/40... Step: 8520... Loss: 0.8409... Val Loss: 1.9409\n",
            "Epoch: 35/40... Step: 8530... Loss: 0.7527... Val Loss: 1.9397\n",
            "Epoch: 35/40... Step: 8540... Loss: 0.7317... Val Loss: 1.9335\n",
            "Epoch: 35/40... Step: 8550... Loss: 0.6732... Val Loss: 1.9372\n",
            "Epoch: 35/40... Step: 8560... Loss: 0.6369... Val Loss: 1.9408\n",
            "Epoch: 35/40... Step: 8570... Loss: 0.7426... Val Loss: 1.9573\n",
            "Epoch: 35/40... Step: 8580... Loss: 0.7362... Val Loss: 1.9500\n",
            "Epoch: 35/40... Step: 8590... Loss: 0.7570... Val Loss: 1.9421\n",
            "Epoch: 35/40... Step: 8600... Loss: 0.5833... Val Loss: 1.9230\n",
            "Epoch: 35/40... Step: 8610... Loss: 0.7811... Val Loss: 1.9020\n",
            "Epoch: 36/40... Step: 8620... Loss: 0.8548... Val Loss: 1.8957\n",
            "Epoch: 36/40... Step: 8630... Loss: 0.7370... Val Loss: 1.9121\n",
            "Epoch: 36/40... Step: 8640... Loss: 0.6007... Val Loss: 1.9385\n",
            "Epoch: 36/40... Step: 8650... Loss: 0.8010... Val Loss: 1.9554\n",
            "Epoch: 36/40... Step: 8660... Loss: 0.8376... Val Loss: 1.9627\n",
            "Epoch: 36/40... Step: 8670... Loss: 0.8363... Val Loss: 1.9618\n",
            "Epoch: 36/40... Step: 8680... Loss: 0.9528... Val Loss: 1.9559\n",
            "Epoch: 36/40... Step: 8690... Loss: 0.5803... Val Loss: 1.9436\n",
            "Epoch: 36/40... Step: 8700... Loss: 0.5293... Val Loss: 1.9492\n",
            "Epoch: 36/40... Step: 8710... Loss: 0.7192... Val Loss: 1.9706\n",
            "Epoch: 36/40... Step: 8720... Loss: 0.7136... Val Loss: 1.9815\n",
            "Epoch: 36/40... Step: 8730... Loss: 0.5988... Val Loss: 1.9829\n",
            "Epoch: 36/40... Step: 8740... Loss: 0.6823... Val Loss: 1.9950\n",
            "Epoch: 36/40... Step: 8750... Loss: 0.8440... Val Loss: 1.9957\n",
            "Epoch: 36/40... Step: 8760... Loss: 0.6963... Val Loss: 1.9908\n",
            "Epoch: 36/40... Step: 8770... Loss: 0.8412... Val Loss: 1.9810\n",
            "Epoch: 36/40... Step: 8780... Loss: 0.6399... Val Loss: 1.9601\n",
            "Epoch: 36/40... Step: 8790... Loss: 0.6319... Val Loss: 1.9364\n",
            "Epoch: 36/40... Step: 8800... Loss: 0.8657... Val Loss: 1.9420\n",
            "Epoch: 36/40... Step: 8810... Loss: 0.7154... Val Loss: 1.9514\n",
            "Epoch: 36/40... Step: 8820... Loss: 0.8344... Val Loss: 1.9623\n",
            "Epoch: 36/40... Step: 8830... Loss: 0.5029... Val Loss: 1.9662\n",
            "Epoch: 36/40... Step: 8840... Loss: 0.7934... Val Loss: 1.9466\n",
            "Epoch: 36/40... Step: 8850... Loss: 0.5848... Val Loss: 1.9266\n",
            "Epoch: 37/40... Step: 8860... Loss: 0.6873... Val Loss: 1.9361\n",
            "Epoch: 37/40... Step: 8870... Loss: 0.7977... Val Loss: 1.9410\n",
            "Epoch: 37/40... Step: 8880... Loss: 0.7347... Val Loss: 1.9344\n",
            "Epoch: 37/40... Step: 8890... Loss: 0.6898... Val Loss: 1.9345\n",
            "Epoch: 37/40... Step: 8900... Loss: 0.6857... Val Loss: 1.9335\n",
            "Epoch: 37/40... Step: 8910... Loss: 0.6098... Val Loss: 1.9294\n",
            "Epoch: 37/40... Step: 8920... Loss: 0.7132... Val Loss: 1.9442\n",
            "Epoch: 37/40... Step: 8930... Loss: 0.7341... Val Loss: 1.9627\n",
            "Epoch: 37/40... Step: 8940... Loss: 0.7959... Val Loss: 1.9565\n",
            "Epoch: 37/40... Step: 8950... Loss: 0.8324... Val Loss: 1.9686\n",
            "Epoch: 37/40... Step: 8960... Loss: 0.7397... Val Loss: 1.9752\n",
            "Epoch: 37/40... Step: 8970... Loss: 0.7159... Val Loss: 1.9666\n",
            "Epoch: 37/40... Step: 8980... Loss: 0.7476... Val Loss: 1.9842\n",
            "Epoch: 37/40... Step: 8990... Loss: 0.7759... Val Loss: 1.9805\n",
            "Epoch: 37/40... Step: 9000... Loss: 0.8252... Val Loss: 1.9910\n",
            "Epoch: 37/40... Step: 9010... Loss: 0.8782... Val Loss: 1.9949\n",
            "Epoch: 37/40... Step: 9020... Loss: 0.5279... Val Loss: 1.9910\n",
            "Epoch: 37/40... Step: 9030... Loss: 0.6739... Val Loss: 1.9830\n",
            "Epoch: 37/40... Step: 9040... Loss: 0.6193... Val Loss: 1.9759\n",
            "Epoch: 37/40... Step: 9050... Loss: 0.6434... Val Loss: 1.9883\n",
            "Epoch: 37/40... Step: 9060... Loss: 0.8058... Val Loss: 1.9907\n",
            "Epoch: 37/40... Step: 9070... Loss: 0.5583... Val Loss: 1.9795\n",
            "Epoch: 37/40... Step: 9080... Loss: 0.6363... Val Loss: 1.9713\n",
            "Epoch: 37/40... Step: 9090... Loss: 0.7841... Val Loss: 1.9599\n",
            "Epoch: 37/40... Step: 9100... Loss: 0.5994... Val Loss: 1.9561\n",
            "Epoch: 38/40... Step: 9110... Loss: 0.7930... Val Loss: 1.9671\n",
            "Epoch: 38/40... Step: 9120... Loss: 0.5484... Val Loss: 1.9773\n",
            "Epoch: 38/40... Step: 9130... Loss: 0.6779... Val Loss: 1.9665\n",
            "Epoch: 38/40... Step: 9140... Loss: 0.6761... Val Loss: 1.9676\n",
            "Epoch: 38/40... Step: 9150... Loss: 0.6668... Val Loss: 1.9746\n",
            "Epoch: 38/40... Step: 9160... Loss: 0.7236... Val Loss: 1.9713\n",
            "Epoch: 38/40... Step: 9170... Loss: 0.7540... Val Loss: 1.9674\n",
            "Epoch: 38/40... Step: 9180... Loss: 0.5585... Val Loss: 1.9654\n",
            "Epoch: 38/40... Step: 9190... Loss: 0.7377... Val Loss: 1.9653\n",
            "Epoch: 38/40... Step: 9200... Loss: 0.5427... Val Loss: 1.9728\n",
            "Epoch: 38/40... Step: 9210... Loss: 0.7997... Val Loss: 1.9902\n",
            "Epoch: 38/40... Step: 9220... Loss: 0.5743... Val Loss: 1.9970\n",
            "Epoch: 38/40... Step: 9230... Loss: 0.7675... Val Loss: 2.0029\n",
            "Epoch: 38/40... Step: 9240... Loss: 0.6243... Val Loss: 1.9792\n",
            "Epoch: 38/40... Step: 9250... Loss: 0.7078... Val Loss: 1.9730\n",
            "Epoch: 38/40... Step: 9260... Loss: 0.5160... Val Loss: 1.9854\n",
            "Epoch: 38/40... Step: 9270... Loss: 0.7171... Val Loss: 1.9973\n",
            "Epoch: 38/40... Step: 9280... Loss: 0.6672... Val Loss: 1.9843\n",
            "Epoch: 38/40... Step: 9290... Loss: 0.6463... Val Loss: 1.9683\n",
            "Epoch: 38/40... Step: 9300... Loss: 0.6015... Val Loss: 1.9572\n",
            "Epoch: 38/40... Step: 9310... Loss: 0.6421... Val Loss: 1.9644\n",
            "Epoch: 38/40... Step: 9320... Loss: 0.7517... Val Loss: 1.9717\n",
            "Epoch: 38/40... Step: 9330... Loss: 0.6748... Val Loss: 1.9491\n",
            "Epoch: 38/40... Step: 9340... Loss: 0.5952... Val Loss: 1.9276\n",
            "Epoch: 39/40... Step: 9350... Loss: 0.7196... Val Loss: 1.9349\n",
            "Epoch: 39/40... Step: 9360... Loss: 0.7149... Val Loss: 1.9399\n",
            "Epoch: 39/40... Step: 9370... Loss: 0.7181... Val Loss: 1.9558\n",
            "Epoch: 39/40... Step: 9380... Loss: 0.5790... Val Loss: 1.9684\n",
            "Epoch: 39/40... Step: 9390... Loss: 0.6792... Val Loss: 1.9692\n",
            "Epoch: 39/40... Step: 9400... Loss: 0.9047... Val Loss: 1.9898\n",
            "Epoch: 39/40... Step: 9410... Loss: 0.8638... Val Loss: 2.0014\n",
            "Epoch: 39/40... Step: 9420... Loss: 0.5514... Val Loss: 2.0128\n",
            "Epoch: 39/40... Step: 9430... Loss: 0.5798... Val Loss: 2.0128\n",
            "Epoch: 39/40... Step: 9440... Loss: 0.7610... Val Loss: 2.0238\n",
            "Epoch: 39/40... Step: 9450... Loss: 0.8191... Val Loss: 2.0283\n",
            "Epoch: 39/40... Step: 9460... Loss: 0.8153... Val Loss: 2.0271\n",
            "Epoch: 39/40... Step: 9470... Loss: 0.4506... Val Loss: 2.0181\n",
            "Epoch: 39/40... Step: 9480... Loss: 0.7531... Val Loss: 2.0199\n",
            "Epoch: 39/40... Step: 9490... Loss: 0.6387... Val Loss: 2.0301\n",
            "Epoch: 39/40... Step: 9500... Loss: 0.7775... Val Loss: 2.0257\n",
            "Epoch: 39/40... Step: 9510... Loss: 0.6733... Val Loss: 2.0138\n",
            "Epoch: 39/40... Step: 9520... Loss: 0.6847... Val Loss: 1.9928\n",
            "Epoch: 39/40... Step: 9530... Loss: 0.6009... Val Loss: 1.9914\n",
            "Epoch: 39/40... Step: 9540... Loss: 0.6744... Val Loss: 2.0082\n",
            "Epoch: 39/40... Step: 9550... Loss: 0.7329... Val Loss: 2.0091\n",
            "Epoch: 39/40... Step: 9560... Loss: 0.5723... Val Loss: 2.0034\n",
            "Epoch: 39/40... Step: 9570... Loss: 0.6774... Val Loss: 2.0043\n",
            "Epoch: 39/40... Step: 9580... Loss: 0.5950... Val Loss: 2.0053\n",
            "Epoch: 39/40... Step: 9590... Loss: 0.6448... Val Loss: 1.9862\n",
            "Epoch: 40/40... Step: 9600... Loss: 0.7822... Val Loss: 1.9758\n",
            "Epoch: 40/40... Step: 9610... Loss: 0.7922... Val Loss: 1.9838\n",
            "Epoch: 40/40... Step: 9620... Loss: 0.4535... Val Loss: 1.9924\n",
            "Epoch: 40/40... Step: 9630... Loss: 0.7616... Val Loss: 2.0083\n",
            "Epoch: 40/40... Step: 9640... Loss: 0.7311... Val Loss: 2.0092\n",
            "Epoch: 40/40... Step: 9650... Loss: 0.4468... Val Loss: 1.9958\n",
            "Epoch: 40/40... Step: 9660... Loss: 0.5931... Val Loss: 2.0001\n",
            "Epoch: 40/40... Step: 9670... Loss: 0.7672... Val Loss: 2.0182\n",
            "Epoch: 40/40... Step: 9680... Loss: 0.5557... Val Loss: 2.0337\n",
            "Epoch: 40/40... Step: 9690... Loss: 0.8112... Val Loss: 2.0495\n",
            "Epoch: 40/40... Step: 9700... Loss: 0.6417... Val Loss: 2.0557\n",
            "Epoch: 40/40... Step: 9710... Loss: 0.6342... Val Loss: 2.0608\n",
            "Epoch: 40/40... Step: 9720... Loss: 0.5950... Val Loss: 2.0458\n",
            "Epoch: 40/40... Step: 9730... Loss: 0.6469... Val Loss: 2.0320\n",
            "Epoch: 40/40... Step: 9740... Loss: 0.7451... Val Loss: 2.0421\n",
            "Epoch: 40/40... Step: 9750... Loss: 0.7549... Val Loss: 2.0528\n",
            "Epoch: 40/40... Step: 9760... Loss: 0.5661... Val Loss: 2.0516\n",
            "Epoch: 40/40... Step: 9770... Loss: 0.7001... Val Loss: 2.0378\n",
            "Epoch: 40/40... Step: 9780... Loss: 0.6716... Val Loss: 2.0247\n",
            "Epoch: 40/40... Step: 9790... Loss: 0.6527... Val Loss: 2.0133\n",
            "Epoch: 40/40... Step: 9800... Loss: 0.6082... Val Loss: 2.0166\n",
            "Epoch: 40/40... Step: 9810... Loss: 0.7532... Val Loss: 2.0172\n",
            "Epoch: 40/40... Step: 9820... Loss: 0.6601... Val Loss: 2.0048\n",
            "Epoch: 40/40... Step: 9830... Loss: 0.5257... Val Loss: 1.9888\n",
            "Epoch: 40/40... Step: 9840... Loss: 0.6987... Val Loss: 1.9973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdV0TKz2yOXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "  x = np.array([[net.char2int[char]]])\n",
        "  x = one_hot_encode(x, len(net.chars))\n",
        "  inputs = torch.from_numpy(x)\n",
        "  if(train_on_gpu):\n",
        "    inputs = inputs.cuda()\n",
        "  h = tuple([each.data for each in h])\n",
        "  out, h = net(inputs, h)\n",
        "\n",
        "  p = F.softmax(out, dim=1).data\n",
        "  if(train_on_gpu):\n",
        "    p = p.cpu()\n",
        "  \n",
        "  if top_k is None:\n",
        "    top_ch = np.arange(len(net.chars))\n",
        "  else:\n",
        "      p, top_ch = p.topk(top_k)\n",
        "      top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "  # select the likely next character with some element of randomness\n",
        "  p = p.numpy().squeeze()\n",
        "  char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "  # return the encoded value of the predicted char and the hidden state\n",
        "  return net.int2char[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl8Yf1l0ODBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gExSrtfAOW-P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "58a2df6f-f290-4ff6-8321-599f27d9a817"
      },
      "source": [
        "print(sample(net, 1000, prime='Harry', top_k=5))"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Harry Potter, he was still staring to the wall. ‘It’s – theme people to tolk and Mrs Dursley gossepibed shanps into a sight of diskbing. The Dursleys shudderly, but they was nothing how is the bashroom, Mr Dursley picked up his brigh twat when he dad seemed Potter who had a small son, head. Ne, you could have been crinking our over the monn. He had seen a lattle swoke and sat aistore and Mrs Dursley stopped Mrs Dursley. ‘Well, He couldn’t kill Harry Potter cake the Put-Outer almost twice as quobas arose his freat the weother of showing on the people he’s kissed and stared at the window. ‘Shhohid never know … her sister, buckus it was shousing a map. How very walk havion a smopet and patted her he tanted to the spot. He dad noticed something as it, the Potters … Hapry, the thought before he can homing a little come sear as she wis now and Mrs Dursley stupped 28493.indb 13 8493.indb 14 18/07/2014 16:36 8/07/2014 16:36 8/07/2014 16:36 5 THE BOY WHO LIVED to notice something as inetead what me, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF4xycgvOYkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}