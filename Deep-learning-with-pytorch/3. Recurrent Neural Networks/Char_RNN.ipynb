{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Char-RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPAatqPGXgw0xSU3Il6+LLX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/purvasingh96/Deep-learning-with-neural-networks/blob/master/Deep-learning-with-pytorch/3.%20Recurrent%20Neural%20Networks/Char_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX4-qHyjXdfh",
        "colab_type": "text"
      },
      "source": [
        "# Text generation with an RNN\n",
        "\n",
        "\n",
        "This notebook demonstrates how to generate text using a **charecter-level LSTM with PyTorch** using dataset from the book **Anna Karenina**. Given a sequence of charecter from this book, the model will generate longer sequences of data by calling the model repeatedly.\n",
        "\n",
        "While some of the sentences are grammatical, most do not make sense. The model has not learned the meaning of words, but consider:\n",
        "\n",
        "* The model is character-based. When training started, the model did not know how to spell an English word, or that words were even a unit of text.\n",
        "\n",
        "* The structure of the output resembles a playâ€”blocks of text generally begin with a speaker name, in all capital letters similar to the dataset.\n",
        "\n",
        "* As demonstrated below, the model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure. Below is the **general architecture of the character-wise RNN.**<br>\n",
        "<img src=\"https://github.com/purvasingh96/Deep-learning-with-neural-networks/blob/master/Deep-learning-with-pytorch/3.%20Recurrent%20Neural%20Networks/images/lstm_rnn_architecture.png?raw=1\"></img> \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaIndEAooDrp",
        "colab_type": "text"
      },
      "source": [
        "# Set Up\n",
        "### Import PyTorch and other libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUisr5JLZG18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "224cXT0PoWx5",
        "colab_type": "text"
      },
      "source": [
        "# Download the Anna Karenina data\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAOjATtAoky4",
        "colab_type": "text"
      },
      "source": [
        "### Read the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzQo8BJ_omPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('sample_data/anna.txt', 'r') as f:\n",
        "  text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnN-njxboz-f",
        "colab_type": "text"
      },
      "source": [
        "### First look at the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE8YyQE5o23N",
        "colab_type": "code",
        "outputId": "b9b9da11-4eed-4368-fc06-9f9fd85a561a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "print(text[:100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chapter 1\n",
            "\n",
            "\n",
            "Happy families are all alike; every unhappy family is unhappy in its own\n",
            "way.\n",
            "\n",
            "Everythin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4VDdReg0gS5",
        "colab_type": "text"
      },
      "source": [
        "### GPU Usage\n",
        "Enable GPU acceleration to execute this notebook faster. In Colab: *Runtime > Change runtime type > Hardware acclerator > GPU*. If running locally make sure TensorFlow version >= 1.11."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPxKuEzGrK6t",
        "colab_type": "text"
      },
      "source": [
        "# Process the text\n",
        "### Vectorize the text (Tokenization)\n",
        "Before training, we need to map strings to a numerical representation. Create two lookup tables: one mapping characters to numbers, and another for numbers to characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Po_LfvwrVTj",
        "colab_type": "code",
        "outputId": "e5017e03-d5c3-4de3-c8cd-8a371e4586f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch:ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode text\n",
        "encoded = np.array([char2int[ch] for ch in text])\n",
        "encoded"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([51, 65, 10, ..., 13,  1, 65])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdomr__F0tiW",
        "colab_type": "text"
      },
      "source": [
        "# Pre-processing the data\n",
        "Our LSTM expects an input that is **one-hot encoded** meaning that each character is converted into an integer (via our created dictionary) and then converted into a column vector where only it's corresponding integer index will have the value of 1 and the rest of the vector will be filled with 0's."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV3jIMqG0234",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "  # initialize encoded array\n",
        "  # arr.shape = (3,8)\n",
        "  # np.arange(3) = [0, 1, 2]\n",
        "  # arr.flatten() = ([[1, 2, 3]]) => ([1, 2, 3])\n",
        "  one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "  one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
        "  one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "  return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiR-a3iH1bIC",
        "colab_type": "code",
        "outputId": "d6702e09-6c1f-4dc3-caa4-1518bbb33833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "test_seq = np.array([[3, 4, 5]])\n",
        "one_hot_encode(test_seq, 8)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0.]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPMJtKD34UG9",
        "colab_type": "text"
      },
      "source": [
        "# Making training mini-batches\n",
        "\n",
        "To train on this data, we create mini batches for training. We want our batches to be multiple sequences of some desired number of sequence steps as below-<br>\n",
        "<img src=\"https://github.com/purvasingh96/Deep-learning-with-neural-networks/blob/master/Deep-learning-with-pytorch/3.%20Recurrent%20Neural%20Networks/images/mini_batch_1.png?raw=1\"></img><br><br>\n",
        "In this example, we'll take the encoded characters (passed in as the arr parameter) and split them into multiple sequences, given by batch_size. Each of our sequences will be seq_length long."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeMQ8TXG79db",
        "colab_type": "text"
      },
      "source": [
        "# Creating Batches\n",
        "\n",
        "### 1. Discard text to accomodate completely full mini-batches\n",
        "\n",
        "* batch_size = `N (2)`\n",
        "* seq_length = `M (3)`\n",
        "* no. of charecters in one batch =` N * M (2 * 3 = 6 )`\n",
        "* Total batches `(K)` that can be made out of the given array :\n",
        "\n",
        "`len(arr)/ (no. of charecters per batch) = 12/6 = 2`\n",
        "\n",
        "* Total charecters in array to be kept in-order to accomodate completely full mini-batch - \n",
        "\n",
        "`arr[:N * M * K] = uptil arr[10]` (discarding arr[11]=12)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIONZY45hPeM",
        "colab_type": "text"
      },
      "source": [
        "### 2. Split the array into N batches\n",
        "You can do this by using :<br>`arr.reshape((batch_size, -1))`.<br>\n",
        "After this the size of array should be -<br>\n",
        "`N * (M * K)`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_r0oetGk4bd",
        "colab_type": "text"
      },
      "source": [
        "### 3. Iterate through mini-batches\n",
        "The idea is, each batch is of size `(N * M) window` on `N * (M * K) array`. This window slides over by `seq_length`. We also want both input and target arrays.\n",
        "<br>\n",
        "Target arrays are basically input arrays shifted over by one charecter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnQnpPo7k6bj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}