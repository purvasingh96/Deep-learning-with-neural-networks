{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Char-RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMu2NXk0OSfj/Rm7R6+DxnS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/purvasingh96/Deep-learning-with-neural-networks/blob/master/Deep-learning-with-pytorch/3.%20Recurrent%20Neural%20Networks/Char_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX4-qHyjXdfh",
        "colab_type": "text"
      },
      "source": [
        "# Text generation with an RNN\n",
        "\n",
        "\n",
        "This notebook demonstrates how to generate text using a **charecter-level LSTM with PyTorch** using dataset from the book **Anna Karenina**. Given a sequence of charecter from this book, the model will generate longer sequences of data by calling the model repeatedly.\n",
        "\n",
        "While some of the sentences are grammatical, most do not make sense. The model has not learned the meaning of words, but consider:\n",
        "\n",
        "* The model is character-based. When training started, the model did not know how to spell an English word, or that words were even a unit of text.\n",
        "\n",
        "* The structure of the output resembles a playâ€”blocks of text generally begin with a speaker name, in all capital letters similar to the dataset.\n",
        "\n",
        "* As demonstrated below, the model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure. Below is the **general architecture of the character-wise RNN.**<br>\n",
        "<img src=\"https://github.com/purvasingh96/Deep-learning-with-neural-networks/blob/master/Deep-learning-with-pytorch/3.%20Recurrent%20Neural%20Networks/images/lstm_rnn_architecture.png?raw=1\"></img> \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaIndEAooDrp",
        "colab_type": "text"
      },
      "source": [
        "# Set Up\n",
        "### Import PyTorch and other libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUisr5JLZG18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "224cXT0PoWx5",
        "colab_type": "text"
      },
      "source": [
        "# Download the Anna Karenina data\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAOjATtAoky4",
        "colab_type": "text"
      },
      "source": [
        "### Read the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzQo8BJ_omPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('sample_data/anna.txt', 'r') as f:\n",
        "  text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnN-njxboz-f",
        "colab_type": "text"
      },
      "source": [
        "### First look at the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE8YyQE5o23N",
        "colab_type": "code",
        "outputId": "5c672253-632d-49bd-fd53-d30a39a5f3e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "print(text[:100])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chapter 1\n",
            "\n",
            "\n",
            "Happy families are all alike; every unhappy family is unhappy in its own\n",
            "way.\n",
            "\n",
            "Everythin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4VDdReg0gS5",
        "colab_type": "text"
      },
      "source": [
        "### GPU Usage\n",
        "Enable GPU acceleration to execute this notebook faster. In Colab: *Runtime > Change runtime type > Hardware acclerator > GPU*. If running locally make sure TensorFlow version >= 1.11."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPxKuEzGrK6t",
        "colab_type": "text"
      },
      "source": [
        "# Process the text\n",
        "### Vectorize the text (Tokenization)\n",
        "Before training, we need to map strings to a numerical representation. Create two lookup tables: one mapping characters to numbers, and another for numbers to characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Po_LfvwrVTj",
        "colab_type": "code",
        "outputId": "f825c926-bdaa-48c4-cac6-9b2f6baa0404",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch:ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode text\n",
        "encoded = np.array([char2int[ch] for ch in text])\n",
        "encoded"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([63, 61, 26, ..., 19, 28, 61])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdomr__F0tiW",
        "colab_type": "text"
      },
      "source": [
        "# Pre-processing the data\n",
        "Our LSTM expects an input that is **one-hot encoded** meaning that each character is converted into an integer (via our created dictionary) and then converted into a column vector where only it's corresponding integer index will have the value of 1 and the rest of the vector will be filled with 0's."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV3jIMqG0234",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "  # initialize encoded array\n",
        "  # arr.shape = (3,8)\n",
        "  # np.arange(3) = [0, 1, 2]\n",
        "  # arr.flatten() = ([[1, 2, 3]]) => ([1, 2, 3])\n",
        "  one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "  one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
        "  one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "  return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiR-a3iH1bIC",
        "colab_type": "code",
        "outputId": "e6ada36d-0483-4049-8384-cb17f6c7f1af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "test_seq = np.array([[3, 4, 5]])\n",
        "one_hot_encode(test_seq, 8)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0.]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPMJtKD34UG9",
        "colab_type": "text"
      },
      "source": [
        "# Making training mini-batches\n",
        "\n",
        "To train on this data, we create mini batches for training. We want our batches to be multiple sequences of some desired number of sequence steps as below-<br>\n",
        "<img src=\"https://github.com/purvasingh96/Deep-learning-with-neural-networks/blob/master/Deep-learning-with-pytorch/3.%20Recurrent%20Neural%20Networks/images/mini_batch_1.png?raw=1\"></img><br><br>\n",
        "In this example, we'll take the encoded characters (passed in as the arr parameter) and split them into multiple sequences, given by batch_size. Each of our sequences will be seq_length long."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeMQ8TXG79db",
        "colab_type": "text"
      },
      "source": [
        "# Creating Batches\n",
        "\n",
        "### 1. Discard text to accomodate completely full mini-batches\n",
        "\n",
        "* batch_size = `N (2)`\n",
        "* seq_length = `M (3)`\n",
        "* no. of charecters in one batch =` N * M (2 * 3 = 6 )`\n",
        "* Total batches `(K)` that can be made out of the given array :\n",
        "\n",
        "`len(arr)/ (no. of charecters per batch) = 12/6 = 2`\n",
        "\n",
        "* Total charecters in array to be kept in-order to accomodate completely full mini-batch - \n",
        "\n",
        "`arr[:N * M * K] = uptil arr[10]` (discarding arr[11]=12)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIONZY45hPeM",
        "colab_type": "text"
      },
      "source": [
        "### 2. Split the array into N batches\n",
        "You can do this by using :<br>`arr.reshape((batch_size, -1))`.<br>\n",
        "After this the size of array should be -<br>\n",
        "`N * (M * K)`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_r0oetGk4bd",
        "colab_type": "text"
      },
      "source": [
        "### 3. Iterate through mini-batches\n",
        "The idea is, each batch is of size `(N * M) window` on `N * (M * K) array`. This window slides over by `seq_length`. We also want both input and target arrays.\n",
        "<br>\n",
        "Target arrays are basically input arrays shifted over by one charecter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnQnpPo7k6bj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "  total_batch_size = batch_size*seq_length\n",
        "  n_batches = len(arr)//total_batch_size\n",
        "  arr = arr[:n_batches*total_batch_size]\n",
        "  arr = arr.reshape((batch_size, -1))\n",
        "  print(arr.shape)\n",
        "  # iterate through array, on seq_length at a time\n",
        "  for n in range(0, arr.shape[1], seq_length):\n",
        "    x = arr[:, n:n+seq_length]\n",
        "    y = np.zeros_like(x)\n",
        "    try:\n",
        "      y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "    except IndexError:\n",
        "      y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "    yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuIRaLeDmpQs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "14e7918a-3e86-4f79-eee8-03a6d00374ea"
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8, 80250)\n",
            "x\n",
            " [[63 61 26  9 28  8  0 19 50 35]\n",
            " [74 13 12 68 19 43 26 73  8 68]\n",
            " [ 9 74 12 12 70 69 43  8 10 19]\n",
            " [19 12 74 21  8 19 12 74  0 28]\n",
            " [28  8 12 12 19  4  0 74 21 19]\n",
            " [28 19 69  8 68 19  9  0 26 38]\n",
            " [12 12 70 12 28 26 13 28 68 19]\n",
            " [ 8 72 68 19  9  0 70 13 73  8]]\n",
            "\n",
            "y\n",
            " [[61 26  9 28  8  0 19 50 35 35]\n",
            " [13 12 68 19 43 26 73  8 68 19]\n",
            " [74 12 12 70 69 43  8 10 19 56]\n",
            " [12 74 21  8 19 12 74  0 28 19]\n",
            " [ 8 12 12 19  4  0 74 21 19 28]\n",
            " [19 69  8 68 19  9  0 26 38 11]\n",
            " [12 70 12 28 26 13 28 68 19 28]\n",
            " [72 68 19  9  0 70 13 73  8 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSHBwUXxsbaD",
        "colab_type": "text"
      },
      "source": [
        "# Defining the network with PyTorch\n",
        "Below is sample structure of our LSTM model: <br>\n",
        "<img src=\"https://github.com/purvasingh96/Deep-learning-with-neural-networks/blob/master/Deep-learning-with-pytorch/3.%20Recurrent%20Neural%20Networks/data/15.%20rnn_classifier.png?raw=1\"></img><br>\n",
        "We will use PyTorch to define the model's architecture and define the forward pass method as well.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PC7YF30t59L",
        "colab_type": "text"
      },
      "source": [
        "### Model Structure\n",
        "In `__init__` followinf structure can be defined -<br>\n",
        "* Storing necessasry dictionaries (int2char, char2int)\n",
        "* Defining LSTM layer that takes the following parameters - \n",
        "  * `input size`\n",
        "  * hidden layer size (`n_hidden`)\n",
        "  * number of layers (`n_layers`)\n",
        "  * dropout probability (`drop_prob`)\n",
        "  * Boolean batch first (`batch_first`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osx33r64uw-B",
        "colab_type": "text"
      },
      "source": [
        "### LSTM Inputs/Outputs\n",
        "Basic LSTM can be created as follows - \n",
        "```python\n",
        "self.lstm = nn.LSTM(input_size, n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        " ```\n",
        "An initial hidden state of all zeros needs to be created as well -<br>\n",
        "```python\n",
        "self.init_hidden()\n",
        "``` \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob9_qiovwzxl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ebcb1f2-85a8-425b-b951-7484b336ccaa"
      },
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t6gGbIMxgD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "  def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
        "    super().__init__()\n",
        "    self.drop_prob = drop_prob\n",
        "    self.n_layers = n_layers\n",
        "    self.n_hidden = n_hidden\n",
        "    self.lr = lr\n",
        "\n",
        "    # creating charecter dictionaries\n",
        "    self.chars = tokens\n",
        "    self.int2char = dict(enumerate(self.chars))\n",
        "    self.char2int = {ch:ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "    # defining LSTM model\n",
        "    self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, batch_first=True, dropout=drop_prob)\n",
        "\n",
        "    # defining dropout layer\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "    # defining final fully-connected layer\n",
        "    self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    # lstm will generate new output and new hidden state\n",
        "    r_output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "    # passing x output through dropout layer\n",
        "    out = self.dropout(t_output)\n",
        "\n",
        "    # stacking LSTM using view\n",
        "    # Using contigious to reshape output\n",
        "    out = out.contigious().view(-1, self.n_hidden)\n",
        "\n",
        "    # put out through fully connected layer\n",
        "    out = self.fc(out)\n",
        "\n",
        "    # returning final output and hidden state\n",
        "    return out, hidden\n",
        "  \n",
        "  def init_hidden(self, batch_size):\n",
        "    # creating 2 new tensors\n",
        "    # size = n_layers * batch_size * n_hidden\n",
        "    # initialize to 0 for hidden and cell state of LSTM\n",
        "    weight = next(self.parameters()).data\n",
        "    print(self.parameters())\n",
        "    if(train_on_gpu):\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "    else:\n",
        "      hidden - (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "    return hidden\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhYeUS9W2Iyx",
        "colab_type": "text"
      },
      "source": [
        "# Time to Train\n",
        "\n",
        "Below we will use **Adam optimizer and Cross Entropy**. We calculate loss and perform backpropogation as usual. Few points to note -<br>\n",
        "* Within the batch loop, we detach the hidden state from its history; this time setting it equal to a new tuple variable because an LSTM has a hidden state that is a tuple of the hidden and cell states.\n",
        "* We use clip_grad_norm_ to help prevent exploding gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng9M-eqj2o2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "  net.train()\n",
        "  opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # creating training and validation data\n",
        "  val_idx = int(len(data)*(1-val_frac))\n",
        "  data, val_data = data[:val_idx], data[val_idx:]\n",
        "\n",
        "  if(train_on_gpu):\n",
        "    net.cuda()\n",
        "  counter = 0\n",
        "  n_chars = len(net.chars)\n",
        "  for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    for x, y in get_batches(data, batch_size, seq_length):\n",
        "      counter += 1\n",
        "\n",
        "      # one hot encode our data and make them torch tensors\n",
        "      x = one_hot_encode(x,n_chars)\n",
        "      inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "      if(train_on_gpu):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        \n",
        "      # create new hidden state variable to \n",
        "      # avoid traversing entire history \n",
        "      h = tuple([each.data for each in h])\n",
        "\n",
        "      net.zero_grad()\n",
        "      output, h = net(inputs, h)\n",
        "\n",
        "      loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "      loss.backward()\n",
        "\n",
        "      nn.utils.clip_grad_norm(net.parameters(), clip)\n",
        "      opt.step()\n",
        "\n",
        "      # loss statistics\n",
        "      if counter%print_every == 0:\n",
        "        val_h = net.init_hidden(batch_size)\n",
        "        val_losses = []\n",
        "        net.eval()\n",
        "        for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "          x = one_hot_encode(x, n_chars)\n",
        "          x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "          val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "          inputs, targets = x, y\n",
        "          if(train_on_gpu):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            \n",
        "          output, val_h = net(inputs, val_h)\n",
        "          val_loss = criterion(output, targets.view(batch_size*seq_length).long()) \n",
        "\n",
        "          val_losses.append(val_loss.item())  \n",
        "\n",
        "        net.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SHmvxYvGE4s",
        "colab_type": "text"
      },
      "source": [
        "### Instantiating the model\n",
        "Before training the model, we will first create the network with some given hyper-parameters. Then define mini-batches and start training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i95OCSFPJSdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}