{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Char-RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNsd3rm9R2G3WsUg0ty5SVz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/purvasingh96/Deep-learning-with-neural-networks/blob/master/Deep-learning-with-pytorch/3.%20Recurrent%20Neural%20Networks/Char_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX4-qHyjXdfh",
        "colab_type": "text"
      },
      "source": [
        "# Text generation with an RNN\n",
        "\n",
        "\n",
        "This notebook demonstrates how to generate text using a **charecter-level LSTM with PyTorch** using dataset from the book **Anna Karenina**. Given a sequence of charecter from this book, the model will generate longer sequences of data by calling the model repeatedly.\n",
        "\n",
        "While some of the sentences are grammatical, most do not make sense. The model has not learned the meaning of words, but consider:\n",
        "\n",
        "* The model is character-based. When training started, the model did not know how to spell an English word, or that words were even a unit of text.\n",
        "\n",
        "* The structure of the output resembles a playâ€”blocks of text generally begin with a speaker name, in all capital letters similar to the dataset.\n",
        "\n",
        "* As demonstrated below, the model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure. Below is the **general architecture of the character-wise RNN.**<br>\n",
        "<img src=\"https://github.com/purvasingh96/Deep-learning-with-neural-networks/blob/master/Deep-learning-with-pytorch/3.%20Recurrent%20Neural%20Networks/images/lstm_rnn_architecture.png?raw=1\"></img> \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaIndEAooDrp",
        "colab_type": "text"
      },
      "source": [
        "# Set Up\n",
        "### Import PyTorch and other libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUisr5JLZG18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "224cXT0PoWx5",
        "colab_type": "text"
      },
      "source": [
        "# Download the Anna Karenina data\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAOjATtAoky4",
        "colab_type": "text"
      },
      "source": [
        "### Read the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzQo8BJ_omPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('sample_data/anna.txt', 'r') as f:\n",
        "  text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnN-njxboz-f",
        "colab_type": "text"
      },
      "source": [
        "### First look at the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE8YyQE5o23N",
        "colab_type": "code",
        "outputId": "b9b9da11-4eed-4368-fc06-9f9fd85a561a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "print(text[:100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chapter 1\n",
            "\n",
            "\n",
            "Happy families are all alike; every unhappy family is unhappy in its own\n",
            "way.\n",
            "\n",
            "Everythin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPxKuEzGrK6t",
        "colab_type": "text"
      },
      "source": [
        "# Process the text\n",
        "### Vectorize the text (Tokenization)\n",
        "Before training, we need to map strings to a numerical representation. Create two lookup tables: one mapping characters to numbers, and another for numbers to characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Po_LfvwrVTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch:ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv73eh0hrsRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}